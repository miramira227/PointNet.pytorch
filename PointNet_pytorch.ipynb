{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointNet.pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNKYHjXhXwrietDZ3wJZdGh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miramira227/PointNet_pytorch/blob/master/PointNet_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-3Re1b_UeaH",
        "colab_type": "text"
      },
      "source": [
        "# **Data Upload**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IN6-6XKOUeu8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6a6847d2-d044-4a2f-c81e-81514542cf4c"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlLmE6TdUp_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8d867639-c008-4b5d-9c3d-81d550e616f2"
      },
      "source": [
        "! mkdir mtrain\n",
        "! mkdir mtest\n",
        "! mkdir shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘shape’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tp4-bY2Uvjs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a66afb45-5782-4f48-81a5-31e241e7a849"
      },
      "source": [
        "cd drive/My\\ Drive/pointnet_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/pointnet_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPMWWJhXUv1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! cp model40jtrain.zip '/content/mtrain'\n",
        "! cp model40jtest.zip '/content/mtest'\n",
        "! cp shapej.zip '/content/shape'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUhBZExUVCQB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "124c7670-cf6f-4397-9154-0da9e16b1cf9"
      },
      "source": [
        "cd /content/mtrain/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mtrain\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xc0dxueBV8K_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip model40jtrain.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbS9uzU5V-iA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "685b0ae3-83e2-44dc-a562-59ea7d6dac69"
      },
      "source": [
        "cd /content/mtest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mtest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PahLRwDMWL8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip model40jtest.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjwAEC5xWWe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm model40jtest.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb6MV8SCWaLz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8138b667-2872-4b86-957a-00164e5897dd"
      },
      "source": [
        "cd /content/shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/shape\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33hhowSsWdz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip shapej.zip\n",
        "! rm shapej.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-5sSfcaQf9X",
        "colab_type": "text"
      },
      "source": [
        "#**Check Device Type**##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDQZW783PuSC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7e62c1f7-77bd-41e8-d2b4-6f54b38162eb"
      },
      "source": [
        "import torch \n",
        "\n",
        "if torch.cuda.is_available:\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "# check tensor device \n",
        "a = torch.randn(1, 2, 3).to(device)\n",
        "print(f'current device is {a.device}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "current device is cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al2zf-qWQjm0",
        "colab_type": "text"
      },
      "source": [
        "#**1. Data Augmentation**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_0j6RCPwu3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "f1e03812-67ee-45a4-8016-00aecc988027"
      },
      "source": [
        "import torch \n",
        "import numpy as np \n",
        "\n",
        "input = torch.randn(2, 5, 3).to(device)\n",
        "\n",
        "def data_aug(input):\n",
        "  angle = np.random.uniform(0.0, np.pi * 2)\n",
        "  rotate_matrix = torch.FloatTensor([[np.cos(angle), -1 * np.sin(angle)], \n",
        "                          [np.sin(angle), np.cos(angle)]]).repeat(input.size(0), 1, 1)    # (2, 2) --> (7, 2, 2)\n",
        "  rotate_matrix = rotate_matrix.to(device)\n",
        "  # data augmentation along up-axis(suppose z-axis).\n",
        "  input[:, :, 0:2] = torch.bmm(input[:, :, 0:2], rotate_matrix)\n",
        "  input = torch.normal(0.0, 0.2, size = input.size()).to(device)\n",
        "  return input\n",
        "\n",
        "print(f'original input was \\n {input}')\n",
        "out = data_aug(input)\n",
        "print(f'output is \\n {out}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original input was \n",
            " tensor([[[-0.1801,  0.0308, -0.3258],\n",
            "         [ 0.0419,  0.5573, -0.7421],\n",
            "         [-1.4897, -1.3174, -0.1955],\n",
            "         [-0.4736,  0.2591, -2.5837],\n",
            "         [-0.5975, -0.6196,  1.1691]],\n",
            "\n",
            "        [[-0.2908, -1.0805, -2.0121],\n",
            "         [ 0.4024,  1.7966,  0.8716],\n",
            "         [-0.1868,  1.3511, -1.2778],\n",
            "         [ 0.5074,  0.7624, -2.7151],\n",
            "         [-0.7577, -0.2963,  1.2320]]], device='cuda:0')\n",
            "output is \n",
            " tensor([[[-0.0577, -0.0540,  0.1236],\n",
            "         [-0.0824, -0.0208, -0.1363],\n",
            "         [ 0.1318, -0.1262,  0.0690],\n",
            "         [ 0.0821, -0.3246,  0.5202],\n",
            "         [ 0.1820, -0.1063,  0.0706]],\n",
            "\n",
            "        [[-0.5081, -0.1779, -0.1136],\n",
            "         [ 0.1404, -0.3579,  0.2005],\n",
            "         [ 0.1134,  0.2821,  0.2932],\n",
            "         [-0.0901, -0.1031, -0.3225],\n",
            "         [ 0.2648,  0.2162,  0.2515]]], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZbokx5_QoJD",
        "colab_type": "text"
      },
      "source": [
        "#**2. Transform Netw**#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHuncaF7Qr2g",
        "colab_type": "text"
      },
      "source": [
        "**(1) input transformer matrix**\n",
        "\n",
        "* make transformer matrix of size(batch, 3, 3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybvnYdFqQnob",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c852e3b-0fbd-4df2-f975-07b5abbef7d0"
      },
      "source": [
        "# 1. input transform net \n",
        "import torch.nn as nn \n",
        "import torch \n",
        "import numpy as np \n",
        "\n",
        "# suppose input point cloud size is (2, 5, 3) = (batch size, length, dim)\n",
        "input_pointset = torch.randn((2, 5, 3)).transpose(1, 2).to(device)  \n",
        "\n",
        "class MiniPNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MiniPNet, self).__init__()\n",
        "    self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "    self.bn1 = nn.BatchNorm1d(64)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.conv2 =nn.Conv1d(64, 128, 1)\n",
        "    self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    self.fc3 = nn.Linear(256, 9)    \n",
        "\n",
        "    with torch.no_grad():\n",
        "      self.fc3.weight.data.fill_(0)\n",
        "      self.fc3.bias.data.fill_(0)\n",
        "      iden = torch.eye(3).view(-1)   # double type is required\n",
        "      self.fc3.bias.data = self.fc3.bias.data + iden    # += invokes 'requires grad' error \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "    x = torch.max(x, dim=2)[0]    \n",
        "\n",
        "    x = self.relu(self.bn4(self.fc1(x)))\n",
        "    x = self.relu(self.bn5(self.fc2(x)))\n",
        "\n",
        "    x = self.fc3(x)\n",
        "\n",
        "    x = torch.reshape(x, (-1, 3, 3))\n",
        "    # print(f'input transformer matrix is {x[0]}')\n",
        "    return x\n",
        "\n",
        "\n",
        "net = MiniPNet().to(device)\n",
        "out = net(input_pointset)\n",
        "print(f\"out size is {out.size()}\")\n",
        "\n",
        "for a, b in zip(net.named_modules(), net.parameters()):\n",
        "  print(a, b)\n",
        "\n",
        "print(net.named_modules)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "out size is torch.Size([2, 3, 3])\n",
            "('', MiniPNet(\n",
            "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
            "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
            ")) Parameter containing:\n",
            "tensor([[[ 0.4101],\n",
            "         [-0.1927],\n",
            "         [-0.0100]],\n",
            "\n",
            "        [[ 0.0440],\n",
            "         [-0.0199],\n",
            "         [-0.3048]],\n",
            "\n",
            "        [[ 0.3874],\n",
            "         [-0.2564],\n",
            "         [ 0.2794]],\n",
            "\n",
            "        [[ 0.2301],\n",
            "         [-0.5724],\n",
            "         [ 0.5226]],\n",
            "\n",
            "        [[ 0.0204],\n",
            "         [-0.4834],\n",
            "         [ 0.0385]],\n",
            "\n",
            "        [[ 0.0956],\n",
            "         [ 0.0078],\n",
            "         [-0.4958]],\n",
            "\n",
            "        [[ 0.4679],\n",
            "         [ 0.3313],\n",
            "         [ 0.5448]],\n",
            "\n",
            "        [[-0.0982],\n",
            "         [-0.1034],\n",
            "         [-0.5080]],\n",
            "\n",
            "        [[ 0.1470],\n",
            "         [-0.2198],\n",
            "         [-0.3217]],\n",
            "\n",
            "        [[ 0.0854],\n",
            "         [ 0.0135],\n",
            "         [-0.1298]],\n",
            "\n",
            "        [[-0.5404],\n",
            "         [ 0.3621],\n",
            "         [ 0.0782]],\n",
            "\n",
            "        [[ 0.4342],\n",
            "         [-0.0731],\n",
            "         [ 0.1652]],\n",
            "\n",
            "        [[ 0.2876],\n",
            "         [-0.0714],\n",
            "         [ 0.1882]],\n",
            "\n",
            "        [[ 0.5048],\n",
            "         [ 0.5088],\n",
            "         [-0.2484]],\n",
            "\n",
            "        [[ 0.5167],\n",
            "         [-0.1261],\n",
            "         [ 0.2343]],\n",
            "\n",
            "        [[ 0.3188],\n",
            "         [-0.2842],\n",
            "         [-0.4907]],\n",
            "\n",
            "        [[-0.4516],\n",
            "         [-0.1643],\n",
            "         [-0.5083]],\n",
            "\n",
            "        [[-0.3995],\n",
            "         [-0.2859],\n",
            "         [-0.4176]],\n",
            "\n",
            "        [[ 0.0287],\n",
            "         [-0.0211],\n",
            "         [-0.0551]],\n",
            "\n",
            "        [[ 0.2641],\n",
            "         [ 0.0757],\n",
            "         [ 0.0995]],\n",
            "\n",
            "        [[-0.3839],\n",
            "         [-0.2809],\n",
            "         [-0.5583]],\n",
            "\n",
            "        [[-0.3958],\n",
            "         [ 0.1858],\n",
            "         [ 0.3131]],\n",
            "\n",
            "        [[ 0.1209],\n",
            "         [ 0.2728],\n",
            "         [ 0.1399]],\n",
            "\n",
            "        [[-0.5628],\n",
            "         [ 0.2992],\n",
            "         [ 0.5207]],\n",
            "\n",
            "        [[-0.5658],\n",
            "         [-0.5574],\n",
            "         [-0.2519]],\n",
            "\n",
            "        [[ 0.2748],\n",
            "         [-0.0515],\n",
            "         [-0.4486]],\n",
            "\n",
            "        [[ 0.5113],\n",
            "         [-0.4169],\n",
            "         [ 0.1761]],\n",
            "\n",
            "        [[-0.1022],\n",
            "         [-0.0611],\n",
            "         [ 0.1694]],\n",
            "\n",
            "        [[-0.0661],\n",
            "         [ 0.5449],\n",
            "         [ 0.1183]],\n",
            "\n",
            "        [[-0.2285],\n",
            "         [-0.4859],\n",
            "         [-0.2800]],\n",
            "\n",
            "        [[ 0.0528],\n",
            "         [-0.4978],\n",
            "         [-0.2889]],\n",
            "\n",
            "        [[ 0.0816],\n",
            "         [-0.5674],\n",
            "         [-0.2915]],\n",
            "\n",
            "        [[ 0.0239],\n",
            "         [-0.2470],\n",
            "         [ 0.5099]],\n",
            "\n",
            "        [[ 0.1261],\n",
            "         [-0.0159],\n",
            "         [-0.5041]],\n",
            "\n",
            "        [[ 0.3743],\n",
            "         [ 0.4635],\n",
            "         [ 0.4964]],\n",
            "\n",
            "        [[-0.2581],\n",
            "         [ 0.1534],\n",
            "         [-0.3142]],\n",
            "\n",
            "        [[-0.3474],\n",
            "         [-0.5665],\n",
            "         [-0.0338]],\n",
            "\n",
            "        [[-0.1727],\n",
            "         [ 0.2713],\n",
            "         [ 0.3380]],\n",
            "\n",
            "        [[-0.1615],\n",
            "         [ 0.4007],\n",
            "         [ 0.0198]],\n",
            "\n",
            "        [[-0.3050],\n",
            "         [ 0.0135],\n",
            "         [-0.3435]],\n",
            "\n",
            "        [[-0.2286],\n",
            "         [-0.0582],\n",
            "         [ 0.5038]],\n",
            "\n",
            "        [[ 0.0033],\n",
            "         [-0.1067],\n",
            "         [ 0.4447]],\n",
            "\n",
            "        [[-0.0242],\n",
            "         [-0.0531],\n",
            "         [ 0.2246]],\n",
            "\n",
            "        [[ 0.2481],\n",
            "         [ 0.5495],\n",
            "         [ 0.4366]],\n",
            "\n",
            "        [[-0.1547],\n",
            "         [-0.2263],\n",
            "         [ 0.2637]],\n",
            "\n",
            "        [[-0.0806],\n",
            "         [-0.5140],\n",
            "         [-0.1213]],\n",
            "\n",
            "        [[-0.3531],\n",
            "         [ 0.5281],\n",
            "         [-0.1790]],\n",
            "\n",
            "        [[-0.2915],\n",
            "         [-0.4445],\n",
            "         [-0.2388]],\n",
            "\n",
            "        [[-0.2289],\n",
            "         [-0.5209],\n",
            "         [-0.0478]],\n",
            "\n",
            "        [[ 0.1593],\n",
            "         [ 0.4722],\n",
            "         [-0.3619]],\n",
            "\n",
            "        [[-0.4395],\n",
            "         [-0.1922],\n",
            "         [-0.0404]],\n",
            "\n",
            "        [[ 0.1789],\n",
            "         [ 0.5270],\n",
            "         [-0.0266]],\n",
            "\n",
            "        [[-0.0809],\n",
            "         [-0.4851],\n",
            "         [ 0.2909]],\n",
            "\n",
            "        [[ 0.0273],\n",
            "         [-0.2415],\n",
            "         [ 0.5208]],\n",
            "\n",
            "        [[ 0.2536],\n",
            "         [ 0.2407],\n",
            "         [-0.4636]],\n",
            "\n",
            "        [[-0.4390],\n",
            "         [-0.3815],\n",
            "         [-0.3866]],\n",
            "\n",
            "        [[-0.3420],\n",
            "         [-0.2184],\n",
            "         [ 0.3262]],\n",
            "\n",
            "        [[-0.0201],\n",
            "         [ 0.4492],\n",
            "         [-0.4758]],\n",
            "\n",
            "        [[-0.3920],\n",
            "         [-0.1651],\n",
            "         [-0.4311]],\n",
            "\n",
            "        [[-0.2710],\n",
            "         [ 0.3707],\n",
            "         [ 0.2207]],\n",
            "\n",
            "        [[ 0.2123],\n",
            "         [ 0.1759],\n",
            "         [-0.3747]],\n",
            "\n",
            "        [[ 0.5368],\n",
            "         [-0.5748],\n",
            "         [-0.5189]],\n",
            "\n",
            "        [[-0.1007],\n",
            "         [-0.1791],\n",
            "         [ 0.0907]],\n",
            "\n",
            "        [[-0.0515],\n",
            "         [ 0.2516],\n",
            "         [ 0.2673]]], device='cuda:0', requires_grad=True)\n",
            "('conv1', Conv1d(3, 64, kernel_size=(1,), stride=(1,))) Parameter containing:\n",
            "tensor([-0.0467,  0.4789, -0.4692,  0.4609,  0.0440, -0.3705,  0.5002, -0.4226,\n",
            "        -0.3929, -0.5766, -0.1398,  0.3987, -0.1889,  0.1091,  0.0112, -0.4714,\n",
            "         0.4328, -0.3153, -0.1100, -0.3255, -0.0399,  0.3739,  0.1350, -0.5488,\n",
            "        -0.5379,  0.0634, -0.1483,  0.3591,  0.1323, -0.2824,  0.3929, -0.4742,\n",
            "         0.1524, -0.2551,  0.5230, -0.1221, -0.5726, -0.2769,  0.3334, -0.4200,\n",
            "        -0.4742, -0.3728, -0.0743,  0.5064, -0.5569, -0.1799,  0.4911,  0.4256,\n",
            "        -0.0776,  0.1346, -0.2321,  0.2027,  0.3389, -0.4527,  0.5543, -0.2554,\n",
            "        -0.2578,  0.0154,  0.0721, -0.2140,  0.0846, -0.3785, -0.5725, -0.4425],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "('bn1', BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "('relu', ReLU()) Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "('conv2', Conv1d(64, 128, kernel_size=(1,), stride=(1,))) Parameter containing:\n",
            "tensor([[[ 0.0933],\n",
            "         [ 0.1142],\n",
            "         [ 0.1089],\n",
            "         ...,\n",
            "         [ 0.1176],\n",
            "         [-0.0049],\n",
            "         [-0.0863]],\n",
            "\n",
            "        [[-0.0616],\n",
            "         [ 0.0701],\n",
            "         [-0.0770],\n",
            "         ...,\n",
            "         [ 0.0015],\n",
            "         [-0.0753],\n",
            "         [ 0.0640]],\n",
            "\n",
            "        [[-0.0490],\n",
            "         [ 0.1218],\n",
            "         [ 0.0447],\n",
            "         ...,\n",
            "         [ 0.0963],\n",
            "         [-0.0516],\n",
            "         [-0.0587]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0046],\n",
            "         [ 0.0865],\n",
            "         [ 0.0607],\n",
            "         ...,\n",
            "         [ 0.0865],\n",
            "         [ 0.0078],\n",
            "         [-0.0639]],\n",
            "\n",
            "        [[ 0.0132],\n",
            "         [-0.0656],\n",
            "         [-0.0586],\n",
            "         ...,\n",
            "         [ 0.0529],\n",
            "         [ 0.1032],\n",
            "         [-0.0479]],\n",
            "\n",
            "        [[ 0.0272],\n",
            "         [ 0.1047],\n",
            "         [ 0.0245],\n",
            "         ...,\n",
            "         [ 0.1179],\n",
            "         [ 0.0710],\n",
            "         [-0.1019]]], device='cuda:0', requires_grad=True)\n",
            "('bn2', BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Parameter containing:\n",
            "tensor([-0.0994,  0.0345,  0.1026,  0.0209,  0.0603, -0.0112,  0.0982,  0.0665,\n",
            "        -0.0236, -0.0005,  0.0614,  0.1070,  0.0485, -0.0737,  0.0261, -0.0620,\n",
            "         0.1018, -0.1089,  0.0011,  0.1013, -0.0935, -0.0379, -0.0505,  0.0807,\n",
            "        -0.0019,  0.0449, -0.0537, -0.0827, -0.0839, -0.0468, -0.1016,  0.1202,\n",
            "        -0.0782,  0.0266,  0.0139, -0.1044, -0.0514,  0.0666,  0.1138, -0.0175,\n",
            "        -0.0725, -0.0615, -0.0380,  0.0717, -0.0141,  0.1199,  0.0877, -0.0581,\n",
            "         0.0597, -0.0206, -0.1171,  0.1012,  0.0491,  0.0006, -0.1179,  0.1245,\n",
            "         0.0018,  0.0607,  0.0936, -0.0319, -0.1014, -0.1222, -0.0254, -0.0938,\n",
            "        -0.0054, -0.0210, -0.0262,  0.0252, -0.0212,  0.0090,  0.0155, -0.1090,\n",
            "         0.0607,  0.0495,  0.0826,  0.0779, -0.1055,  0.0628, -0.0021,  0.0448,\n",
            "         0.0687, -0.0804,  0.0597, -0.0102,  0.0138, -0.0092, -0.1241, -0.0584,\n",
            "        -0.1031,  0.0188, -0.0305,  0.0428,  0.0114,  0.0501, -0.0213, -0.1156,\n",
            "        -0.1035, -0.0914,  0.1174, -0.0161, -0.0615,  0.0274, -0.0929, -0.0497,\n",
            "        -0.1030,  0.0317, -0.0690,  0.1013,  0.0525,  0.1016,  0.1032,  0.0197,\n",
            "        -0.0865,  0.0051, -0.1049,  0.0154,  0.0112, -0.0233, -0.0278,  0.0204,\n",
            "        -0.0179, -0.0126,  0.0546, -0.0527, -0.1093,  0.1070, -0.0037,  0.0610],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "('conv3', Conv1d(128, 1024, kernel_size=(1,), stride=(1,))) Parameter containing:\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1.], device='cuda:0', requires_grad=True)\n",
            "('bn3', BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Parameter containing:\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', requires_grad=True)\n",
            "('fc1', Linear(in_features=1024, out_features=512, bias=True)) Parameter containing:\n",
            "tensor([[[-0.0037],\n",
            "         [-0.0791],\n",
            "         [ 0.0075],\n",
            "         ...,\n",
            "         [-0.0724],\n",
            "         [ 0.0098],\n",
            "         [-0.0190]],\n",
            "\n",
            "        [[ 0.0092],\n",
            "         [ 0.0675],\n",
            "         [-0.0449],\n",
            "         ...,\n",
            "         [ 0.0840],\n",
            "         [-0.0401],\n",
            "         [ 0.0059]],\n",
            "\n",
            "        [[-0.0317],\n",
            "         [ 0.0601],\n",
            "         [ 0.0336],\n",
            "         ...,\n",
            "         [ 0.0779],\n",
            "         [-0.0855],\n",
            "         [-0.0444]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0452],\n",
            "         [ 0.0578],\n",
            "         [-0.0337],\n",
            "         ...,\n",
            "         [-0.0586],\n",
            "         [-0.0042],\n",
            "         [-0.0406]],\n",
            "\n",
            "        [[-0.0623],\n",
            "         [ 0.0876],\n",
            "         [ 0.0693],\n",
            "         ...,\n",
            "         [ 0.0850],\n",
            "         [-0.0379],\n",
            "         [ 0.0674]],\n",
            "\n",
            "        [[-0.0089],\n",
            "         [ 0.0681],\n",
            "         [-0.0693],\n",
            "         ...,\n",
            "         [ 0.0740],\n",
            "         [-0.0741],\n",
            "         [-0.0275]]], device='cuda:0', requires_grad=True)\n",
            "('bn4', BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Parameter containing:\n",
            "tensor([-0.0581, -0.0844,  0.0875,  ..., -0.0645,  0.0736,  0.0551],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "('fc2', Linear(in_features=512, out_features=256, bias=True)) Parameter containing:\n",
            "tensor([1., 1., 1.,  ..., 1., 1., 1.], device='cuda:0', requires_grad=True)\n",
            "('bn5', BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)) Parameter containing:\n",
            "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)\n",
            "('fc3', Linear(in_features=256, out_features=9, bias=True)) Parameter containing:\n",
            "tensor([[-0.0019, -0.0072, -0.0238,  ...,  0.0207, -0.0008, -0.0135],\n",
            "        [ 0.0092,  0.0041, -0.0308,  ..., -0.0070,  0.0051, -0.0209],\n",
            "        [-0.0209, -0.0284,  0.0079,  ...,  0.0091,  0.0009,  0.0122],\n",
            "        ...,\n",
            "        [ 0.0244,  0.0088,  0.0279,  ...,  0.0117,  0.0152, -0.0072],\n",
            "        [-0.0206,  0.0227,  0.0185,  ...,  0.0115, -0.0266,  0.0249],\n",
            "        [ 0.0207, -0.0252,  0.0173,  ...,  0.0258,  0.0078,  0.0217]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "<bound method Module.named_modules of MiniPNet(\n",
            "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
            "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
            "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
            "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
            "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wweWF-UBRNdC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkedQXboRNz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "6f6fe0ce-dda1-4441-dca1-2f1002fd2333"
      },
      "source": [
        "# 2. feature transform net\n",
        "import torch.nn as nn \n",
        "import torch \n",
        "import numpy as np \n",
        "\n",
        "\n",
        "input_pointset = torch.randn((2, 5, 128)).transpose(1, 2).to(device)   \n",
        "\n",
        "# k = integer related to the shape of input tensor, \n",
        "# and also a key factor for the shape of output transformer matrix (batch, k, k)\n",
        "# which will transform the input tensor\n",
        "\n",
        "class FeatPNet(nn.Module):\n",
        "  def __init__(self, k):\n",
        "    super(FeatPNet, self).__init__()\n",
        "    self.k = k\n",
        "\n",
        "    self.conv1 = nn.Conv1d(self.k, 64, 1)\n",
        "    self.bn1 = nn.BatchNorm1d(64)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.conv2 =nn.Conv1d(64, 128, 1)\n",
        "    self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(128, 1024, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.bn5 = nn.BatchNorm1d(256)\n",
        "\n",
        "    self.fc3 = nn.Linear(256, self.k * self.k)    \n",
        "    self.fc3.weight.data.fill_(0)\n",
        "    self.fc3.bias.data.fill_(0)\n",
        "\n",
        "    iden = torch.eye(self.k).view(-1)\n",
        "    \n",
        "    self.fc3.bias.data = self.fc3.bias.data + iden\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "    x = self.relu(self.bn3(self.conv3(x)))\n",
        "\n",
        "    x = torch.max(x, dim=2)[0]    # (batch, 1024)\n",
        "\n",
        "    x = self.relu(self.bn4(self.fc1(x)))\n",
        "    x = self.relu(self.bn5(self.fc2(x)))\n",
        "\n",
        "    x = self.fc3(x)\n",
        "    x = torch.reshape(x, (-1, self.k, self.k))\n",
        "    # print(f'transformer matrix is {x[0]}')\n",
        "    return x\n",
        "\n",
        "net = FeatPNet(input_pointset.size(1)).to(device) #dimension of input tensor\n",
        "print(f\"input size is {input_pointset.size()}\")\n",
        "out = net(input_pointset)\n",
        "print(f\"output size is {out.size()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input size is torch.Size([2, 128, 5])\n",
            "output size is torch.Size([2, 128, 128])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpa_BEclSUHE",
        "colab_type": "text"
      },
      "source": [
        "#**Feature Transformer Regularizer**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8dzf2ZOSX1j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5e8ce458-446e-4c96-c983-dff7345186b2"
      },
      "source": [
        "import torch \n",
        "\n",
        "feat_stn = torch.randn((32, 64, 64)).to(device)\n",
        "\n",
        "def regularizer(feat_stn):\n",
        "  size = feat_stn.size(1)\n",
        "  # iden = torch.eye(size).view(-1).repeat(feat_stn.size(0)).view(-1, size, size).to(device)\n",
        "  iden = torch.eye(size).repeat(feat_stn.size(0), 1, 1).to(device)\n",
        "  AAT = torch.bmm(feat_stn, feat_stn.transpose(1, 2)).to(device)\n",
        "    \n",
        "  norm = 0\n",
        "  for i in range(iden.size(0)):\n",
        "    # torch.norm is used to calculate frobius norm of matrix, \n",
        "    # which is similar to L2 norm in vector\n",
        "    fro = torch.norm(iden[i] - AAT[i])    \n",
        "    norm += fro\n",
        "  return norm/iden.size(0)\n",
        "\n",
        "out = regularizer(feat_stn)\n",
        "print(f\"regularizer out value is {out}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "regularizer out value is 717.2933349609375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH7KAkEaTG1O",
        "colab_type": "text"
      },
      "source": [
        "#**Baseline for Global Feature**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIHnHkH6TL3G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b12676e0-6e5a-4fb4-d96b-93b17b801c37"
      },
      "source": [
        "import torch.nn as nn \n",
        "import torch \n",
        "import numpy as np \n",
        "\n",
        "input = torch.randn((2, 5, 3)).transpose(1, 2).to(device)\n",
        "\n",
        "class Baseline(nn.Module):\n",
        "  def __init__(self, k=64):\n",
        "    super(Baseline, self).__init__()\n",
        "    self.mini_T = MiniPNet()\n",
        "\n",
        "    self.conv1 = nn.Conv1d(3, 64, 1)\n",
        "    self.bn1 = nn.BatchNorm1d(64)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.conv2 = nn.Conv1d(64, 64, 1)\n",
        "    self.bn2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    self.feat_T = FeatPNet(k)\n",
        "\n",
        "    self.conv3 = nn.Conv1d(64, 64, 1)\n",
        "    self.bn3 = nn.BatchNorm1d(64)\n",
        "\n",
        "    self.conv4 = nn.Conv1d(64, 128, 1)\n",
        "    self.bn4 = nn.BatchNorm1d(128)\n",
        "\n",
        "    self.conv5 = nn.Conv1d(128, 1024, 1)\n",
        "    self.bn5 = nn.BatchNorm1d(1024)\n",
        "\n",
        "  def forward(self, x):\n",
        "    mini_stn = self.mini_T(x)    \n",
        "\n",
        "    x = torch.bmm(mini_stn, x)\n",
        "\n",
        "    x = self.relu(self.bn1(self.conv1(x)))\n",
        "    x = self.relu(self.bn2(self.conv2(x)))\n",
        "\n",
        "    feat_stn = self.feat_T(x)\n",
        "\n",
        "    local_feat = torch.bmm(feat_stn, x)\n",
        "\n",
        "    x = self.relu(self.bn3(self.conv3(local_feat)))\n",
        "    x = self.relu(self.bn4(self.conv4(x)))\n",
        "    x = self.relu(self.bn5(self.conv5(x)))\n",
        "\n",
        "    global_feat = torch.max(x, dim=2)[0]\n",
        "\n",
        "    return global_feat, feat_stn\n",
        "\n",
        "net = Baseline().to(device)\n",
        "global_feat, feat_stn = net(input)\n",
        "print(f\"global_feat size: {global_feat.size()}, \\nfeat_stn size: {feat_stn.size()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "global_feat size: torch.Size([2, 1024]), \n",
            "feat_stn size: torch.Size([2, 64, 64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Kp9MbuTkVv",
        "colab_type": "text"
      },
      "source": [
        "#**Classification Module**#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VorRML6Tm9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Classification(nn.Module):\n",
        "  def __init__(self, num_cls):\n",
        "    super(Classification, self).__init__()\n",
        "    self.fc1 = nn.Linear(1024, 512)\n",
        "    self.bn1 = nn.BatchNorm1d(512)\n",
        "    \n",
        "    self.fc2 = nn.Linear(512, 256)\n",
        "    self.bn2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.3)    # rate to be zeroed \n",
        "    self.fc3 = nn.Linear(256, num_cls)\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    global_feat, feat_stn = x[0], x[1]\n",
        "    x = self.relu(self.bn1(self.fc1(global_feat)))\n",
        "    x = self.relu(self.bn2(self.dropout(self.fc2(x))))\n",
        "    x = self.relu(self.fc3(x))\n",
        "    \n",
        "    cls_score = F.log_softmax(x, dim= 1)\n",
        "    # print(f'cls_score is {cls_score}')\n",
        "    return cls_score, feat_stn, global_feat \n",
        "\n",
        "\n",
        "num_cls = 40\n",
        "\n",
        "input = torch.rand(2, 5, 3).transpose(1, 2).to(device)\n",
        "cls_net = nn.Sequential(Baseline(), Classification(num_cls)).to(device).eval()\n",
        "cls_score, feat_stn, global_feat = cls_net(input)\n",
        "# print(f\"cls_score size: {cls_score.size()}, \\nfeat_stn size: {feat_stn.size()},\"\n",
        "# f\"\\nglobal_feat size: {global_feat.size()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGnwQBlQTuHb",
        "colab_type": "text"
      },
      "source": [
        "#**Train/Val Split**#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWt8MtsVWyvv",
        "colab_type": "text"
      },
      "source": [
        "val 결과 나온 weight 먼저 확인하고 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyuJmZxlW7tB",
        "colab_type": "text"
      },
      "source": [
        "# **Classification Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLYbYhJsTzJv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee3bff65-f4d6-4fc9-8c28-1389548d7218"
      },
      "source": [
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn \n",
        "import json\n",
        "import torch\n",
        "import math\n",
        "import time\n",
        "import os \n",
        "\n",
        "model = nn.Sequential(Baseline(), Classification(40)).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
        "\n",
        "batch = 32\n",
        "\n",
        "train_path = '/content/mtrain'\n",
        "train_data = os.listdir(train_path)\n",
        "\n",
        "test_path = '/content/mtest'\n",
        "test_data = os.listdir(test_path)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch, shuffle=True)\n",
        "\n",
        "train_loss, val_loss = [], []\n",
        "epochs = 100\n",
        "\n",
        "now = time.time()\n",
        "print('training start!')\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for bidx, trainb32 in enumerate(train_loader):\n",
        "\n",
        "    bpts, blabel = [], []\n",
        "    \n",
        "    for i, data in enumerate(trainb32):\n",
        "      path = os.path.join(train_path, data)\n",
        "      \n",
        "      with open(path, 'r') as f:\n",
        "        jdata = json.load(f)\n",
        "      \n",
        "      label = jdata['label']\n",
        "      pts = jdata['pts']\n",
        "\n",
        "      bpts.append(pts) \n",
        "      blabel.append(label)\n",
        "\n",
        "    bpts = torch.tensor(bpts).transpose(1, 2).to(device)\n",
        "    blabel = torch.tensor(blabel).to(device)\n",
        "    \n",
        "    input = data_aug(bpts).to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    y_pred, feat_stn, glob_feat = model(input)\n",
        "    # print(f'global_feat is {global_feat}')\n",
        "    loss = F.nll_loss(y_pred, blabel) + 0.001 * regularizer(feat_stn)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "    if bidx % 10 == 9:\n",
        "      vrunning_loss = 0\n",
        "      vacc = 0\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        \n",
        "        # val batch\n",
        "        for vbidx, testb32 in enumerate(test_loader):\n",
        "          bpts, blabel = [], []\n",
        "          \n",
        "          for j, data in enumerate(testb32):\n",
        "            path = os.path.join(test_path, data)\n",
        "            \n",
        "            with open(path, 'r') as f:\n",
        "              jdata = json.load(f)\n",
        "            \n",
        "            label = jdata['label']\n",
        "            pts = jdata['pts']\n",
        "\n",
        "            bpts.append(pts) \n",
        "            blabel.append(label)\n",
        "\n",
        "          bpts = torch.tensor(bpts).transpose(1, 2).to(device)\n",
        "          blabel = torch.tensor(blabel).to(device)\n",
        "          \n",
        "          input = data_aug(bpts).to(device)\n",
        "          vy_pred, vfeat_stn, vglob_feat = model(input)\n",
        "          # print(f'global_feat is {vglob_feat}')\n",
        "          vloss = F.nll_loss(vy_pred, blabel) + 0.001 * regularizer(vfeat_stn)\n",
        "          _, vy_max = torch.max(vy_pred, dim=1) \n",
        "          vy_acc = torch.sum(vy_max == blabel) / batch\n",
        "          \n",
        "          vacc += vy_acc\n",
        "          vrunning_loss += vloss\n",
        "\n",
        "      # print every training 10th batch\n",
        "      train_loss.append(running_loss / len(train_loader))\n",
        "      val_loss.append(vrunning_loss / len(test_loader))\n",
        "\n",
        "      print(f\"Epoch {epoch+1}/{epochs} {bidx}/{len(train_loader)}.. \"\n",
        "            f\"Train loss: {running_loss / 10:.3f}..\"\n",
        "            f\"Val loss: {vrunning_loss / len(test_loader):.3f}..\"\n",
        "            f\"Val Accuracy: {vacc/len(test_loader):.3f}..\"\n",
        "            f\"Time: {time.time() - now}\")\n",
        "      now = time.time()\n",
        "      running_loss = 0\n",
        "      model.train()\n",
        "      \n",
        "          \n",
        "print(f'training finish! training time is {time.time() - now}')\n",
        "print(model.parameters())\n",
        "\n",
        "savePath = '/content/modelpath.pth'\n",
        "torch.save(model.state_dict(), '/content/modelpath.pth')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 88/100 29/238.. Train loss: 3.410..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.515955686569214\n",
            "Epoch 88/100 39/238.. Train loss: 3.366..Val loss: 3.590..Val Accuracy: 0.000..Time: 5.55347204208374\n",
            "Epoch 88/100 49/238.. Train loss: 3.412..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.48908805847168\n",
            "Epoch 88/100 59/238.. Train loss: 3.437..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.599501848220825\n",
            "Epoch 88/100 69/238.. Train loss: 3.339..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.51908802986145\n",
            "Epoch 88/100 79/238.. Train loss: 3.386..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.58879828453064\n",
            "Epoch 88/100 89/238.. Train loss: 3.417..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.548323392868042\n",
            "Epoch 88/100 99/238.. Train loss: 3.374..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.6686036586761475\n",
            "Epoch 88/100 109/238.. Train loss: 3.412..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.619024991989136\n",
            "Epoch 88/100 119/238.. Train loss: 3.448..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.5897135734558105\n",
            "Epoch 88/100 129/238.. Train loss: 3.409..Val loss: 3.589..Val Accuracy: 0.000..Time: 5.5890820026397705\n",
            "Epoch 88/100 139/238.. Train loss: 3.456..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.565517902374268\n",
            "Epoch 88/100 149/238.. Train loss: 3.440..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.569021940231323\n",
            "Epoch 88/100 159/238.. Train loss: 3.455..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.570079326629639\n",
            "Epoch 88/100 169/238.. Train loss: 3.358..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.612884044647217\n",
            "Epoch 88/100 179/238.. Train loss: 3.438..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.501947641372681\n",
            "Epoch 88/100 189/238.. Train loss: 3.439..Val loss: 3.593..Val Accuracy: 0.000..Time: 5.587913274765015\n",
            "Epoch 88/100 199/238.. Train loss: 3.385..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.564551591873169\n",
            "Epoch 88/100 209/238.. Train loss: 3.400..Val loss: 3.590..Val Accuracy: 0.000..Time: 5.537845134735107\n",
            "Epoch 88/100 219/238.. Train loss: 3.353..Val loss: 3.591..Val Accuracy: 0.000..Time: 5.551797151565552\n",
            "Epoch 88/100 229/238.. Train loss: 3.399..Val loss: 3.590..Val Accuracy: 0.000..Time: 5.5339515209198\n",
            "Epoch 89/100 9/238.. Train loss: 3.362..Val loss: 3.589..Val Accuracy: 0.000..Time: 6.660680055618286\n",
            "Epoch 89/100 19/238.. Train loss: 3.459..Val loss: 3.593..Val Accuracy: 0.000..Time: 6.232794523239136\n",
            "Epoch 89/100 29/238.. Train loss: 3.401..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.491513967514038\n",
            "Epoch 89/100 39/238.. Train loss: 3.424..Val loss: 3.588..Val Accuracy: 0.000..Time: 5.560094833374023\n",
            "Epoch 89/100 49/238.. Train loss: 3.410..Val loss: 3.592..Val Accuracy: 0.000..Time: 5.534459829330444\n",
            "Epoch 89/100 59/238.. Train loss: 3.338..Val loss: 3.591..Val Accuracy: 0.000..Time: 5.556957721710205\n",
            "Epoch 89/100 69/238.. Train loss: 3.441..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.540388822555542\n",
            "Epoch 89/100 79/238.. Train loss: 3.481..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.500024795532227\n",
            "Epoch 89/100 89/238.. Train loss: 3.419..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.566916227340698\n",
            "Epoch 89/100 99/238.. Train loss: 3.377..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.4937379360198975\n",
            "Epoch 89/100 109/238.. Train loss: 3.441..Val loss: 3.586..Val Accuracy: 0.000..Time: 5.577816009521484\n",
            "Epoch 89/100 119/238.. Train loss: 3.430..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.469295978546143\n",
            "Epoch 89/100 129/238.. Train loss: 3.319..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.561673402786255\n",
            "Epoch 89/100 139/238.. Train loss: 3.364..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.782708644866943\n",
            "Epoch 89/100 149/238.. Train loss: 3.352..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.572593450546265\n",
            "Epoch 89/100 159/238.. Train loss: 3.377..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.583139657974243\n",
            "Epoch 89/100 169/238.. Train loss: 3.429..Val loss: 3.614..Val Accuracy: 0.000..Time: 5.526755094528198\n",
            "Epoch 89/100 179/238.. Train loss: 3.406..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.649748086929321\n",
            "Epoch 89/100 189/238.. Train loss: 3.497..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.630712509155273\n",
            "Epoch 89/100 199/238.. Train loss: 3.410..Val loss: 3.593..Val Accuracy: 0.000..Time: 5.571094989776611\n",
            "Epoch 89/100 209/238.. Train loss: 3.488..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.478348255157471\n",
            "Epoch 89/100 219/238.. Train loss: 3.348..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.581651926040649\n",
            "Epoch 89/100 229/238.. Train loss: 3.342..Val loss: 3.590..Val Accuracy: 0.000..Time: 5.5920796394348145\n",
            "Epoch 90/100 9/238.. Train loss: 3.403..Val loss: 3.596..Val Accuracy: 0.000..Time: 6.341175079345703\n",
            "Epoch 90/100 19/238.. Train loss: 3.363..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.497404336929321\n",
            "Epoch 90/100 29/238.. Train loss: 3.377..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.5858471393585205\n",
            "Epoch 90/100 39/238.. Train loss: 3.456..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.563359260559082\n",
            "Epoch 90/100 49/238.. Train loss: 3.413..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.556140422821045\n",
            "Epoch 90/100 59/238.. Train loss: 3.499..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.499662160873413\n",
            "Epoch 90/100 69/238.. Train loss: 3.405..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.596121549606323\n",
            "Epoch 90/100 79/238.. Train loss: 3.417..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.532851696014404\n",
            "Epoch 90/100 89/238.. Train loss: 3.412..Val loss: 3.590..Val Accuracy: 0.000..Time: 5.64736270904541\n",
            "Epoch 90/100 99/238.. Train loss: 3.433..Val loss: 3.590..Val Accuracy: 0.000..Time: 5.548784494400024\n",
            "Epoch 90/100 109/238.. Train loss: 3.447..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.5007147789001465\n",
            "Epoch 90/100 119/238.. Train loss: 3.436..Val loss: 3.584..Val Accuracy: 0.000..Time: 5.565492391586304\n",
            "Epoch 90/100 129/238.. Train loss: 3.465..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.49449610710144\n",
            "Epoch 90/100 139/238.. Train loss: 3.453..Val loss: 3.589..Val Accuracy: 0.000..Time: 5.5981605052948\n",
            "Epoch 90/100 149/238.. Train loss: 3.374..Val loss: 3.588..Val Accuracy: 0.000..Time: 5.520067930221558\n",
            "Epoch 90/100 159/238.. Train loss: 3.403..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.5775909423828125\n",
            "Epoch 90/100 169/238.. Train loss: 3.357..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.509305715560913\n",
            "Epoch 90/100 179/238.. Train loss: 3.320..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.583714962005615\n",
            "Epoch 90/100 189/238.. Train loss: 3.429..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.563966512680054\n",
            "Epoch 90/100 199/238.. Train loss: 3.424..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.513465642929077\n",
            "Epoch 90/100 209/238.. Train loss: 3.396..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.607801675796509\n",
            "Epoch 90/100 219/238.. Train loss: 3.360..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.587742805480957\n",
            "Epoch 90/100 229/238.. Train loss: 3.294..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.583107233047485\n",
            "Epoch 91/100 9/238.. Train loss: 3.428..Val loss: 3.613..Val Accuracy: 0.000..Time: 6.337264060974121\n",
            "Epoch 91/100 19/238.. Train loss: 3.393..Val loss: 3.613..Val Accuracy: 0.000..Time: 5.581185817718506\n",
            "Epoch 91/100 29/238.. Train loss: 3.449..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.565738201141357\n",
            "Epoch 91/100 39/238.. Train loss: 3.360..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.5134289264678955\n",
            "Epoch 91/100 49/238.. Train loss: 3.407..Val loss: 3.591..Val Accuracy: 0.000..Time: 5.530497312545776\n",
            "Epoch 91/100 59/238.. Train loss: 3.419..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.56798243522644\n",
            "Epoch 91/100 69/238.. Train loss: 3.421..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.495940208435059\n",
            "Epoch 91/100 79/238.. Train loss: 3.364..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.584785223007202\n",
            "Epoch 91/100 89/238.. Train loss: 3.406..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.483806371688843\n",
            "Epoch 91/100 99/238.. Train loss: 3.391..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.632452726364136\n",
            "Epoch 91/100 109/238.. Train loss: 3.438..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.500831365585327\n",
            "Epoch 91/100 119/238.. Train loss: 3.318..Val loss: 3.612..Val Accuracy: 0.000..Time: 5.576786279678345\n",
            "Epoch 91/100 129/238.. Train loss: 3.424..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.5824408531188965\n",
            "Epoch 91/100 139/238.. Train loss: 3.350..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.570840358734131\n",
            "Epoch 91/100 149/238.. Train loss: 3.272..Val loss: 3.614..Val Accuracy: 0.000..Time: 5.549333333969116\n",
            "Epoch 91/100 159/238.. Train loss: 3.404..Val loss: 3.613..Val Accuracy: 0.000..Time: 5.528401136398315\n",
            "Epoch 91/100 169/238.. Train loss: 3.387..Val loss: 3.623..Val Accuracy: 0.000..Time: 5.575128078460693\n",
            "Epoch 91/100 179/238.. Train loss: 3.510..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.494633674621582\n",
            "Epoch 91/100 189/238.. Train loss: 3.382..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.573790550231934\n",
            "Epoch 91/100 199/238.. Train loss: 3.456..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.529265403747559\n",
            "Epoch 91/100 209/238.. Train loss: 3.454..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.581499814987183\n",
            "Epoch 91/100 219/238.. Train loss: 3.444..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.553778409957886\n",
            "Epoch 91/100 229/238.. Train loss: 3.399..Val loss: 3.582..Val Accuracy: 0.000..Time: 5.555892467498779\n",
            "Epoch 92/100 9/238.. Train loss: 3.378..Val loss: 3.602..Val Accuracy: 0.000..Time: 6.299774408340454\n",
            "Epoch 92/100 19/238.. Train loss: 3.396..Val loss: 3.593..Val Accuracy: 0.000..Time: 5.580979824066162\n",
            "Epoch 92/100 29/238.. Train loss: 3.407..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.5249762535095215\n",
            "Epoch 92/100 39/238.. Train loss: 3.447..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.723029613494873\n",
            "Epoch 92/100 49/238.. Train loss: 3.394..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.683559894561768\n",
            "Epoch 92/100 59/238.. Train loss: 3.305..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.67788290977478\n",
            "Epoch 92/100 69/238.. Train loss: 3.389..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.490100622177124\n",
            "Epoch 92/100 79/238.. Train loss: 3.433..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.562446117401123\n",
            "Epoch 92/100 89/238.. Train loss: 3.408..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.546236038208008\n",
            "Epoch 92/100 99/238.. Train loss: 3.435..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.572973966598511\n",
            "Epoch 92/100 109/238.. Train loss: 3.454..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.581225156784058\n",
            "Epoch 92/100 119/238.. Train loss: 3.369..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.5543694496154785\n",
            "Epoch 92/100 129/238.. Train loss: 3.454..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.598743200302124\n",
            "Epoch 92/100 139/238.. Train loss: 3.400..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.483916521072388\n",
            "Epoch 92/100 149/238.. Train loss: 3.408..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.562906742095947\n",
            "Epoch 92/100 159/238.. Train loss: 3.376..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.51834511756897\n",
            "Epoch 92/100 169/238.. Train loss: 3.404..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.5569703578948975\n",
            "Epoch 92/100 179/238.. Train loss: 3.466..Val loss: 3.589..Val Accuracy: 0.000..Time: 5.542790412902832\n",
            "Epoch 92/100 189/238.. Train loss: 3.416..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.616095781326294\n",
            "Epoch 92/100 199/238.. Train loss: 3.421..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.5877649784088135\n",
            "Epoch 92/100 209/238.. Train loss: 3.418..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.532767057418823\n",
            "Epoch 92/100 219/238.. Train loss: 3.379..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.599104404449463\n",
            "Epoch 92/100 229/238.. Train loss: 3.305..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.48955774307251\n",
            "Epoch 93/100 9/238.. Train loss: 3.468..Val loss: 3.602..Val Accuracy: 0.000..Time: 6.30190372467041\n",
            "Epoch 93/100 19/238.. Train loss: 3.418..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.521775007247925\n",
            "Epoch 93/100 29/238.. Train loss: 3.377..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.5272345542907715\n",
            "Epoch 93/100 39/238.. Train loss: 3.399..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.54212760925293\n",
            "Epoch 93/100 49/238.. Train loss: 3.350..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.52581262588501\n",
            "Epoch 93/100 59/238.. Train loss: 3.384..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.573549747467041\n",
            "Epoch 93/100 69/238.. Train loss: 3.323..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.572192668914795\n",
            "Epoch 93/100 79/238.. Train loss: 3.423..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.526632070541382\n",
            "Epoch 93/100 89/238.. Train loss: 3.416..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.591708660125732\n",
            "Epoch 93/100 99/238.. Train loss: 3.392..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.59426736831665\n",
            "Epoch 93/100 109/238.. Train loss: 3.415..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.572272300720215\n",
            "Epoch 93/100 119/238.. Train loss: 3.405..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.538604259490967\n",
            "Epoch 93/100 129/238.. Train loss: 3.483..Val loss: 3.591..Val Accuracy: 0.000..Time: 5.557132959365845\n",
            "Epoch 93/100 139/238.. Train loss: 3.396..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.524082899093628\n",
            "Epoch 93/100 149/238.. Train loss: 3.365..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.600221872329712\n",
            "Epoch 93/100 159/238.. Train loss: 3.408..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.587551832199097\n",
            "Epoch 93/100 169/238.. Train loss: 3.405..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.50683856010437\n",
            "Epoch 93/100 179/238.. Train loss: 3.417..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.538357257843018\n",
            "Epoch 93/100 189/238.. Train loss: 3.415..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.48119044303894\n",
            "Epoch 93/100 199/238.. Train loss: 3.415..Val loss: 3.592..Val Accuracy: 0.000..Time: 5.5863118171691895\n",
            "Epoch 93/100 209/238.. Train loss: 3.470..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.5155205726623535\n",
            "Epoch 93/100 219/238.. Train loss: 3.374..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.57926344871521\n",
            "Epoch 93/100 229/238.. Train loss: 3.339..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.503298997879028\n",
            "Epoch 94/100 9/238.. Train loss: 3.463..Val loss: 3.603..Val Accuracy: 0.000..Time: 6.415043592453003\n",
            "Epoch 94/100 19/238.. Train loss: 3.376..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.609599351882935\n",
            "Epoch 94/100 29/238.. Train loss: 3.378..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.587104320526123\n",
            "Epoch 94/100 39/238.. Train loss: 3.429..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.529247045516968\n",
            "Epoch 94/100 49/238.. Train loss: 3.409..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.581411600112915\n",
            "Epoch 94/100 59/238.. Train loss: 3.381..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.54107403755188\n",
            "Epoch 94/100 69/238.. Train loss: 3.388..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.547331094741821\n",
            "Epoch 94/100 79/238.. Train loss: 3.442..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.55388617515564\n",
            "Epoch 94/100 89/238.. Train loss: 3.430..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.595642566680908\n",
            "Epoch 94/100 99/238.. Train loss: 3.420..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.543686389923096\n",
            "Epoch 94/100 109/238.. Train loss: 3.413..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.543154954910278\n",
            "Epoch 94/100 119/238.. Train loss: 3.493..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.564114809036255\n",
            "Epoch 94/100 129/238.. Train loss: 3.436..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.5628862380981445\n",
            "Epoch 94/100 139/238.. Train loss: 3.438..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.6989216804504395\n",
            "Epoch 94/100 149/238.. Train loss: 3.399..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.641701698303223\n",
            "Epoch 94/100 159/238.. Train loss: 3.349..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.67282247543335\n",
            "Epoch 94/100 169/238.. Train loss: 3.350..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.567052364349365\n",
            "Epoch 94/100 179/238.. Train loss: 3.330..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.584028482437134\n",
            "Epoch 94/100 189/238.. Train loss: 3.348..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.5577263832092285\n",
            "Epoch 94/100 199/238.. Train loss: 3.465..Val loss: 3.620..Val Accuracy: 0.000..Time: 5.573084592819214\n",
            "Epoch 94/100 209/238.. Train loss: 3.411..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.668671607971191\n",
            "Epoch 94/100 219/238.. Train loss: 3.288..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.551535606384277\n",
            "Epoch 94/100 229/238.. Train loss: 3.423..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.557603597640991\n",
            "Epoch 95/100 9/238.. Train loss: 3.354..Val loss: 3.610..Val Accuracy: 0.000..Time: 6.297250270843506\n",
            "Epoch 95/100 19/238.. Train loss: 3.434..Val loss: 3.613..Val Accuracy: 0.000..Time: 5.567290782928467\n",
            "Epoch 95/100 29/238.. Train loss: 3.462..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.60303258895874\n",
            "Epoch 95/100 39/238.. Train loss: 3.399..Val loss: 3.588..Val Accuracy: 0.000..Time: 5.577878713607788\n",
            "Epoch 95/100 49/238.. Train loss: 3.423..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.616556167602539\n",
            "Epoch 95/100 59/238.. Train loss: 3.351..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.589805364608765\n",
            "Epoch 95/100 69/238.. Train loss: 3.446..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.686539173126221\n",
            "Epoch 95/100 79/238.. Train loss: 3.344..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.63585901260376\n",
            "Epoch 95/100 89/238.. Train loss: 3.424..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.5511908531188965\n",
            "Epoch 95/100 99/238.. Train loss: 3.422..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.599527597427368\n",
            "Epoch 95/100 109/238.. Train loss: 3.363..Val loss: 3.615..Val Accuracy: 0.000..Time: 5.582948684692383\n",
            "Epoch 95/100 119/238.. Train loss: 3.441..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.631237030029297\n",
            "Epoch 95/100 129/238.. Train loss: 3.415..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.575794219970703\n",
            "Epoch 95/100 139/238.. Train loss: 3.392..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.574709177017212\n",
            "Epoch 95/100 149/238.. Train loss: 3.413..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.598841190338135\n",
            "Epoch 95/100 159/238.. Train loss: 3.350..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.53244423866272\n",
            "Epoch 95/100 169/238.. Train loss: 3.378..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.6295082569122314\n",
            "Epoch 95/100 179/238.. Train loss: 3.371..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.549408197402954\n",
            "Epoch 95/100 189/238.. Train loss: 3.467..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.6666364669799805\n",
            "Epoch 95/100 199/238.. Train loss: 3.357..Val loss: 3.586..Val Accuracy: 0.000..Time: 5.544946908950806\n",
            "Epoch 95/100 209/238.. Train loss: 3.341..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.5804643630981445\n",
            "Epoch 95/100 219/238.. Train loss: 3.484..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.572803020477295\n",
            "Epoch 95/100 229/238.. Train loss: 3.402..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.547162294387817\n",
            "Epoch 96/100 9/238.. Train loss: 3.303..Val loss: 3.611..Val Accuracy: 0.000..Time: 6.304550647735596\n",
            "Epoch 96/100 19/238.. Train loss: 3.431..Val loss: 3.616..Val Accuracy: 0.000..Time: 5.569626569747925\n",
            "Epoch 96/100 29/238.. Train loss: 3.451..Val loss: 3.611..Val Accuracy: 0.000..Time: 5.562812566757202\n",
            "Epoch 96/100 39/238.. Train loss: 3.345..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.554161787033081\n",
            "Epoch 96/100 49/238.. Train loss: 3.473..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.5296502113342285\n",
            "Epoch 96/100 59/238.. Train loss: 3.448..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.541410684585571\n",
            "Epoch 96/100 69/238.. Train loss: 3.406..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.553767204284668\n",
            "Epoch 96/100 79/238.. Train loss: 3.486..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.56114387512207\n",
            "Epoch 96/100 89/238.. Train loss: 3.377..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.545219659805298\n",
            "Epoch 96/100 99/238.. Train loss: 3.460..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.568509101867676\n",
            "Epoch 96/100 109/238.. Train loss: 3.355..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.6234235763549805\n",
            "Epoch 96/100 119/238.. Train loss: 3.378..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.590527772903442\n",
            "Epoch 96/100 129/238.. Train loss: 3.443..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.615706205368042\n",
            "Epoch 96/100 139/238.. Train loss: 3.289..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.577134132385254\n",
            "Epoch 96/100 149/238.. Train loss: 3.389..Val loss: 3.611..Val Accuracy: 0.000..Time: 5.5760838985443115\n",
            "Epoch 96/100 159/238.. Train loss: 3.464..Val loss: 3.613..Val Accuracy: 0.000..Time: 5.585968255996704\n",
            "Epoch 96/100 169/238.. Train loss: 3.355..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.577421426773071\n",
            "Epoch 96/100 179/238.. Train loss: 3.429..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.5434534549713135\n",
            "Epoch 96/100 189/238.. Train loss: 3.331..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.5720484256744385\n",
            "Epoch 96/100 199/238.. Train loss: 3.477..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.6001482009887695\n",
            "Epoch 96/100 209/238.. Train loss: 3.329..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.535391330718994\n",
            "Epoch 96/100 219/238.. Train loss: 3.456..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.614461898803711\n",
            "Epoch 96/100 229/238.. Train loss: 3.398..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.522229909896851\n",
            "Epoch 97/100 9/238.. Train loss: 3.378..Val loss: 3.605..Val Accuracy: 0.000..Time: 6.459853887557983\n",
            "Epoch 97/100 19/238.. Train loss: 3.441..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.673396110534668\n",
            "Epoch 97/100 29/238.. Train loss: 3.353..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.53345513343811\n",
            "Epoch 97/100 39/238.. Train loss: 3.386..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.6299145221710205\n",
            "Epoch 97/100 49/238.. Train loss: 3.396..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.544865369796753\n",
            "Epoch 97/100 59/238.. Train loss: 3.395..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.643529891967773\n",
            "Epoch 97/100 69/238.. Train loss: 3.313..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.606376886367798\n",
            "Epoch 97/100 79/238.. Train loss: 3.384..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.514195203781128\n",
            "Epoch 97/100 89/238.. Train loss: 3.415..Val loss: 3.616..Val Accuracy: 0.000..Time: 5.571976184844971\n",
            "Epoch 97/100 99/238.. Train loss: 3.371..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.516194820404053\n",
            "Epoch 97/100 109/238.. Train loss: 3.417..Val loss: 3.613..Val Accuracy: 0.000..Time: 5.557006120681763\n",
            "Epoch 97/100 119/238.. Train loss: 3.429..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.5056703090667725\n",
            "Epoch 97/100 129/238.. Train loss: 3.402..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.566203355789185\n",
            "Epoch 97/100 139/238.. Train loss: 3.438..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.583459138870239\n",
            "Epoch 97/100 149/238.. Train loss: 3.476..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.583051919937134\n",
            "Epoch 97/100 159/238.. Train loss: 3.494..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.583506345748901\n",
            "Epoch 97/100 169/238.. Train loss: 3.393..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.508095979690552\n",
            "Epoch 97/100 179/238.. Train loss: 3.399..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.581974744796753\n",
            "Epoch 97/100 189/238.. Train loss: 3.411..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.5809242725372314\n",
            "Epoch 97/100 199/238.. Train loss: 3.372..Val loss: 3.593..Val Accuracy: 0.000..Time: 5.576890468597412\n",
            "Epoch 97/100 209/238.. Train loss: 3.358..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.527705192565918\n",
            "Epoch 97/100 219/238.. Train loss: 3.437..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.545214414596558\n",
            "Epoch 97/100 229/238.. Train loss: 3.394..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.526270627975464\n",
            "Epoch 98/100 9/238.. Train loss: 3.332..Val loss: 3.615..Val Accuracy: 0.000..Time: 6.286360025405884\n",
            "Epoch 98/100 19/238.. Train loss: 3.390..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.580481052398682\n",
            "Epoch 98/100 29/238.. Train loss: 3.337..Val loss: 3.614..Val Accuracy: 0.000..Time: 5.551929473876953\n",
            "Epoch 98/100 39/238.. Train loss: 3.454..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.523699522018433\n",
            "Epoch 98/100 49/238.. Train loss: 3.339..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.5898261070251465\n",
            "Epoch 98/100 59/238.. Train loss: 3.377..Val loss: 3.625..Val Accuracy: 0.000..Time: 5.506305456161499\n",
            "Epoch 98/100 69/238.. Train loss: 3.407..Val loss: 3.612..Val Accuracy: 0.000..Time: 5.634671688079834\n",
            "Epoch 98/100 79/238.. Train loss: 3.419..Val loss: 3.611..Val Accuracy: 0.000..Time: 5.543050050735474\n",
            "Epoch 98/100 89/238.. Train loss: 3.402..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.586030960083008\n",
            "Epoch 98/100 99/238.. Train loss: 3.381..Val loss: 3.612..Val Accuracy: 0.000..Time: 5.555791616439819\n",
            "Epoch 98/100 109/238.. Train loss: 3.426..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.603904485702515\n",
            "Epoch 98/100 119/238.. Train loss: 3.427..Val loss: 3.609..Val Accuracy: 0.000..Time: 5.56909966468811\n",
            "Epoch 98/100 129/238.. Train loss: 3.405..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.538854360580444\n",
            "Epoch 98/100 139/238.. Train loss: 3.449..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.519740343093872\n",
            "Epoch 98/100 149/238.. Train loss: 3.380..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.5167059898376465\n",
            "Epoch 98/100 159/238.. Train loss: 3.480..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.578568935394287\n",
            "Epoch 98/100 169/238.. Train loss: 3.384..Val loss: 3.592..Val Accuracy: 0.000..Time: 5.531383037567139\n",
            "Epoch 98/100 179/238.. Train loss: 3.416..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.601520299911499\n",
            "Epoch 98/100 189/238.. Train loss: 3.417..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.542259216308594\n",
            "Epoch 98/100 199/238.. Train loss: 3.470..Val loss: 3.588..Val Accuracy: 0.000..Time: 5.585388422012329\n",
            "Epoch 98/100 209/238.. Train loss: 3.410..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.6248390674591064\n",
            "Epoch 98/100 219/238.. Train loss: 3.369..Val loss: 3.594..Val Accuracy: 0.000..Time: 5.576613664627075\n",
            "Epoch 98/100 229/238.. Train loss: 3.378..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.648325681686401\n",
            "Epoch 99/100 9/238.. Train loss: 3.417..Val loss: 3.592..Val Accuracy: 0.000..Time: 6.489255666732788\n",
            "Epoch 99/100 19/238.. Train loss: 3.416..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.655464172363281\n",
            "Epoch 99/100 29/238.. Train loss: 3.420..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.651780605316162\n",
            "Epoch 99/100 39/238.. Train loss: 3.400..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.564744710922241\n",
            "Epoch 99/100 49/238.. Train loss: 3.382..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.61696982383728\n",
            "Epoch 99/100 59/238.. Train loss: 3.387..Val loss: 3.595..Val Accuracy: 0.000..Time: 5.5651960372924805\n",
            "Epoch 99/100 69/238.. Train loss: 3.354..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.632396221160889\n",
            "Epoch 99/100 79/238.. Train loss: 3.309..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.628420829772949\n",
            "Epoch 99/100 89/238.. Train loss: 3.518..Val loss: 3.612..Val Accuracy: 0.000..Time: 5.567192792892456\n",
            "Epoch 99/100 99/238.. Train loss: 3.325..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.7609875202178955\n",
            "Epoch 99/100 109/238.. Train loss: 3.308..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.646280527114868\n",
            "Epoch 99/100 119/238.. Train loss: 3.369..Val loss: 3.605..Val Accuracy: 0.000..Time: 5.6220152378082275\n",
            "Epoch 99/100 129/238.. Train loss: 3.391..Val loss: 3.616..Val Accuracy: 0.000..Time: 5.513202667236328\n",
            "Epoch 99/100 139/238.. Train loss: 3.454..Val loss: 3.615..Val Accuracy: 0.000..Time: 5.614121675491333\n",
            "Epoch 99/100 149/238.. Train loss: 3.481..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.63956093788147\n",
            "Epoch 99/100 159/238.. Train loss: 3.408..Val loss: 3.602..Val Accuracy: 0.000..Time: 5.656785488128662\n",
            "Epoch 99/100 169/238.. Train loss: 3.363..Val loss: 3.599..Val Accuracy: 0.000..Time: 5.62856125831604\n",
            "Epoch 99/100 179/238.. Train loss: 3.472..Val loss: 3.592..Val Accuracy: 0.000..Time: 5.554724216461182\n",
            "Epoch 99/100 189/238.. Train loss: 3.441..Val loss: 3.593..Val Accuracy: 0.000..Time: 5.569965124130249\n",
            "Epoch 99/100 199/238.. Train loss: 3.372..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.579332113265991\n",
            "Epoch 99/100 209/238.. Train loss: 3.495..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.579547882080078\n",
            "Epoch 99/100 219/238.. Train loss: 3.333..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.555664539337158\n",
            "Epoch 99/100 229/238.. Train loss: 3.384..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.594043493270874\n",
            "Epoch 100/100 9/238.. Train loss: 3.438..Val loss: 3.605..Val Accuracy: 0.000..Time: 6.33291220664978\n",
            "Epoch 100/100 19/238.. Train loss: 3.417..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.582298994064331\n",
            "Epoch 100/100 29/238.. Train loss: 3.381..Val loss: 3.604..Val Accuracy: 0.000..Time: 5.645225286483765\n",
            "Epoch 100/100 39/238.. Train loss: 3.430..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.628113031387329\n",
            "Epoch 100/100 49/238.. Train loss: 3.334..Val loss: 3.603..Val Accuracy: 0.000..Time: 5.532641649246216\n",
            "Epoch 100/100 59/238.. Train loss: 3.470..Val loss: 3.608..Val Accuracy: 0.000..Time: 5.653894901275635\n",
            "Epoch 100/100 69/238.. Train loss: 3.457..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.566514492034912\n",
            "Epoch 100/100 79/238.. Train loss: 3.450..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.600411653518677\n",
            "Epoch 100/100 89/238.. Train loss: 3.395..Val loss: 3.589..Val Accuracy: 0.000..Time: 5.538350582122803\n",
            "Epoch 100/100 99/238.. Train loss: 3.373..Val loss: 3.596..Val Accuracy: 0.000..Time: 5.605011701583862\n",
            "Epoch 100/100 109/238.. Train loss: 3.460..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.574192523956299\n",
            "Epoch 100/100 119/238.. Train loss: 3.308..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.563990592956543\n",
            "Epoch 100/100 129/238.. Train loss: 3.342..Val loss: 3.601..Val Accuracy: 0.000..Time: 5.585124731063843\n",
            "Epoch 100/100 139/238.. Train loss: 3.458..Val loss: 3.614..Val Accuracy: 0.000..Time: 5.495338678359985\n",
            "Epoch 100/100 149/238.. Train loss: 3.409..Val loss: 3.612..Val Accuracy: 0.000..Time: 5.59952712059021\n",
            "Epoch 100/100 159/238.. Train loss: 3.383..Val loss: 3.606..Val Accuracy: 0.000..Time: 5.523090600967407\n",
            "Epoch 100/100 169/238.. Train loss: 3.354..Val loss: 3.612..Val Accuracy: 0.000..Time: 5.592541456222534\n",
            "Epoch 100/100 179/238.. Train loss: 3.413..Val loss: 3.607..Val Accuracy: 0.000..Time: 5.51892614364624\n",
            "Epoch 100/100 189/238.. Train loss: 3.430..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.6018970012664795\n",
            "Epoch 100/100 199/238.. Train loss: 3.394..Val loss: 3.597..Val Accuracy: 0.000..Time: 5.560779333114624\n",
            "Epoch 100/100 209/238.. Train loss: 3.385..Val loss: 3.598..Val Accuracy: 0.000..Time: 5.659875154495239\n",
            "Epoch 100/100 219/238.. Train loss: 3.369..Val loss: 3.610..Val Accuracy: 0.000..Time: 5.598342657089233\n",
            "Epoch 100/100 229/238.. Train loss: 3.399..Val loss: 3.600..Val Accuracy: 0.000..Time: 5.529493570327759\n",
            "training finish! training time is 0.8109636306762695\n",
            "<generator object Module.parameters at 0x7fade1fd7e08>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhEh_CNKK9yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f70583d5-37ee-49f5-98b8-14b2652bcd5b"
      },
      "source": [
        "import torch \n",
        "\n",
        "a = torch.tensor([1, 2, 3, 1])\n",
        "b = torch.tensor([2, 2, 1, 1])\n",
        "print(torch.sum(a == b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOMC8OhoXY8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "21be92f1-6bcf-4b4d-deb1-3b7f6634e1da"
      },
      "source": [
        "model = nn.Sequential(Baseline(), Classification(40)).to(device)\n",
        "model.load_state_dict(torch.load('/content/modelpath.pth'))\n",
        "model.eval()\n",
        "\n",
        "for param in model.parameters():\n",
        "  print(param.size(), param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 1]) Parameter containing:\n",
            "tensor([[[-0.0778],\n",
            "         [-0.5144],\n",
            "         [-0.4890]],\n",
            "\n",
            "        [[-0.2123],\n",
            "         [-0.0484],\n",
            "         [ 0.1117]],\n",
            "\n",
            "        [[ 0.3344],\n",
            "         [-0.3609],\n",
            "         [ 0.4029]],\n",
            "\n",
            "        [[-0.2175],\n",
            "         [ 0.1201],\n",
            "         [ 0.6253]],\n",
            "\n",
            "        [[-0.5256],\n",
            "         [-0.4006],\n",
            "         [-0.1874]],\n",
            "\n",
            "        [[-0.4075],\n",
            "         [-0.5153],\n",
            "         [-0.3382]],\n",
            "\n",
            "        [[-0.0083],\n",
            "         [-0.5175],\n",
            "         [-0.4295]],\n",
            "\n",
            "        [[-0.4127],\n",
            "         [-0.2899],\n",
            "         [ 0.3528]],\n",
            "\n",
            "        [[ 0.0796],\n",
            "         [ 0.5573],\n",
            "         [-0.1775]],\n",
            "\n",
            "        [[-0.1548],\n",
            "         [-0.1058],\n",
            "         [ 0.5159]],\n",
            "\n",
            "        [[-0.0103],\n",
            "         [ 0.2960],\n",
            "         [-0.3208]],\n",
            "\n",
            "        [[ 0.1083],\n",
            "         [-0.2178],\n",
            "         [-0.2965]],\n",
            "\n",
            "        [[ 0.2534],\n",
            "         [ 0.0459],\n",
            "         [-0.4102]],\n",
            "\n",
            "        [[-0.2943],\n",
            "         [-0.2182],\n",
            "         [ 0.3456]],\n",
            "\n",
            "        [[-0.0425],\n",
            "         [-0.5693],\n",
            "         [-0.4695]],\n",
            "\n",
            "        [[-0.5755],\n",
            "         [ 0.1578],\n",
            "         [ 0.2771]],\n",
            "\n",
            "        [[ 0.3056],\n",
            "         [ 0.3222],\n",
            "         [ 0.0802]],\n",
            "\n",
            "        [[-0.5666],\n",
            "         [ 0.4303],\n",
            "         [ 0.2770]],\n",
            "\n",
            "        [[ 0.1845],\n",
            "         [-0.1097],\n",
            "         [ 0.4534]],\n",
            "\n",
            "        [[-0.2435],\n",
            "         [-0.0554],\n",
            "         [ 0.3375]],\n",
            "\n",
            "        [[-0.4873],\n",
            "         [-0.2998],\n",
            "         [-0.3571]],\n",
            "\n",
            "        [[ 0.4219],\n",
            "         [-0.5705],\n",
            "         [ 0.4857]],\n",
            "\n",
            "        [[-0.3868],\n",
            "         [-0.0142],\n",
            "         [ 0.4414]],\n",
            "\n",
            "        [[-0.1792],\n",
            "         [-0.1664],\n",
            "         [-0.5797]],\n",
            "\n",
            "        [[-0.2985],\n",
            "         [-0.3841],\n",
            "         [ 0.1349]],\n",
            "\n",
            "        [[ 0.0599],\n",
            "         [-0.4711],\n",
            "         [-0.1111]],\n",
            "\n",
            "        [[ 0.0914],\n",
            "         [ 0.0103],\n",
            "         [-0.4581]],\n",
            "\n",
            "        [[ 0.4145],\n",
            "         [-0.3091],\n",
            "         [-0.0294]],\n",
            "\n",
            "        [[-0.3942],\n",
            "         [ 0.4620],\n",
            "         [ 0.2121]],\n",
            "\n",
            "        [[ 0.0342],\n",
            "         [ 0.1671],\n",
            "         [-0.0641]],\n",
            "\n",
            "        [[-0.1124],\n",
            "         [-0.1171],\n",
            "         [-0.1989]],\n",
            "\n",
            "        [[ 0.4175],\n",
            "         [-0.0456],\n",
            "         [ 0.3257]],\n",
            "\n",
            "        [[ 0.1362],\n",
            "         [ 0.5174],\n",
            "         [ 0.6372]],\n",
            "\n",
            "        [[-0.1882],\n",
            "         [-0.0297],\n",
            "         [ 0.5284]],\n",
            "\n",
            "        [[ 0.0124],\n",
            "         [-0.0360],\n",
            "         [ 0.1132]],\n",
            "\n",
            "        [[ 0.1091],\n",
            "         [ 0.4740],\n",
            "         [-0.0224]],\n",
            "\n",
            "        [[ 0.0495],\n",
            "         [ 0.3472],\n",
            "         [ 0.5604]],\n",
            "\n",
            "        [[ 0.1724],\n",
            "         [-0.4390],\n",
            "         [-0.1495]],\n",
            "\n",
            "        [[-0.0521],\n",
            "         [ 0.4711],\n",
            "         [ 0.0833]],\n",
            "\n",
            "        [[ 0.4772],\n",
            "         [ 0.1567],\n",
            "         [ 0.1525]],\n",
            "\n",
            "        [[ 0.3516],\n",
            "         [-0.5146],\n",
            "         [ 0.4343]],\n",
            "\n",
            "        [[ 0.1452],\n",
            "         [-0.1555],\n",
            "         [-0.1052]],\n",
            "\n",
            "        [[-0.5099],\n",
            "         [-0.2939],\n",
            "         [-0.1628]],\n",
            "\n",
            "        [[-0.3957],\n",
            "         [-0.5245],\n",
            "         [ 0.1905]],\n",
            "\n",
            "        [[ 0.4928],\n",
            "         [ 0.0138],\n",
            "         [ 0.0589]],\n",
            "\n",
            "        [[-0.1686],\n",
            "         [-0.2868],\n",
            "         [-0.2923]],\n",
            "\n",
            "        [[ 0.2123],\n",
            "         [-0.2960],\n",
            "         [-0.4098]],\n",
            "\n",
            "        [[ 0.2364],\n",
            "         [ 0.3670],\n",
            "         [ 0.2808]],\n",
            "\n",
            "        [[ 0.3343],\n",
            "         [ 0.3034],\n",
            "         [-0.5662]],\n",
            "\n",
            "        [[-0.3909],\n",
            "         [-0.0576],\n",
            "         [-0.2299]],\n",
            "\n",
            "        [[ 0.5352],\n",
            "         [ 0.1013],\n",
            "         [ 0.4339]],\n",
            "\n",
            "        [[-0.2299],\n",
            "         [ 0.3609],\n",
            "         [-0.6124]],\n",
            "\n",
            "        [[-0.3551],\n",
            "         [ 0.0428],\n",
            "         [-0.5685]],\n",
            "\n",
            "        [[ 0.5221],\n",
            "         [ 0.1694],\n",
            "         [-0.4030]],\n",
            "\n",
            "        [[ 0.2909],\n",
            "         [-0.2932],\n",
            "         [ 0.3024]],\n",
            "\n",
            "        [[ 0.3802],\n",
            "         [ 0.2713],\n",
            "         [ 0.0626]],\n",
            "\n",
            "        [[-0.3995],\n",
            "         [-0.3820],\n",
            "         [ 0.3714]],\n",
            "\n",
            "        [[-0.2141],\n",
            "         [-0.1747],\n",
            "         [-0.2974]],\n",
            "\n",
            "        [[ 0.5823],\n",
            "         [-0.1455],\n",
            "         [-0.3501]],\n",
            "\n",
            "        [[ 0.3389],\n",
            "         [ 0.0348],\n",
            "         [ 0.3382]],\n",
            "\n",
            "        [[-0.5456],\n",
            "         [-0.2298],\n",
            "         [ 0.1085]],\n",
            "\n",
            "        [[ 0.6288],\n",
            "         [-0.0481],\n",
            "         [-0.3385]],\n",
            "\n",
            "        [[-0.4684],\n",
            "         [-0.3784],\n",
            "         [-0.0181]],\n",
            "\n",
            "        [[-0.1140],\n",
            "         [ 0.0525],\n",
            "         [-0.3846]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([-0.0312,  0.3175, -0.5425, -0.2756,  0.3302,  0.1329, -0.2848,  0.5359,\n",
            "        -0.1241, -0.1320,  0.2535,  0.1512,  0.1609, -0.4033, -0.5006, -0.2806,\n",
            "         0.3078, -0.3691,  0.3309, -0.3653,  0.1597,  0.0939, -0.2176,  0.1197,\n",
            "        -0.5463,  0.4941,  0.4748,  0.0247,  0.1473,  0.2930,  0.4246,  0.2240,\n",
            "         0.5659,  0.3634,  0.1675,  0.1439, -0.1266, -0.0404, -0.4524, -0.3217,\n",
            "         0.0110, -0.3550,  0.3807, -0.1712,  0.2603,  0.3187, -0.1304,  0.5402,\n",
            "         0.2334, -0.4773,  0.0466,  0.2056,  0.4565, -0.1604, -0.4735,  0.3192,\n",
            "        -0.3125,  0.3858,  0.1096, -0.4177, -0.3979,  0.0129,  0.0077,  0.4956],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([0.9641, 0.9819, 1.0646, 0.9276, 0.9901, 1.0205, 0.9732, 1.0165, 0.9998,\n",
            "        1.0319, 1.0692, 1.0004, 0.9609, 1.0147, 1.0418, 0.9633, 0.9407, 1.0093,\n",
            "        1.0474, 0.9567, 1.0076, 0.9843, 0.9680, 1.0308, 0.9961, 1.0201, 0.9721,\n",
            "        0.9206, 0.9637, 1.0166, 1.0452, 1.0461, 0.9774, 1.0187, 1.0398, 1.0727,\n",
            "        1.0182, 1.0088, 0.9905, 0.9694, 0.9504, 1.0095, 0.9610, 0.9699, 0.9785,\n",
            "        1.0265, 0.9768, 1.0176, 0.9997, 0.9639, 0.9906, 1.0093, 1.0218, 0.9692,\n",
            "        1.1014, 1.0071, 1.0239, 1.0379, 0.9666, 1.0287, 0.9961, 1.0395, 1.0042,\n",
            "        1.0121], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([ 0.0249, -0.0116, -0.0666,  0.0269,  0.0253, -0.0110, -0.0141, -0.0002,\n",
            "         0.0917,  0.0147,  0.0563, -0.0463, -0.0203,  0.0325, -0.0492, -0.0082,\n",
            "        -0.0017,  0.0292, -0.0691,  0.0446,  0.0660, -0.0306,  0.0235,  0.0317,\n",
            "        -0.0397,  0.0950, -0.0046, -0.0646,  0.0063,  0.1090,  0.0325, -0.0973,\n",
            "        -0.0355,  0.0070, -0.0014,  0.0757,  0.0421,  0.0664,  0.0083, -0.0523,\n",
            "        -0.0295, -0.0288,  0.0175, -0.0655, -0.0556, -0.0110, -0.0067,  0.0125,\n",
            "        -0.0617,  0.0035, -0.0954,  0.1098,  0.0824,  0.0361, -0.0350,  0.0751,\n",
            "         0.0197,  0.0060, -0.0326, -0.0606, -0.0197,  0.0650,  0.0141, -0.0155],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([128, 64, 1]) Parameter containing:\n",
            "tensor([[[-0.0444],\n",
            "         [-0.0958],\n",
            "         [ 0.1992],\n",
            "         ...,\n",
            "         [ 0.0589],\n",
            "         [-0.0588],\n",
            "         [-0.0559]],\n",
            "\n",
            "        [[ 0.1011],\n",
            "         [ 0.0962],\n",
            "         [-0.0200],\n",
            "         ...,\n",
            "         [-0.1503],\n",
            "         [-0.0272],\n",
            "         [-0.0714]],\n",
            "\n",
            "        [[-0.0477],\n",
            "         [-0.0119],\n",
            "         [-0.0355],\n",
            "         ...,\n",
            "         [-0.0064],\n",
            "         [ 0.0509],\n",
            "         [ 0.0108]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0406],\n",
            "         [ 0.1108],\n",
            "         [-0.1083],\n",
            "         ...,\n",
            "         [ 0.0216],\n",
            "         [ 0.1081],\n",
            "         [-0.0158]],\n",
            "\n",
            "        [[-0.0795],\n",
            "         [ 0.0308],\n",
            "         [ 0.1922],\n",
            "         ...,\n",
            "         [-0.0653],\n",
            "         [-0.0161],\n",
            "         [-0.0594]],\n",
            "\n",
            "        [[ 0.0623],\n",
            "         [ 0.0346],\n",
            "         [ 0.1631],\n",
            "         ...,\n",
            "         [-0.0012],\n",
            "         [-0.0680],\n",
            "         [-0.0439]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([-0.0187,  0.0553,  0.0771,  0.0168, -0.0432, -0.0623, -0.0705, -0.0837,\n",
            "        -0.0948, -0.0568, -0.0560, -0.1008, -0.0342, -0.1172, -0.0367, -0.0379,\n",
            "         0.0324, -0.0144, -0.0201,  0.0188, -0.0235,  0.0232,  0.0141,  0.1212,\n",
            "        -0.1007, -0.0545, -0.0554, -0.0678, -0.1005, -0.1046,  0.0060,  0.0941,\n",
            "        -0.1068,  0.0280, -0.0545,  0.0710,  0.0576,  0.1208, -0.0287,  0.1198,\n",
            "         0.0267, -0.0101, -0.0715, -0.0549, -0.0382, -0.0637,  0.0062, -0.0285,\n",
            "         0.0955,  0.0124, -0.1167,  0.1379, -0.0818,  0.1092, -0.0158, -0.1020,\n",
            "         0.0681,  0.0544,  0.1033,  0.0024, -0.1125,  0.0491, -0.0585, -0.1054,\n",
            "        -0.0804, -0.1099,  0.1043, -0.1162,  0.0291,  0.0990, -0.0197,  0.0320,\n",
            "        -0.0076, -0.0756,  0.1485, -0.0064,  0.0174, -0.0548,  0.0262,  0.0528,\n",
            "         0.1080, -0.0115,  0.0576, -0.0095, -0.0985,  0.0009, -0.0476, -0.0945,\n",
            "        -0.1305, -0.0263, -0.0848,  0.0666,  0.1006,  0.0033,  0.0251, -0.0985,\n",
            "         0.0956, -0.0358,  0.0864, -0.0324,  0.0651, -0.1238,  0.1175,  0.0319,\n",
            "        -0.0519, -0.0070,  0.1184, -0.0889,  0.0421,  0.0570, -0.0379,  0.0056,\n",
            "         0.0076,  0.0436,  0.0983, -0.0149,  0.0777, -0.0380, -0.0357, -0.0450,\n",
            "         0.0772, -0.1166,  0.0224, -0.0599,  0.0179, -0.0810, -0.1094,  0.0430],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([1.0353, 1.0033, 0.9685, 1.0075, 0.9989, 0.9747, 0.9584, 0.9583, 1.0522,\n",
            "        0.9908, 1.0111, 1.0026, 0.9977, 1.0380, 0.9597, 1.0346, 0.9803, 0.9785,\n",
            "        1.0782, 1.0544, 0.9886, 1.0089, 0.9779, 1.0436, 0.9322, 0.9996, 0.9751,\n",
            "        0.9797, 0.9969, 0.9961, 1.0324, 1.0445, 1.0540, 0.9705, 0.9890, 0.9661,\n",
            "        1.0177, 1.0593, 1.0265, 0.9933, 1.0796, 1.0141, 1.0354, 1.0045, 1.0186,\n",
            "        1.0048, 0.9574, 0.9566, 0.9710, 0.9528, 1.0021, 1.0235, 0.9746, 1.0053,\n",
            "        0.9414, 0.9702, 0.9650, 1.0488, 1.0264, 0.9166, 1.0284, 1.0233, 1.0534,\n",
            "        0.9359, 1.0163, 1.0651, 1.0449, 1.0305, 1.0328, 1.0382, 0.9676, 0.9763,\n",
            "        0.9949, 0.9873, 1.0004, 0.9553, 0.9845, 1.0652, 1.0142, 1.0423, 1.0007,\n",
            "        0.9773, 0.9618, 0.9608, 0.9449, 1.0471, 1.0172, 0.9750, 1.0066, 0.9579,\n",
            "        0.9951, 1.0769, 0.9861, 0.9697, 0.9681, 1.0755, 0.9752, 1.0508, 0.9983,\n",
            "        0.9614, 1.0261, 1.0057, 1.0188, 1.0088, 0.9832, 0.9199, 0.9975, 1.0127,\n",
            "        1.0056, 0.9781, 1.0024, 0.9633, 0.9810, 0.9819, 1.0416, 0.9580, 1.0090,\n",
            "        0.9610, 0.9896, 0.9996, 0.9945, 1.0496, 0.9492, 1.0038, 0.9507, 0.9657,\n",
            "        1.0753, 1.0090], device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([-0.0533,  0.0126, -0.0638, -0.0243, -0.0329, -0.0080,  0.0434, -0.0391,\n",
            "        -0.0653, -0.0062,  0.0171, -0.0057,  0.0123,  0.0090, -0.0069,  0.0286,\n",
            "        -0.0192,  0.0118, -0.0469, -0.0188,  0.0517, -0.0211,  0.0133,  0.0195,\n",
            "        -0.0821,  0.0302, -0.0194, -0.0702,  0.0219, -0.0071,  0.0062, -0.0286,\n",
            "         0.0366,  0.0302,  0.0448, -0.0374,  0.0480,  0.0195, -0.0201,  0.0209,\n",
            "        -0.0196,  0.0009,  0.0071, -0.0115,  0.0852, -0.0010,  0.0339,  0.0584,\n",
            "        -0.0123, -0.0174, -0.0249,  0.0219, -0.0470,  0.0154, -0.0211, -0.0159,\n",
            "        -0.0721, -0.0004, -0.0015,  0.0158,  0.0727,  0.0626,  0.0476, -0.0715,\n",
            "        -0.0009, -0.0785,  0.0465, -0.0034, -0.0451,  0.0258,  0.0049,  0.0633,\n",
            "        -0.0266,  0.0173, -0.0199,  0.0185, -0.0418,  0.0638, -0.0131, -0.0082,\n",
            "         0.0041, -0.0255,  0.0773,  0.0191,  0.0184,  0.0066, -0.0140, -0.0613,\n",
            "        -0.0072, -0.0652,  0.0428, -0.0369, -0.0057, -0.0291,  0.0372,  0.0038,\n",
            "         0.0277, -0.0006, -0.0531, -0.0460,  0.0438,  0.0141, -0.0570, -0.0443,\n",
            "         0.0161, -0.0738,  0.0547, -0.0150,  0.0242,  0.0072, -0.0122,  0.0097,\n",
            "         0.0733,  0.0155, -0.0117, -0.0097,  0.0032,  0.0021, -0.0507,  0.0160,\n",
            "        -0.0211,  0.0235, -0.0467,  0.0054, -0.0613, -0.0302, -0.0479, -0.0622],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024, 128, 1]) Parameter containing:\n",
            "tensor([[[-0.0727],\n",
            "         [ 0.0559],\n",
            "         [ 0.0043],\n",
            "         ...,\n",
            "         [ 0.0485],\n",
            "         [ 0.0977],\n",
            "         [ 0.0976]],\n",
            "\n",
            "        [[-0.0129],\n",
            "         [ 0.1011],\n",
            "         [ 0.0834],\n",
            "         ...,\n",
            "         [-0.0221],\n",
            "         [ 0.0207],\n",
            "         [-0.0568]],\n",
            "\n",
            "        [[-0.0553],\n",
            "         [-0.1012],\n",
            "         [-0.0171],\n",
            "         ...,\n",
            "         [ 0.0327],\n",
            "         [-0.0135],\n",
            "         [ 0.0115]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.1177],\n",
            "         [ 0.0848],\n",
            "         [ 0.0538],\n",
            "         ...,\n",
            "         [-0.0408],\n",
            "         [ 0.1031],\n",
            "         [-0.0433]],\n",
            "\n",
            "        [[-0.0694],\n",
            "         [ 0.0195],\n",
            "         [-0.0393],\n",
            "         ...,\n",
            "         [-0.0393],\n",
            "         [-0.0543],\n",
            "         [-0.0041]],\n",
            "\n",
            "        [[ 0.0267],\n",
            "         [ 0.0705],\n",
            "         [-0.0863],\n",
            "         ...,\n",
            "         [-0.0112],\n",
            "         [-0.0326],\n",
            "         [ 0.0425]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([-0.0827,  0.0635,  0.0766,  ...,  0.0109, -0.0080, -0.0645],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([0.9750, 0.9826, 1.0554,  ..., 1.0449, 0.9640, 0.9630], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([ 0.0046,  0.0017, -0.0102,  ...,  0.0059, -0.0055,  0.0081],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512, 1024]) Parameter containing:\n",
            "tensor([[-0.0278, -0.0143,  0.0161,  ..., -0.0109,  0.0085,  0.0064],\n",
            "        [ 0.0217, -0.0446, -0.0445,  ...,  0.0459,  0.0242,  0.0145],\n",
            "        [-0.0301, -0.0320, -0.0367,  ...,  0.0092,  0.0246, -0.0433],\n",
            "        ...,\n",
            "        [ 0.0217, -0.0117, -0.0061,  ..., -0.0592,  0.0385,  0.0213],\n",
            "        [-0.0134, -0.0066,  0.0419,  ...,  0.0001,  0.0185, -0.0348],\n",
            "        [-0.0280,  0.0740,  0.0594,  ...,  0.0458,  0.0003, -0.0095]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([ 1.7112e-03,  1.4678e-02, -1.4228e-03,  1.2168e-02, -9.3394e-03,\n",
            "         2.0578e-02, -5.2770e-04, -3.7996e-03, -2.8242e-03,  2.1922e-02,\n",
            "        -1.1611e-02, -1.8358e-02,  2.5334e-02, -3.0309e-02, -1.6752e-02,\n",
            "        -3.1100e-03, -2.7656e-02,  1.6320e-02,  2.5152e-03, -3.1573e-02,\n",
            "        -4.7123e-03,  1.1985e-02,  6.1990e-03,  1.3584e-02,  8.6319e-04,\n",
            "        -2.6072e-02,  1.5486e-02,  2.9146e-02, -2.5562e-02,  2.1407e-02,\n",
            "        -1.6937e-02,  2.9673e-02,  1.1655e-02, -3.0508e-02, -2.4837e-03,\n",
            "         7.0633e-04,  1.3997e-02,  2.8812e-02,  6.6498e-03,  1.8074e-02,\n",
            "        -2.8416e-02,  1.1429e-02, -1.0100e-02,  1.3045e-02, -7.8533e-03,\n",
            "         5.3517e-03,  1.2085e-02, -3.0540e-02,  3.3130e-03,  1.6605e-02,\n",
            "        -2.4977e-02,  1.5826e-02, -1.3202e-03,  9.0757e-04, -2.7353e-03,\n",
            "        -1.6628e-03,  2.4225e-03,  7.5539e-03,  1.0816e-02, -3.5524e-02,\n",
            "        -4.5509e-03,  1.5443e-02, -1.8717e-02,  1.5537e-02, -2.0297e-02,\n",
            "         8.1653e-03,  1.9774e-02, -4.2766e-03,  4.1534e-03, -7.6052e-03,\n",
            "        -7.9943e-03,  1.4799e-02, -6.7477e-03,  2.2822e-02,  1.1107e-02,\n",
            "         3.7804e-02, -2.0623e-03, -5.1167e-04, -2.9367e-02, -2.6204e-02,\n",
            "        -1.8833e-02,  2.3267e-02, -1.0762e-02, -8.8667e-03,  1.9185e-02,\n",
            "         1.2420e-02, -2.8624e-02,  2.3983e-02, -5.6100e-03, -2.7621e-02,\n",
            "        -2.4706e-02, -1.0337e-02, -1.1354e-02, -3.1986e-02,  2.2733e-02,\n",
            "         1.8310e-03,  2.3623e-02, -3.3549e-02, -2.7741e-02,  2.8359e-02,\n",
            "        -2.6853e-02,  5.1480e-03,  2.0061e-03, -6.6687e-03,  1.1071e-02,\n",
            "         3.8722e-02, -3.3066e-02,  1.1303e-02,  6.9908e-03,  9.0463e-03,\n",
            "         2.1330e-02, -2.4009e-02, -1.6782e-02,  1.0862e-02,  2.1132e-02,\n",
            "         1.0612e-02, -3.1564e-02, -1.0555e-02,  7.3434e-03,  3.2111e-04,\n",
            "        -2.8200e-02, -3.9283e-02,  2.1935e-02, -2.9485e-02, -8.6963e-03,\n",
            "         2.7468e-02, -4.1419e-03,  1.0807e-02, -2.1504e-02, -2.1291e-02,\n",
            "        -7.5983e-03,  7.8946e-03,  3.1782e-02,  2.7076e-02, -1.0413e-02,\n",
            "         1.6249e-02,  3.7870e-02,  1.6357e-03, -2.6114e-02,  7.7878e-04,\n",
            "         2.3962e-02, -4.4015e-03,  2.6993e-03,  1.5008e-03, -7.9192e-03,\n",
            "         5.6452e-03, -1.7085e-02,  9.7053e-03,  3.3125e-03,  1.8982e-02,\n",
            "         1.5935e-02, -2.8070e-02, -3.0843e-02,  7.8570e-03, -2.4557e-02,\n",
            "         3.2685e-02,  2.5410e-02, -2.1832e-03,  2.7995e-02,  2.8168e-02,\n",
            "         1.0629e-02,  6.7207e-03, -3.5051e-03, -2.3824e-02,  2.8875e-02,\n",
            "        -4.1456e-03, -2.1614e-02, -2.5465e-02, -1.3185e-03, -1.4548e-02,\n",
            "         1.3898e-02, -2.0026e-02, -1.8971e-02, -6.6898e-04,  6.8163e-03,\n",
            "        -8.1453e-04, -9.9750e-03, -2.8678e-02,  1.1148e-02,  1.8908e-02,\n",
            "         1.6989e-02, -2.4994e-02, -4.1163e-03,  1.3258e-02, -1.5382e-02,\n",
            "         2.5723e-03, -7.6945e-03,  8.9242e-03,  1.1566e-02, -2.5743e-02,\n",
            "        -2.4912e-02,  2.3128e-02, -2.1555e-02, -2.7933e-02, -9.3008e-03,\n",
            "         1.0746e-02,  2.5112e-02, -2.9848e-03,  2.0773e-02, -8.0675e-03,\n",
            "         2.7368e-02, -1.5051e-02,  1.9726e-02,  3.1581e-03, -2.8039e-02,\n",
            "         1.9083e-02, -6.5823e-03, -2.4321e-02,  5.3165e-03,  5.3517e-03,\n",
            "        -9.9564e-03, -2.2769e-02, -6.7177e-03,  2.1300e-02,  2.7092e-02,\n",
            "        -1.3367e-02,  2.4062e-02, -8.7421e-04, -1.5930e-02,  2.9433e-02,\n",
            "        -2.8826e-02,  2.5004e-02,  1.2111e-02, -1.1385e-02, -1.4960e-02,\n",
            "         1.2234e-02,  3.1402e-02,  5.1605e-05, -1.6139e-02, -1.9878e-02,\n",
            "        -3.2989e-02,  4.9299e-03,  1.5726e-03,  1.6472e-02, -2.3564e-02,\n",
            "        -2.1327e-02,  8.3859e-03, -1.1248e-02, -1.6592e-02,  1.7393e-02,\n",
            "         2.4437e-02, -1.6513e-02, -2.1569e-02,  1.5050e-02, -2.8036e-02,\n",
            "        -3.8305e-03, -7.3153e-03,  1.9694e-02, -2.3840e-03, -1.4760e-02,\n",
            "        -2.7259e-02, -1.0766e-02,  2.8299e-02, -1.2054e-02,  1.0275e-02,\n",
            "         1.1780e-02,  2.5600e-02, -2.1226e-02,  2.7242e-02,  1.7936e-02,\n",
            "         3.1353e-02, -1.6578e-02,  1.6534e-02,  2.2950e-02, -9.3768e-03,\n",
            "        -5.7127e-03,  4.1807e-04,  2.3125e-02,  2.5230e-02,  1.7781e-02,\n",
            "         2.2832e-02,  6.0515e-03,  1.8609e-02, -3.1224e-02, -1.6052e-02,\n",
            "        -1.5083e-02, -3.1267e-02,  7.8130e-03,  2.2015e-02,  1.2133e-02,\n",
            "         3.1532e-03, -1.3686e-02,  2.1329e-02,  1.2190e-02, -2.4601e-02,\n",
            "        -3.6683e-02, -1.4155e-02,  6.2529e-04,  1.5415e-02, -4.5309e-03,\n",
            "        -2.4800e-02,  3.7147e-03, -1.2358e-03, -7.1592e-03, -2.3025e-02,\n",
            "        -9.2759e-03, -2.4960e-03, -5.1777e-03,  2.5439e-02,  3.0151e-02,\n",
            "        -1.4924e-02, -2.3235e-02,  2.2775e-02, -2.4663e-02,  4.2501e-03,\n",
            "        -2.1340e-03,  2.3912e-02,  1.5713e-02, -7.5676e-03,  1.2906e-02,\n",
            "        -2.7984e-02,  2.4936e-02, -7.8957e-03, -1.4759e-02,  1.9478e-02,\n",
            "        -7.3913e-03, -3.4972e-02,  1.3676e-02, -1.5192e-02,  2.2526e-02,\n",
            "         1.6489e-02, -1.2324e-02, -9.0316e-03,  1.0366e-02, -2.9784e-02,\n",
            "         1.6000e-02, -2.5784e-02, -1.5755e-02,  1.2665e-02,  4.5807e-03,\n",
            "         5.3816e-03, -8.1545e-03,  1.3847e-02,  3.1193e-02,  1.9425e-02,\n",
            "        -7.5149e-03, -1.6687e-02, -8.8330e-03, -2.0570e-02,  1.9186e-02,\n",
            "        -2.7133e-02,  6.6072e-03,  5.1033e-03,  1.5311e-03,  2.5542e-02,\n",
            "        -2.1816e-02,  1.7701e-02, -2.0761e-02,  1.9304e-02, -1.0027e-02,\n",
            "        -3.2140e-02,  2.2110e-02, -1.3281e-02, -1.1629e-02,  1.9859e-02,\n",
            "         1.6190e-02, -2.8721e-02, -1.2311e-03,  2.2057e-04,  1.1743e-02,\n",
            "         2.4817e-02, -5.5454e-03, -2.2072e-02,  4.6820e-03, -3.0165e-02,\n",
            "         3.1281e-02, -2.0760e-03, -1.5851e-02,  1.6750e-02,  2.4805e-02,\n",
            "         2.8484e-02,  1.3038e-02,  1.1101e-02,  3.5244e-03, -2.1324e-02,\n",
            "         2.0294e-02, -1.4230e-02, -1.0890e-02,  5.7971e-03,  2.3854e-02,\n",
            "        -1.1686e-02, -1.3243e-02, -9.7047e-03, -2.5801e-02,  9.7113e-03,\n",
            "        -1.5384e-02,  4.1096e-03,  3.2094e-03,  6.7253e-03,  1.9136e-02,\n",
            "        -1.2112e-03, -1.7077e-02, -2.5271e-03, -2.6791e-02,  1.0899e-02,\n",
            "         2.0821e-02, -5.2468e-04, -6.8531e-03, -1.6081e-02,  3.0725e-02,\n",
            "        -1.1039e-02, -2.7179e-02, -4.5342e-04, -2.8460e-02,  2.0259e-03,\n",
            "         2.0054e-02, -2.4085e-02, -2.1382e-02, -1.1982e-02, -1.4953e-02,\n",
            "        -1.9781e-02, -1.3769e-03, -2.4527e-02,  1.1255e-02, -2.4731e-02,\n",
            "        -1.4868e-02, -3.5037e-04, -2.0058e-02, -3.2811e-02,  1.2947e-02,\n",
            "         1.1630e-02,  1.5243e-02,  2.2555e-02,  2.3487e-02,  9.1163e-03,\n",
            "        -3.1455e-02, -1.6917e-02, -1.1682e-02,  1.5685e-02, -1.0793e-02,\n",
            "        -1.4237e-02,  9.5823e-03, -1.4690e-02, -8.1830e-03,  3.8019e-02,\n",
            "         1.0375e-02, -1.7038e-02, -2.8624e-02, -2.0786e-02, -3.1700e-02,\n",
            "         8.5728e-03, -2.2194e-02, -2.6674e-02,  1.0341e-02,  1.9064e-02,\n",
            "         9.6026e-03,  2.5583e-02, -2.0673e-02, -1.9372e-02,  1.4590e-02,\n",
            "        -4.7820e-03,  2.7393e-02, -2.1713e-02,  4.7983e-03, -2.3050e-02,\n",
            "         1.1404e-02,  1.3683e-02, -2.1762e-02, -8.6251e-03,  2.4031e-03,\n",
            "        -1.4292e-02,  6.4950e-04, -2.8353e-02,  7.6070e-03, -2.1037e-03,\n",
            "         1.7943e-03, -5.3328e-03,  2.0631e-02, -2.2455e-02,  2.1846e-02,\n",
            "         2.6340e-02,  3.5101e-03, -1.8423e-02,  1.1031e-02,  1.3606e-02,\n",
            "        -2.4049e-02, -1.5399e-02,  2.7553e-02,  2.6465e-02,  2.4131e-03,\n",
            "         4.2343e-03, -1.0432e-02, -2.8132e-02,  1.1602e-02, -1.5202e-02,\n",
            "         2.5271e-02,  9.4392e-04,  2.6561e-03,  1.5245e-02, -1.4547e-02,\n",
            "         2.6980e-02, -9.2685e-03,  3.4832e-02, -2.1617e-02,  2.0151e-02,\n",
            "        -1.7894e-02,  2.5589e-02,  2.3137e-02,  1.1110e-02, -1.3344e-02,\n",
            "        -2.6257e-02, -2.0078e-03, -1.1523e-02,  7.9889e-03,  2.1441e-02,\n",
            "         1.6368e-02,  5.9123e-03,  1.6705e-02,  2.0712e-05,  1.0643e-02,\n",
            "        -5.0222e-03,  1.6991e-02], device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([0.9985, 0.9698, 1.0033, 0.9784, 1.0419, 0.9898, 0.9628, 1.0026, 1.0447,\n",
            "        1.0079, 0.9724, 0.9392, 0.9760, 1.0276, 0.9717, 1.0055, 1.0870, 0.9930,\n",
            "        0.9943, 1.0727, 1.0384, 1.0377, 1.0507, 1.0148, 0.9530, 0.9448, 0.9955,\n",
            "        1.0039, 0.9716, 0.9474, 1.0044, 1.0061, 0.9703, 1.0103, 1.0201, 0.9875,\n",
            "        0.9912, 0.9818, 1.0424, 0.9760, 1.0280, 0.9288, 1.0176, 0.9639, 1.0437,\n",
            "        1.0320, 0.9727, 1.0207, 1.0051, 1.0339, 1.0450, 0.9851, 1.0026, 1.0121,\n",
            "        1.0310, 0.9855, 1.0532, 1.0386, 0.9830, 0.9462, 1.0335, 0.9996, 0.9764,\n",
            "        0.9799, 0.9247, 1.0203, 0.9893, 1.0074, 0.9902, 1.0211, 0.9597, 1.0584,\n",
            "        0.9974, 1.0017, 1.0387, 1.0092, 0.9886, 0.9806, 1.0507, 1.0086, 1.0281,\n",
            "        1.0269, 0.9683, 1.0230, 0.9793, 0.9115, 1.0064, 1.0392, 1.0118, 1.0602,\n",
            "        0.9834, 0.9832, 0.9320, 1.0112, 1.0068, 1.0083, 0.9268, 0.9524, 1.0537,\n",
            "        1.0205, 1.0467, 1.0747, 0.9667, 1.0137, 0.9620, 1.0204, 0.9974, 0.9695,\n",
            "        0.9527, 1.0001, 1.0370, 0.9781, 0.9620, 0.9615, 0.9987, 1.0637, 1.0330,\n",
            "        1.0083, 0.9827, 1.0123, 1.0149, 0.9656, 0.9794, 0.9812, 0.9916, 1.0419,\n",
            "        0.9720, 0.9886, 0.9420, 0.9769, 0.9536, 1.0128, 1.0362, 0.9888, 1.0295,\n",
            "        1.0213, 0.9961, 0.9761, 1.0522, 0.9737, 1.0090, 0.9481, 0.9896, 1.0054,\n",
            "        1.0345, 0.9912, 1.0035, 1.0055, 0.9849, 0.9499, 0.9835, 1.0067, 0.9423,\n",
            "        0.9941, 1.0305, 1.0242, 0.9912, 0.9486, 1.0567, 0.9778, 0.9645, 1.0598,\n",
            "        1.0110, 1.0232, 0.9596, 1.0523, 0.9752, 0.9797, 1.0057, 1.0079, 1.0400,\n",
            "        0.9528, 1.0400, 0.9263, 1.0117, 1.0054, 1.0420, 0.9988, 1.0137, 1.0072,\n",
            "        0.9888, 1.0243, 0.9557, 1.0171, 0.9246, 0.9870, 1.0358, 1.0340, 1.0443,\n",
            "        1.0190, 0.9710, 1.0105, 1.0276, 0.9815, 0.9739, 1.0177, 1.0435, 1.0002,\n",
            "        1.0142, 0.9724, 0.9153, 0.9668, 1.0436, 0.9769, 1.0586, 1.0013, 1.0158,\n",
            "        1.0003, 1.0092, 0.9897, 0.9857, 1.0473, 1.0066, 1.0085, 1.0402, 0.9566,\n",
            "        1.0177, 1.0168, 0.9498, 0.9872, 1.0115, 1.0455, 0.9778, 0.9761, 0.9739,\n",
            "        1.0084, 1.0254, 1.0525, 0.9574, 1.0210, 0.9876, 1.0309, 1.0563, 0.9669,\n",
            "        1.0044, 0.9301, 0.9787, 0.9972, 0.9867, 0.9766, 1.0164, 1.0397, 1.0128,\n",
            "        0.9699, 0.9983, 1.0054, 0.9959, 1.0564, 0.9997, 0.9919, 1.0197, 0.9693,\n",
            "        0.9287, 0.9850, 0.9600, 0.9693, 1.0067, 1.0150, 1.0340, 0.9634, 1.0312,\n",
            "        1.0398, 1.0483, 1.0505, 1.0355, 0.9925, 1.0408, 1.0073, 1.0195, 0.9950,\n",
            "        1.0024, 0.9828, 1.0315, 1.0505, 1.0067, 1.0353, 1.0269, 1.0114, 1.0119,\n",
            "        0.9966, 0.9847, 1.0349, 0.9761, 0.9701, 1.0075, 1.0475, 0.9763, 1.0289,\n",
            "        1.0168, 1.0442, 1.0307, 1.0308, 0.9566, 0.9807, 0.9405, 0.9856, 1.0631,\n",
            "        1.0361, 0.9762, 0.9972, 0.9896, 0.9712, 1.0593, 0.9813, 1.0268, 0.9870,\n",
            "        1.0243, 1.0348, 0.9972, 1.0278, 0.9583, 0.9758, 1.0038, 0.9643, 0.9839,\n",
            "        1.0104, 1.0319, 1.0163, 1.0168, 1.0393, 0.9483, 1.0142, 1.0378, 0.9872,\n",
            "        1.0416, 1.0451, 1.0153, 1.0028, 0.9416, 0.9731, 0.9712, 0.8702, 0.9973,\n",
            "        1.0198, 0.9941, 0.9614, 1.0114, 1.0181, 0.9949, 0.9481, 0.9907, 1.0302,\n",
            "        1.0308, 0.9677, 1.0446, 1.0495, 0.9765, 0.9795, 1.0048, 1.0088, 0.9972,\n",
            "        1.0172, 0.9597, 0.9380, 1.0116, 0.9714, 1.0299, 1.0260, 1.0069, 0.9565,\n",
            "        1.0118, 1.0519, 1.0083, 1.0311, 0.9903, 1.0791, 1.0256, 0.9785, 0.9866,\n",
            "        1.0452, 0.9866, 0.9770, 0.9645, 0.9980, 0.9969, 0.9476, 1.0415, 0.9418,\n",
            "        1.0287, 0.9856, 1.0117, 1.0127, 1.0032, 0.9486, 0.9428, 0.9578, 1.0295,\n",
            "        0.9940, 0.9913, 0.9763, 1.0538, 0.9381, 0.9436, 1.0584, 0.9890, 0.9829,\n",
            "        0.9689, 0.9686, 0.9890, 0.9937, 1.0134, 1.0311, 1.0530, 1.0260, 1.0003,\n",
            "        0.9411, 0.9191, 0.9854, 1.0413, 1.0439, 0.9947, 0.9873, 1.0041, 1.0077,\n",
            "        1.0033, 1.0177, 0.9709, 0.9830, 1.0393, 0.9890, 0.9500, 0.9444, 1.0587,\n",
            "        1.0390, 0.9697, 0.9808, 0.9478, 1.0121, 0.9594, 0.9683, 1.0216, 1.0054,\n",
            "        0.9958, 1.0171, 0.9710, 1.0042, 0.9308, 0.9976, 0.9787, 1.0359, 0.9974,\n",
            "        1.0209, 1.0068, 0.9761, 1.0040, 0.9690, 1.0715, 1.0103, 0.9598, 1.0011,\n",
            "        0.9486, 1.0145, 0.9955, 0.9773, 0.9720, 1.0342, 0.9834, 1.0051, 1.0639,\n",
            "        1.0277, 1.0271, 1.0307, 0.9784, 0.9981, 1.0045, 0.9982, 1.0139, 1.0338,\n",
            "        1.0010, 1.0531, 1.0380, 1.0268, 1.0141, 0.9516, 1.0131, 0.9947, 1.0322,\n",
            "        1.0328, 0.9634, 1.0288, 1.0164, 1.0527, 1.0366, 1.0024, 1.0441, 1.0239,\n",
            "        1.0011, 0.9856, 1.0519, 1.0309, 0.9989, 0.9998, 0.9710, 0.9972, 0.9711,\n",
            "        1.0285, 0.9998, 1.0021, 1.0343, 1.0025, 1.0467, 0.9417, 0.9582, 1.0093,\n",
            "        1.0411, 0.9767, 0.9303, 1.0256, 1.0141, 0.9461, 0.9848, 0.9383],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([-1.1006e-02, -3.8515e-03,  7.3899e-03,  1.5653e-02, -6.4336e-03,\n",
            "        -1.1382e-02,  3.9666e-03,  3.6206e-02,  1.9159e-02,  4.2837e-03,\n",
            "        -2.7341e-02, -6.8510e-02, -1.8543e-02, -1.5972e-02, -1.6104e-02,\n",
            "         2.2887e-02,  1.9678e-02,  2.3728e-02,  2.3070e-02,  2.6299e-02,\n",
            "         2.7425e-02,  2.1669e-02,  1.3692e-02,  3.5419e-03,  1.2021e-02,\n",
            "        -4.3284e-02,  3.4362e-02,  3.7366e-02, -1.3039e-03, -3.9939e-02,\n",
            "        -9.4952e-02,  2.1350e-02, -3.4224e-02, -6.0389e-02,  7.4812e-02,\n",
            "        -1.5388e-02,  6.3405e-03, -8.7760e-03,  3.4700e-02, -8.3506e-03,\n",
            "        -1.7499e-02, -4.5765e-02,  3.3935e-02, -1.1259e-02,  2.0048e-02,\n",
            "         2.8480e-02, -2.8888e-02,  3.7276e-02, -1.2990e-02,  8.5078e-03,\n",
            "         4.1482e-03,  1.2641e-02, -9.6174e-03,  3.0777e-02,  2.6431e-02,\n",
            "         7.3785e-03, -5.5103e-02,  4.0132e-02, -1.4248e-03,  1.3913e-02,\n",
            "        -1.3269e-02, -1.5211e-02,  5.0242e-02, -2.0487e-02, -6.0634e-02,\n",
            "         1.1960e-03, -9.4431e-04,  3.7409e-02,  1.9972e-02,  8.1240e-03,\n",
            "        -1.5736e-02,  9.3481e-02,  9.0706e-03,  1.3791e-02,  1.8255e-02,\n",
            "        -2.8501e-02, -8.7666e-02, -3.9376e-02,  2.7647e-02,  4.5974e-03,\n",
            "         2.2719e-02,  2.1672e-02,  2.1569e-02,  2.1241e-02, -1.4940e-02,\n",
            "        -5.5554e-02,  2.0405e-02,  1.5846e-02,  1.5610e-03, -1.7292e-02,\n",
            "        -3.6221e-03,  6.8206e-03, -9.7147e-03,  1.1560e-02,  3.6459e-02,\n",
            "         1.8909e-02, -7.2994e-02, -4.5218e-04, -1.2925e-02, -4.0319e-02,\n",
            "         1.3742e-02,  9.3560e-03, -1.1109e-02, -1.5598e-03, -4.8670e-02,\n",
            "         3.4506e-02,  2.7545e-03,  4.2719e-02, -7.7048e-02, -3.1071e-03,\n",
            "         4.7900e-03, -3.2930e-02,  4.0262e-02, -9.1094e-03,  1.4453e-02,\n",
            "         1.4830e-02,  3.5954e-02, -1.0835e-02, -7.4872e-03,  4.9370e-02,\n",
            "         7.7452e-03, -1.1176e-02, -3.0397e-02,  3.9171e-02, -1.9487e-02,\n",
            "         6.0947e-02, -2.0784e-02,  3.7511e-04, -3.6734e-02, -1.8169e-02,\n",
            "        -1.5057e-02,  5.5125e-02,  1.9344e-02, -1.2868e-02,  2.8155e-02,\n",
            "         7.5109e-03,  4.6944e-02, -4.9577e-02, -1.7723e-02, -4.7702e-02,\n",
            "        -1.0449e-02, -4.0172e-02,  1.2918e-02,  1.2053e-02,  5.9766e-03,\n",
            "         2.7347e-03,  3.8927e-02,  3.8091e-04, -2.5567e-02, -2.5793e-02,\n",
            "        -3.7150e-02,  1.8017e-02,  9.0889e-03,  8.0400e-02,  1.8164e-02,\n",
            "        -1.8355e-02,  7.7078e-03, -3.2997e-02,  3.0206e-02, -1.4717e-02,\n",
            "        -6.0468e-02,  3.3111e-02,  1.0404e-02, -4.2846e-03, -3.9138e-02,\n",
            "         5.2126e-02, -2.8354e-02, -4.8466e-04, -1.6090e-02,  4.9311e-03,\n",
            "         3.3110e-02, -1.1525e-02,  1.6571e-02, -7.5436e-02, -2.8782e-02,\n",
            "         2.9715e-03,  3.5571e-02,  9.2721e-03,  1.5926e-03,  3.6476e-02,\n",
            "         1.4549e-02,  1.9477e-02, -4.5573e-02,  2.6498e-02, -7.8699e-03,\n",
            "         2.1830e-02,  3.0754e-02,  6.3133e-02, -4.1346e-02,  9.6635e-04,\n",
            "         2.9791e-02,  6.4613e-03,  1.7780e-02, -2.2187e-02,  1.1384e-03,\n",
            "         1.2260e-02,  3.3566e-02,  1.5114e-02,  3.4456e-02, -1.0705e-02,\n",
            "        -7.5275e-02, -1.2084e-02,  4.4998e-02,  2.3171e-03,  2.1570e-02,\n",
            "         1.0983e-02,  2.9440e-02, -1.8434e-02,  3.5257e-02,  5.7850e-02,\n",
            "         4.7868e-03,  4.4483e-02,  2.8789e-02,  2.2840e-03,  7.1131e-02,\n",
            "        -8.3773e-02,  5.7865e-02,  5.4505e-02,  2.7210e-02, -9.3297e-03,\n",
            "         8.3215e-03,  1.8503e-02, -2.7696e-02, -1.3875e-02,  1.7296e-02,\n",
            "         3.2978e-02,  3.7499e-02,  2.1018e-02, -2.0665e-02,  1.6809e-02,\n",
            "         2.6014e-03,  5.0501e-02, -1.2033e-02,  3.6887e-03, -7.6744e-02,\n",
            "        -4.8227e-02, -1.7094e-02, -8.9921e-03,  1.1266e-02, -3.7808e-02,\n",
            "         4.5895e-02,  4.7836e-02,  3.6688e-02, -4.2978e-02,  1.1586e-02,\n",
            "         3.7180e-02,  7.2262e-03,  6.9681e-02,  1.5120e-03, -1.9590e-02,\n",
            "        -6.6942e-02,  9.4169e-03, -4.8229e-02, -1.2456e-02, -2.5244e-02,\n",
            "        -9.6603e-03,  1.2094e-02, -3.6689e-02,  1.6749e-02, -2.2475e-02,\n",
            "         3.1293e-02, -2.7011e-02,  1.1369e-02,  3.2669e-03,  3.0628e-02,\n",
            "        -7.2045e-02,  2.6755e-02,  1.7281e-03,  3.4237e-02,  2.7182e-03,\n",
            "         2.7613e-02, -3.3164e-04,  1.2741e-02, -1.4510e-02, -9.9403e-03,\n",
            "         6.7287e-02,  4.3113e-02, -2.0690e-02,  4.0227e-03,  1.3140e-02,\n",
            "        -2.6457e-03, -3.5995e-02, -2.1896e-02, -2.5542e-02, -7.0600e-03,\n",
            "         3.4968e-03, -1.7998e-02,  5.4567e-02,  1.1371e-02,  2.7253e-02,\n",
            "        -8.2728e-03,  9.9176e-04, -3.8398e-02, -3.6427e-02, -8.2009e-02,\n",
            "        -3.0131e-02,  3.8780e-02,  5.1786e-02, -5.0168e-02, -2.5171e-02,\n",
            "        -1.3107e-02, -2.5450e-02,  3.3012e-02,  3.0111e-02,  2.9051e-02,\n",
            "        -5.7692e-02,  3.0101e-02,  2.1050e-02, -2.1423e-03,  5.2182e-02,\n",
            "        -4.5258e-02,  8.0372e-04, -2.6080e-02,  2.5280e-02, -8.4545e-03,\n",
            "        -4.4209e-02, -4.0973e-02, -6.0378e-02,  5.5135e-02,  1.9987e-02,\n",
            "        -6.1287e-02,  2.9080e-02,  3.9365e-02,  1.1663e-02,  3.0094e-02,\n",
            "         1.3352e-02, -1.0358e-02,  2.0761e-02, -4.4046e-02, -5.0391e-02,\n",
            "         2.5701e-02, -5.3642e-02, -3.4393e-02,  9.9209e-04, -4.3498e-02,\n",
            "        -3.5432e-02, -1.4660e-02, -9.1301e-03,  4.0686e-02, -4.2164e-02,\n",
            "        -9.7924e-03,  3.7509e-02,  1.7524e-02, -3.9020e-02,  3.6452e-04,\n",
            "         5.1062e-03,  1.1116e-01, -2.9423e-02,  1.0921e-02,  6.2798e-02,\n",
            "         3.9793e-05,  6.0669e-02, -3.5167e-02, -1.0594e-02, -5.1666e-02,\n",
            "         1.3610e-01,  2.5363e-02,  2.6984e-02,  3.5387e-02, -1.3525e-02,\n",
            "         4.6095e-02,  1.9803e-02,  1.8263e-02,  5.1732e-02, -6.3479e-03,\n",
            "         1.5598e-02,  1.6905e-05, -2.1434e-02, -2.4183e-02,  1.1460e-02,\n",
            "        -1.1266e-02, -2.5203e-02, -4.1826e-02, -3.2879e-02,  5.7297e-02,\n",
            "        -4.5019e-02,  5.8775e-02, -5.5163e-02,  6.6119e-03, -7.7408e-02,\n",
            "         5.5657e-02,  3.5398e-02,  2.3885e-02, -4.2341e-02, -8.2962e-02,\n",
            "        -2.2372e-02,  3.4421e-02, -4.2376e-02, -1.1846e-02,  6.7946e-02,\n",
            "        -4.3559e-03, -3.0351e-02, -3.5409e-02,  2.7095e-02,  1.7938e-02,\n",
            "        -2.1297e-02, -1.1415e-02,  2.7854e-02,  3.0529e-02,  2.1367e-02,\n",
            "         4.2880e-02,  1.7029e-02,  1.5366e-02,  2.9798e-02,  1.2757e-02,\n",
            "        -2.4913e-02, -7.2607e-02,  2.0257e-02,  6.0518e-03,  1.9597e-02,\n",
            "         1.2632e-02, -2.2456e-02,  1.5157e-02,  7.9765e-03, -1.1268e-02,\n",
            "         9.4239e-02, -6.4212e-02, -5.9183e-03, -1.2395e-03, -1.2372e-03,\n",
            "        -1.7531e-02, -1.7203e-02,  3.6030e-02,  7.1935e-02,  1.4413e-02,\n",
            "         2.4540e-02, -2.8579e-02,  2.5053e-02,  5.7621e-03,  9.8988e-03,\n",
            "         3.4416e-03, -4.6890e-03, -8.9683e-03,  1.1471e-02, -3.2236e-02,\n",
            "        -5.3141e-03, -8.9499e-02,  4.6985e-02,  6.1030e-02,  5.1802e-02,\n",
            "        -4.9629e-03,  9.5346e-03, -3.0837e-02,  2.5996e-02, -3.4164e-02,\n",
            "        -3.0566e-02,  5.5090e-02,  2.8842e-02, -1.9328e-02,  1.2104e-02,\n",
            "        -2.4248e-02,  2.9826e-02,  4.0160e-03, -1.4556e-02, -5.4589e-02,\n",
            "         1.6327e-02, -6.5955e-02, -2.1684e-02, -1.4362e-03,  3.9982e-02,\n",
            "         2.2074e-02,  1.8037e-02, -5.8432e-03,  4.0729e-02,  3.5772e-02,\n",
            "        -2.7254e-02,  2.8062e-02,  1.7952e-02,  1.6834e-02,  5.9595e-02,\n",
            "         3.2791e-02,  2.4993e-02,  4.2364e-02, -6.1458e-02,  6.4118e-04,\n",
            "         2.1646e-02, -1.9589e-02, -1.7391e-02, -3.6152e-02, -6.7439e-03,\n",
            "         1.0274e-02,  3.3254e-02, -4.1815e-03,  1.5518e-02,  4.6852e-02,\n",
            "         3.6449e-02, -8.2212e-04, -8.5713e-03,  7.7712e-03,  5.0827e-02,\n",
            "         1.2586e-02,  4.4041e-02, -3.4640e-02, -1.2192e-02, -6.7729e-03,\n",
            "         3.5736e-02,  1.3495e-02,  5.5742e-02, -5.2527e-03, -7.2851e-02,\n",
            "         2.8034e-02, -1.7102e-02,  2.8697e-03,  6.6045e-03,  3.7032e-02,\n",
            "        -4.2380e-02,  1.3789e-02, -3.2689e-03,  2.4014e-02,  3.6900e-03,\n",
            "         5.8851e-03, -5.2601e-02], device='cuda:0', requires_grad=True)\n",
            "torch.Size([256, 512]) Parameter containing:\n",
            "tensor([[-0.0210,  0.0011,  0.0138,  ..., -0.0379, -0.0251, -0.0604],\n",
            "        [ 0.0006,  0.0033, -0.0647,  ...,  0.0449, -0.0407,  0.0133],\n",
            "        [-0.0300, -0.0135,  0.0386,  ...,  0.0357,  0.0242,  0.0177],\n",
            "        ...,\n",
            "        [-0.0580,  0.0302, -0.0139,  ..., -0.0088,  0.0151,  0.0438],\n",
            "        [ 0.0663,  0.0246, -0.0116,  ...,  0.0152, -0.0077,  0.0371],\n",
            "        [ 0.0164, -0.0652, -0.0628,  ..., -0.0319, -0.0220,  0.0063]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([-0.0427, -0.0110,  0.0063, -0.0128, -0.0189,  0.0150, -0.0257,  0.0067,\n",
            "         0.0162,  0.0356,  0.0396, -0.0436,  0.0323,  0.0379,  0.0397,  0.0335,\n",
            "        -0.0016, -0.0196, -0.0427, -0.0311,  0.0376,  0.0113, -0.0438, -0.0155,\n",
            "         0.0166, -0.0083,  0.0333,  0.0111,  0.0423,  0.0175,  0.0071, -0.0160,\n",
            "         0.0099,  0.0061,  0.0059, -0.0166, -0.0204, -0.0422, -0.0343,  0.0133,\n",
            "         0.0343, -0.0088, -0.0341,  0.0078,  0.0199,  0.0357, -0.0013, -0.0164,\n",
            "        -0.0004, -0.0239, -0.0053,  0.0150, -0.0143,  0.0341, -0.0362, -0.0155,\n",
            "         0.0304,  0.0138,  0.0084,  0.0270,  0.0244, -0.0234,  0.0219,  0.0055,\n",
            "         0.0157,  0.0136, -0.0068,  0.0205,  0.0061,  0.0092, -0.0058, -0.0041,\n",
            "         0.0152, -0.0111, -0.0002,  0.0308,  0.0407,  0.0298,  0.0417, -0.0262,\n",
            "         0.0123, -0.0182,  0.0308,  0.0112, -0.0432, -0.0189,  0.0011,  0.0175,\n",
            "        -0.0028,  0.0120, -0.0057,  0.0316, -0.0051,  0.0209,  0.0108, -0.0003,\n",
            "         0.0442, -0.0407, -0.0171,  0.0315, -0.0316,  0.0324, -0.0286,  0.0253,\n",
            "         0.0249, -0.0108, -0.0228, -0.0391,  0.0393, -0.0108, -0.0367,  0.0156,\n",
            "         0.0438, -0.0364, -0.0216, -0.0406,  0.0227, -0.0014,  0.0379,  0.0406,\n",
            "         0.0073,  0.0138,  0.0373,  0.0069,  0.0096, -0.0350,  0.0370, -0.0403,\n",
            "        -0.0069,  0.0176,  0.0062, -0.0205,  0.0439, -0.0233,  0.0209, -0.0405,\n",
            "        -0.0358,  0.0434,  0.0002, -0.0207,  0.0409, -0.0148,  0.0362,  0.0153,\n",
            "        -0.0303,  0.0113,  0.0430, -0.0035, -0.0384, -0.0323,  0.0436, -0.0279,\n",
            "        -0.0225, -0.0063,  0.0341, -0.0208,  0.0313,  0.0128, -0.0255,  0.0403,\n",
            "        -0.0253,  0.0398,  0.0158,  0.0082, -0.0006, -0.0016, -0.0386,  0.0228,\n",
            "         0.0386,  0.0408,  0.0182,  0.0298, -0.0166,  0.0339, -0.0195,  0.0045,\n",
            "        -0.0387, -0.0229,  0.0305,  0.0046,  0.0260,  0.0146,  0.0183, -0.0386,\n",
            "        -0.0287, -0.0304,  0.0077,  0.0235,  0.0218, -0.0442,  0.0339, -0.0318,\n",
            "         0.0018,  0.0433,  0.0156,  0.0199, -0.0194, -0.0315,  0.0053, -0.0289,\n",
            "         0.0216, -0.0228, -0.0021,  0.0028,  0.0323, -0.0008, -0.0303, -0.0233,\n",
            "         0.0092, -0.0107, -0.0097,  0.0182,  0.0047, -0.0023, -0.0263, -0.0219,\n",
            "         0.0222, -0.0367,  0.0103,  0.0009,  0.0221,  0.0137, -0.0348, -0.0187,\n",
            "         0.0221,  0.0276,  0.0249,  0.0002,  0.0132,  0.0349, -0.0111, -0.0389,\n",
            "         0.0108,  0.0032,  0.0201, -0.0073,  0.0074,  0.0167,  0.0149,  0.0018,\n",
            "         0.0209,  0.0425,  0.0208,  0.0049,  0.0011, -0.0033, -0.0033,  0.0337,\n",
            "         0.0332,  0.0189, -0.0231,  0.0289,  0.0216,  0.0227, -0.0068,  0.0329],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([0.8894, 1.0412, 1.0828, 1.0547, 1.0456, 0.9500, 0.9709, 0.8953, 0.9528,\n",
            "        1.0209, 0.9799, 1.0986, 1.0089, 1.0145, 1.0150, 1.0291, 0.9592, 1.0615,\n",
            "        0.9395, 0.9683, 0.9779, 1.0784, 0.9548, 0.9684, 1.0083, 1.0179, 0.8999,\n",
            "        0.9549, 0.8725, 1.0364, 0.9624, 1.0198, 1.0336, 0.9723, 0.9801, 1.0048,\n",
            "        1.0529, 0.9844, 0.9518, 0.9220, 1.1092, 0.9758, 0.9587, 1.0258, 0.9292,\n",
            "        0.9390, 1.0044, 0.9509, 0.9511, 0.9579, 1.0046, 1.0368, 0.9646, 0.9521,\n",
            "        0.9768, 0.9783, 0.9686, 0.9659, 0.9676, 0.9589, 1.0086, 1.0481, 1.0134,\n",
            "        0.9285, 0.9869, 0.9601, 1.0589, 1.0224, 1.0893, 1.0748, 1.0121, 0.9546,\n",
            "        0.9548, 0.9953, 0.8884, 0.9804, 1.0616, 0.9613, 1.0518, 0.9482, 0.9833,\n",
            "        0.9669, 0.9942, 0.8942, 0.9003, 1.0365, 0.9673, 1.0458, 1.0121, 0.8937,\n",
            "        0.9615, 0.9535, 1.0491, 1.0435, 1.0063, 0.8796, 0.8846, 0.9907, 0.9740,\n",
            "        0.9319, 0.9852, 0.9982, 0.9417, 0.9211, 0.9797, 0.9469, 1.0776, 1.0716,\n",
            "        0.9625, 0.9131, 0.9947, 0.9918, 0.9042, 1.0142, 0.9689, 1.0337, 0.9297,\n",
            "        1.0005, 0.9380, 1.0702, 0.9815, 0.9631, 1.0723, 0.9660, 1.0303, 0.9426,\n",
            "        0.9734, 1.0436, 0.9701, 1.0010, 0.9048, 0.9615, 1.0229, 1.1070, 1.0415,\n",
            "        1.0591, 0.9388, 0.9181, 0.9681, 0.9763, 0.9120, 0.9531, 0.9424, 1.0045,\n",
            "        0.9726, 1.0277, 1.0076, 1.0455, 0.9680, 1.0246, 1.0487, 0.9609, 0.9989,\n",
            "        0.9483, 0.9429, 0.9866, 0.9735, 0.9939, 0.9366, 1.0451, 1.0285, 0.9577,\n",
            "        0.9715, 0.9945, 0.9711, 1.0365, 0.9462, 1.0753, 0.9730, 0.9984, 1.0069,\n",
            "        0.9820, 0.9559, 1.0815, 0.9478, 0.9495, 0.9669, 1.0404, 1.0500, 0.9127,\n",
            "        0.9697, 0.9525, 0.9541, 1.0158, 1.0343, 0.9139, 0.9448, 1.0238, 1.0510,\n",
            "        1.0587, 0.9938, 1.0801, 0.9780, 0.9894, 0.9960, 0.9865, 0.9958, 0.9797,\n",
            "        0.9420, 0.9409, 0.9874, 0.9444, 0.9431, 0.9907, 0.9616, 0.9770, 1.0405,\n",
            "        0.8923, 0.9723, 0.9476, 0.9790, 0.9841, 1.0493, 0.9602, 0.9356, 1.0168,\n",
            "        0.9475, 0.9213, 0.9600, 1.0474, 0.9789, 0.9733, 0.9534, 1.0085, 0.9139,\n",
            "        1.0456, 1.0387, 0.9696, 0.9659, 0.9348, 0.9537, 0.9210, 0.9658, 0.9884,\n",
            "        0.9594, 0.9385, 0.9610, 1.0646, 1.0617, 1.0048, 0.9005, 1.0485, 1.1101,\n",
            "        0.9914, 1.0071, 0.9634, 1.0767, 1.0551, 0.9989, 0.9857, 1.0432, 0.9472,\n",
            "        0.9351, 1.0256, 0.9505, 1.0023], device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([-0.0483,  0.0293,  0.1198,  0.1229,  0.1201, -0.0869, -0.0175, -0.0510,\n",
            "        -0.0678,  0.0594, -0.0391,  0.1655,  0.1741,  0.1131,  0.0989,  0.1823,\n",
            "        -0.0229,  0.2444, -0.0550, -0.0355,  0.0449,  0.1692, -0.0552,  0.0320,\n",
            "         0.0599,  0.0754, -0.1024,  0.1080, -0.0557,  0.0669,  0.1842,  0.0424,\n",
            "         0.0820, -0.0189, -0.0315,  0.0750,  0.2064, -0.0490, -0.0709, -0.0841,\n",
            "         0.2126,  0.1169, -0.0890,  0.0660, -0.0260, -0.0465,  0.1030, -0.0767,\n",
            "        -0.0925, -0.0492,  0.0517,  0.0936,  0.0404, -0.0288, -0.0470,  0.1000,\n",
            "         0.0107, -0.0405, -0.0677, -0.0291,  0.0515,  0.0903,  0.0421, -0.0663,\n",
            "        -0.0665,  0.0304,  0.1010,  0.0771,  0.2209,  0.1622,  0.0926, -0.1098,\n",
            "        -0.0019,  0.0476,  0.0185,  0.0975,  0.0773, -0.0897,  0.1569, -0.0228,\n",
            "        -0.0223, -0.0350, -0.0557, -0.1195, -0.0594,  0.1782, -0.0815,  0.0783,\n",
            "         0.0810, -0.0730,  0.0333,  0.0095,  0.1415,  0.1926,  0.0653, -0.0570,\n",
            "        -0.0656,  0.1570, -0.0030, -0.0833,  0.0269,  0.1158,  0.0015, -0.0690,\n",
            "        -0.0538, -0.0324,  0.1196,  0.1471, -0.0638, -0.0816,  0.0401,  0.0483,\n",
            "        -0.0598,  0.0480, -0.0542,  0.0778, -0.0525,  0.0405,  0.0549,  0.0389,\n",
            "        -0.0165, -0.0720,  0.0565,  0.0206,  0.1306, -0.0557, -0.0678,  0.1248,\n",
            "         0.0322,  0.0458, -0.0594,  0.0273,  0.0317,  0.1800,  0.0766,  0.2056,\n",
            "        -0.0682, -0.0793, -0.0322, -0.0374, -0.0578, -0.0017, -0.0673,  0.1824,\n",
            "        -0.0446,  0.1243,  0.1195,  0.0459, -0.0223,  0.1237,  0.0937, -0.0992,\n",
            "         0.0144, -0.0324, -0.0554, -0.0014,  0.1202,  0.0722, -0.0469,  0.1250,\n",
            "         0.0457, -0.0260, -0.0721,  0.0167, -0.0665,  0.1196, -0.0429,  0.1896,\n",
            "        -0.0750,  0.0325,  0.0820, -0.0111, -0.0538,  0.0707, -0.0698, -0.0846,\n",
            "        -0.0172,  0.0905,  0.1153, -0.0641, -0.0982, -0.0597, -0.0209, -0.0171,\n",
            "         0.0551, -0.1034, -0.1038,  0.1245,  0.0871,  0.2274, -0.0306,  0.0818,\n",
            "        -0.0145,  0.0352,  0.0345,  0.0273,  0.0487,  0.0492,  0.1306, -0.0739,\n",
            "        -0.0465, -0.0586, -0.0422,  0.0748, -0.0698,  0.0412,  0.0631, -0.1159,\n",
            "        -0.0313, -0.1047, -0.0355,  0.0404,  0.1493, -0.0729, -0.0862,  0.1017,\n",
            "        -0.0290, -0.0332, -0.0144,  0.0187, -0.0612, -0.0175, -0.0192,  0.0496,\n",
            "        -0.0958,  0.1598,  0.1556, -0.0244, -0.0376, -0.0258, -0.0444, -0.0822,\n",
            "        -0.0242,  0.0097, -0.0832, -0.0335, -0.0682,  0.1286,  0.0570,  0.0533,\n",
            "        -0.0794,  0.1075,  0.2534,  0.0024,  0.0919,  0.0309,  0.1892,  0.1157,\n",
            "         0.1080, -0.0506,  0.0442, -0.0456, -0.0486,  0.1362, -0.0664,  0.0217],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([9, 256]) Parameter containing:\n",
            "tensor([[-0.0124, -0.0404, -0.0073,  ..., -0.0036,  0.0285, -0.0121],\n",
            "        [-0.0169, -0.0078, -0.0139,  ..., -0.0047, -0.0150, -0.0133],\n",
            "        [ 0.0248, -0.0342, -0.0431,  ..., -0.0352,  0.0379, -0.0040],\n",
            "        ...,\n",
            "        [ 0.0062, -0.0467, -0.0655,  ..., -0.0601,  0.0146, -0.0442],\n",
            "        [-0.0062,  0.0273,  0.0221,  ...,  0.0324, -0.0045,  0.0015],\n",
            "        [ 0.0025, -0.0047, -0.0007,  ..., -0.0104,  0.0161,  0.0067]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([9]) Parameter containing:\n",
            "tensor([ 1.0357, -0.0118, -0.0412,  0.0909,  1.0224,  0.0562, -0.0324,  0.0275,\n",
            "         1.0278], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64, 3, 1]) Parameter containing:\n",
            "tensor([[[-0.4252],\n",
            "         [ 0.0189],\n",
            "         [-0.5773]],\n",
            "\n",
            "        [[-0.0196],\n",
            "         [-0.2391],\n",
            "         [ 0.4603]],\n",
            "\n",
            "        [[ 0.3574],\n",
            "         [-0.2354],\n",
            "         [ 0.2366]],\n",
            "\n",
            "        [[ 0.5583],\n",
            "         [-0.1996],\n",
            "         [-0.1436]],\n",
            "\n",
            "        [[ 0.5446],\n",
            "         [ 0.2788],\n",
            "         [-0.5865]],\n",
            "\n",
            "        [[ 0.5390],\n",
            "         [-0.0355],\n",
            "         [-0.3610]],\n",
            "\n",
            "        [[ 0.2167],\n",
            "         [ 0.1879],\n",
            "         [-0.3155]],\n",
            "\n",
            "        [[ 0.3600],\n",
            "         [ 0.2462],\n",
            "         [ 0.5664]],\n",
            "\n",
            "        [[-0.4038],\n",
            "         [-0.5957],\n",
            "         [ 0.1777]],\n",
            "\n",
            "        [[-0.1505],\n",
            "         [ 0.2456],\n",
            "         [ 0.1535]],\n",
            "\n",
            "        [[-0.0570],\n",
            "         [ 0.0301],\n",
            "         [-0.3224]],\n",
            "\n",
            "        [[ 0.1233],\n",
            "         [ 0.1600],\n",
            "         [-0.0773]],\n",
            "\n",
            "        [[-0.3347],\n",
            "         [ 0.3700],\n",
            "         [-0.4794]],\n",
            "\n",
            "        [[-0.5006],\n",
            "         [-0.4130],\n",
            "         [-0.4806]],\n",
            "\n",
            "        [[ 0.1722],\n",
            "         [-0.4952],\n",
            "         [-0.5824]],\n",
            "\n",
            "        [[ 0.4178],\n",
            "         [-0.1584],\n",
            "         [-0.1841]],\n",
            "\n",
            "        [[-0.2468],\n",
            "         [ 0.4245],\n",
            "         [-0.1743]],\n",
            "\n",
            "        [[-0.3181],\n",
            "         [ 0.4206],\n",
            "         [ 0.0705]],\n",
            "\n",
            "        [[-0.2629],\n",
            "         [-0.5132],\n",
            "         [ 0.0372]],\n",
            "\n",
            "        [[ 0.6025],\n",
            "         [ 0.2089],\n",
            "         [-0.3317]],\n",
            "\n",
            "        [[ 0.0915],\n",
            "         [ 0.5682],\n",
            "         [ 0.5314]],\n",
            "\n",
            "        [[-0.4970],\n",
            "         [-0.1445],\n",
            "         [-0.0623]],\n",
            "\n",
            "        [[ 0.2729],\n",
            "         [ 0.2482],\n",
            "         [-0.3182]],\n",
            "\n",
            "        [[-0.1346],\n",
            "         [ 0.0469],\n",
            "         [ 0.4022]],\n",
            "\n",
            "        [[ 0.1717],\n",
            "         [-0.3000],\n",
            "         [ 0.4212]],\n",
            "\n",
            "        [[ 0.3511],\n",
            "         [-0.2833],\n",
            "         [ 0.3363]],\n",
            "\n",
            "        [[-0.4086],\n",
            "         [ 0.1061],\n",
            "         [-0.4721]],\n",
            "\n",
            "        [[-0.2515],\n",
            "         [ 0.3579],\n",
            "         [-0.0157]],\n",
            "\n",
            "        [[-0.1185],\n",
            "         [-0.0746],\n",
            "         [ 0.2420]],\n",
            "\n",
            "        [[ 0.1885],\n",
            "         [ 0.1876],\n",
            "         [ 0.0283]],\n",
            "\n",
            "        [[-0.3471],\n",
            "         [ 0.4638],\n",
            "         [-0.4025]],\n",
            "\n",
            "        [[-0.4382],\n",
            "         [ 0.0322],\n",
            "         [-0.2735]],\n",
            "\n",
            "        [[ 0.2280],\n",
            "         [-0.6555],\n",
            "         [ 0.0472]],\n",
            "\n",
            "        [[ 0.0187],\n",
            "         [ 0.2161],\n",
            "         [ 0.2440]],\n",
            "\n",
            "        [[-0.0512],\n",
            "         [ 0.0668],\n",
            "         [-0.0471]],\n",
            "\n",
            "        [[ 0.3023],\n",
            "         [-0.0511],\n",
            "         [-0.5265]],\n",
            "\n",
            "        [[-0.3758],\n",
            "         [ 0.3005],\n",
            "         [-0.4271]],\n",
            "\n",
            "        [[-0.0762],\n",
            "         [ 0.0207],\n",
            "         [-0.1447]],\n",
            "\n",
            "        [[ 0.4589],\n",
            "         [ 0.5515],\n",
            "         [-0.4151]],\n",
            "\n",
            "        [[-0.4677],\n",
            "         [-0.4324],\n",
            "         [ 0.1229]],\n",
            "\n",
            "        [[ 0.2465],\n",
            "         [ 0.2050],\n",
            "         [-0.2454]],\n",
            "\n",
            "        [[ 0.2060],\n",
            "         [-0.4730],\n",
            "         [ 0.1572]],\n",
            "\n",
            "        [[-0.2190],\n",
            "         [ 0.2134],\n",
            "         [ 0.2802]],\n",
            "\n",
            "        [[ 0.2709],\n",
            "         [-0.6025],\n",
            "         [-0.4623]],\n",
            "\n",
            "        [[ 0.0534],\n",
            "         [ 0.3608],\n",
            "         [-0.5298]],\n",
            "\n",
            "        [[-0.3717],\n",
            "         [ 0.4084],\n",
            "         [-0.5028]],\n",
            "\n",
            "        [[ 0.5490],\n",
            "         [ 0.1967],\n",
            "         [ 0.3230]],\n",
            "\n",
            "        [[ 0.2334],\n",
            "         [ 0.1347],\n",
            "         [-0.1673]],\n",
            "\n",
            "        [[-0.2518],\n",
            "         [-0.3680],\n",
            "         [ 0.3531]],\n",
            "\n",
            "        [[-0.1753],\n",
            "         [ 0.5658],\n",
            "         [ 0.2268]],\n",
            "\n",
            "        [[ 0.1921],\n",
            "         [-0.3214],\n",
            "         [-0.1337]],\n",
            "\n",
            "        [[ 0.2203],\n",
            "         [-0.5925],\n",
            "         [ 0.1243]],\n",
            "\n",
            "        [[-0.2027],\n",
            "         [-0.4260],\n",
            "         [-0.2990]],\n",
            "\n",
            "        [[ 0.2325],\n",
            "         [ 0.4559],\n",
            "         [ 0.4670]],\n",
            "\n",
            "        [[-0.0957],\n",
            "         [-0.0890],\n",
            "         [-0.1001]],\n",
            "\n",
            "        [[ 0.1006],\n",
            "         [ 0.2676],\n",
            "         [ 0.1785]],\n",
            "\n",
            "        [[-0.4486],\n",
            "         [-0.2495],\n",
            "         [ 0.4044]],\n",
            "\n",
            "        [[-0.3332],\n",
            "         [ 0.5437],\n",
            "         [-0.3033]],\n",
            "\n",
            "        [[-0.5333],\n",
            "         [ 0.1948],\n",
            "         [ 0.5449]],\n",
            "\n",
            "        [[ 0.4699],\n",
            "         [-0.4595],\n",
            "         [-0.1036]],\n",
            "\n",
            "        [[ 0.4004],\n",
            "         [ 0.5698],\n",
            "         [-0.2463]],\n",
            "\n",
            "        [[-0.2955],\n",
            "         [-0.1256],\n",
            "         [ 0.4576]],\n",
            "\n",
            "        [[-0.4701],\n",
            "         [ 0.0297],\n",
            "         [-0.1260]],\n",
            "\n",
            "        [[-0.0660],\n",
            "         [-0.2230],\n",
            "         [ 0.1367]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([ 0.0735, -0.0186,  0.5518, -0.1507, -0.2223, -0.4679, -0.3341,  0.0215,\n",
            "         0.1831,  0.4545, -0.4738,  0.4446, -0.0597, -0.5340,  0.0931, -0.0202,\n",
            "         0.1577,  0.4607,  0.5481,  0.2188, -0.1240, -0.0745, -0.1750, -0.2295,\n",
            "        -0.0130,  0.5035,  0.5449, -0.3279, -0.1022, -0.0620,  0.1927, -0.0262,\n",
            "        -0.2992,  0.3669,  0.1763,  0.2782, -0.1325, -0.4834, -0.5250,  0.3238,\n",
            "         0.2306, -0.2130,  0.2450,  0.1050,  0.1370, -0.2690, -0.3796, -0.3048,\n",
            "        -0.4070,  0.4757, -0.2191,  0.5634,  0.2509, -0.4714, -0.2520, -0.2697,\n",
            "         0.1960,  0.5034, -0.4667, -0.5520, -0.0396,  0.1779,  0.5360, -0.3947],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([1.0159, 0.9922, 0.9405, 0.9645, 0.9981, 1.0262, 0.9972, 0.9432, 1.1302,\n",
            "        1.0046, 0.9669, 1.0351, 0.9489, 0.9770, 1.0569, 1.0009, 0.9743, 1.0154,\n",
            "        1.0968, 1.0791, 0.9816, 0.9967, 1.0297, 1.0101, 0.9864, 0.9902, 0.9976,\n",
            "        0.9755, 1.0129, 1.0184, 1.0161, 0.9477, 0.9505, 0.9950, 0.9819, 0.9908,\n",
            "        0.9873, 0.9513, 1.0087, 1.0105, 1.0217, 0.9776, 1.0959, 1.0516, 0.9819,\n",
            "        0.9682, 0.9631, 1.0194, 0.9951, 0.9859, 0.9917, 0.9493, 0.9996, 0.9927,\n",
            "        0.9776, 1.0420, 1.0319, 0.9907, 1.0836, 0.9334, 1.0431, 1.0115, 1.0115,\n",
            "        0.9913], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([-0.0799,  0.1444,  0.0017,  0.0099, -0.1827,  0.1567,  0.0646,  0.0605,\n",
            "         0.1835,  0.1099,  0.1611,  0.0227,  0.1137, -0.0145,  0.0877,  0.2146,\n",
            "         0.0100,  0.0979,  0.1275,  0.1588, -0.0890, -0.1962,  0.2236,  0.1385,\n",
            "         0.0234,  0.0492,  0.1351,  0.0827,  0.1719,  0.0274,  0.0734,  0.0678,\n",
            "         0.1318,  0.0848,  0.0199, -0.0515,  0.0444,  0.0105,  0.1447, -0.1340,\n",
            "         0.2069,  0.0786,  0.1511, -0.0218, -0.1761,  0.0274,  0.1448, -0.0375,\n",
            "         0.1959, -0.0052,  0.0301, -0.0180,  0.2496,  0.1234,  0.1088,  0.0113,\n",
            "         0.0474,  0.0260,  0.2575, -0.0400, -0.0183, -0.1222,  0.0879,  0.0032],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64, 64, 1]) Parameter containing:\n",
            "tensor([[[-0.0542],\n",
            "         [ 0.0595],\n",
            "         [ 0.0274],\n",
            "         ...,\n",
            "         [-0.0191],\n",
            "         [ 0.0539],\n",
            "         [ 0.0082]],\n",
            "\n",
            "        [[ 0.1186],\n",
            "         [ 0.0460],\n",
            "         [ 0.1063],\n",
            "         ...,\n",
            "         [ 0.0915],\n",
            "         [ 0.1353],\n",
            "         [ 0.0239]],\n",
            "\n",
            "        [[-0.1432],\n",
            "         [ 0.1164],\n",
            "         [ 0.0585],\n",
            "         ...,\n",
            "         [ 0.0758],\n",
            "         [-0.0834],\n",
            "         [-0.0737]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0884],\n",
            "         [-0.0689],\n",
            "         [ 0.0531],\n",
            "         ...,\n",
            "         [ 0.0281],\n",
            "         [ 0.0660],\n",
            "         [-0.0276]],\n",
            "\n",
            "        [[-0.0924],\n",
            "         [-0.0161],\n",
            "         [-0.0533],\n",
            "         ...,\n",
            "         [-0.1013],\n",
            "         [ 0.0868],\n",
            "         [-0.0128]],\n",
            "\n",
            "        [[-0.1206],\n",
            "         [ 0.0525],\n",
            "         [ 0.1009],\n",
            "         ...,\n",
            "         [ 0.0594],\n",
            "         [-0.0665],\n",
            "         [-0.0172]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([ 0.0889,  0.0042, -0.0290,  0.0968,  0.0096,  0.0638,  0.0919,  0.0588,\n",
            "        -0.0587,  0.1192, -0.0637,  0.0537, -0.0144,  0.0231, -0.0461,  0.0285,\n",
            "        -0.0633, -0.0508,  0.0670, -0.1207,  0.1106, -0.0322, -0.0267,  0.1089,\n",
            "        -0.0971, -0.1268, -0.0481,  0.0367, -0.0623, -0.0305,  0.0354,  0.0266,\n",
            "        -0.0912, -0.0068, -0.0155,  0.1132, -0.0536, -0.0463,  0.1157,  0.0364,\n",
            "         0.0477, -0.0074,  0.0317, -0.0069,  0.0735, -0.0533, -0.0991, -0.1122,\n",
            "         0.0348, -0.0550,  0.0011,  0.1346,  0.1158,  0.0900,  0.0079,  0.0947,\n",
            "        -0.0968,  0.0847, -0.0965, -0.0031,  0.0799,  0.0418, -0.0744,  0.0766],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([0.9541, 0.9455, 0.9889, 0.9929, 0.9976, 0.9974, 0.9556, 0.9382, 1.0492,\n",
            "        0.9370, 0.9492, 0.9439, 0.9162, 0.8932, 0.9564, 0.8974, 0.8918, 1.0866,\n",
            "        0.9529, 0.9780, 0.9596, 0.9342, 1.0579, 1.0804, 1.0597, 0.9250, 0.9342,\n",
            "        0.9584, 0.9645, 0.9769, 0.9623, 0.9330, 0.9235, 0.9017, 0.9579, 0.9907,\n",
            "        1.0029, 0.9504, 0.9546, 1.1446, 1.0386, 0.9882, 1.0045, 1.0109, 0.9733,\n",
            "        0.9825, 0.9724, 1.1876, 0.9666, 0.9374, 1.0295, 0.9676, 0.9285, 0.9664,\n",
            "        1.0703, 0.9360, 1.3486, 1.1117, 0.9534, 0.9657, 0.9825, 0.9803, 0.9460,\n",
            "        0.9141], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([-0.0323, -0.0911,  0.0150,  0.0122,  0.1402,  0.1124,  0.0745, -0.0251,\n",
            "         0.3126, -0.0055, -0.0029,  0.0054,  0.0508, -0.1794, -0.0271, -0.2007,\n",
            "         0.0646,  0.3597, -0.2571,  0.0934,  0.0053,  0.0343, -0.1949,  0.3367,\n",
            "         0.3612,  0.0264,  0.1205,  0.0311, -0.0484,  0.0294,  0.0522,  0.0410,\n",
            "        -0.0219, -0.0540,  0.1202,  0.2669,  0.0368,  0.0554,  0.0793,  0.3545,\n",
            "        -0.2603,  0.0867,  0.0821,  0.0991, -0.0296,  0.1329,  0.0261,  0.4404,\n",
            "         0.1431,  0.0401,  0.0761,  0.0207, -0.0280,  0.0130,  0.2093,  0.0986,\n",
            "         0.6575,  0.3110,  0.0176,  0.0728,  0.0829,  0.0633,  0.1105, -0.0615],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64, 64, 1]) Parameter containing:\n",
            "tensor([[[-0.1028],\n",
            "         [ 0.0892],\n",
            "         [ 0.1233],\n",
            "         ...,\n",
            "         [-0.0472],\n",
            "         [ 0.1012],\n",
            "         [ 0.0773]],\n",
            "\n",
            "        [[ 0.1176],\n",
            "         [ 0.0362],\n",
            "         [-0.0629],\n",
            "         ...,\n",
            "         [ 0.0781],\n",
            "         [ 0.0855],\n",
            "         [ 0.0949]],\n",
            "\n",
            "        [[-0.0086],\n",
            "         [-0.0775],\n",
            "         [ 0.1216],\n",
            "         ...,\n",
            "         [-0.0584],\n",
            "         [-0.1345],\n",
            "         [ 0.1392]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0528],\n",
            "         [-0.1401],\n",
            "         [-0.0305],\n",
            "         ...,\n",
            "         [ 0.0183],\n",
            "         [-0.0846],\n",
            "         [ 0.0960]],\n",
            "\n",
            "        [[-0.0758],\n",
            "         [-0.0449],\n",
            "         [ 0.1002],\n",
            "         ...,\n",
            "         [-0.0234],\n",
            "         [-0.0006],\n",
            "         [ 0.0167]],\n",
            "\n",
            "        [[ 0.0698],\n",
            "         [-0.0542],\n",
            "         [ 0.0754],\n",
            "         ...,\n",
            "         [ 0.0521],\n",
            "         [ 0.0840],\n",
            "         [ 0.0120]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([-0.0305, -0.0849,  0.0973,  0.0180, -0.1229, -0.0620, -0.0106,  0.0014,\n",
            "        -0.0671, -0.1173,  0.1090,  0.1047,  0.0576, -0.0070, -0.0377,  0.0682,\n",
            "        -0.0203, -0.0969,  0.0872,  0.0742, -0.0034,  0.0275,  0.0487,  0.0320,\n",
            "         0.0107, -0.1180,  0.0307, -0.1003, -0.0628,  0.0744,  0.1079,  0.0668,\n",
            "        -0.0830,  0.1177,  0.0609, -0.0194, -0.0727, -0.0896,  0.0409,  0.0479,\n",
            "        -0.0322,  0.0746,  0.0208, -0.0899,  0.0828, -0.1211,  0.0465, -0.0999,\n",
            "         0.0647, -0.1266, -0.0418,  0.0750, -0.0296, -0.0531, -0.1162,  0.0257,\n",
            "         0.1040,  0.0804, -0.0954,  0.0349,  0.1079,  0.0911, -0.0595, -0.0762],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([1.0498, 1.0207, 1.0098, 1.0280, 1.0089, 0.9819, 0.9926, 1.0121, 1.0177,\n",
            "        0.9746, 0.9848, 1.0405, 1.0450, 0.9719, 1.0022, 0.9734, 1.0009, 0.9533,\n",
            "        0.9987, 1.0274, 1.0292, 1.0322, 0.9633, 1.0770, 1.0016, 1.0332, 1.0171,\n",
            "        1.0050, 1.0321, 1.0392, 1.0044, 1.0077, 0.9933, 1.0034, 0.9546, 1.0125,\n",
            "        0.9592, 0.9781, 0.9959, 0.9722, 0.9949, 0.9816, 1.0032, 1.0154, 0.9900,\n",
            "        0.9803, 0.9986, 1.0447, 0.9696, 1.1004, 1.0222, 0.9901, 1.0009, 1.0399,\n",
            "        0.9650, 0.9680, 1.0069, 1.0212, 1.0091, 0.9908, 1.0031, 1.0044, 1.0354,\n",
            "        0.9836], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([ 0.0593,  0.0040,  0.0135,  0.0040, -0.0014, -0.0162, -0.0117,  0.0291,\n",
            "         0.0280, -0.0203, -0.0361, -0.0078,  0.0432, -0.0191,  0.0238,  0.0100,\n",
            "        -0.0064,  0.0202, -0.0272,  0.0220, -0.0409,  0.0063, -0.0142, -0.0043,\n",
            "        -0.0123,  0.0582,  0.0149, -0.0305, -0.0069,  0.0238,  0.0020,  0.0533,\n",
            "        -0.0262, -0.0013, -0.0574, -0.0031, -0.0617, -0.0009,  0.0033, -0.0470,\n",
            "        -0.0163, -0.0264, -0.0056,  0.0024, -0.0045,  0.0259, -0.0328, -0.0187,\n",
            "        -0.0335, -0.0043, -0.0116, -0.0072,  0.0104,  0.0546, -0.0431,  0.0186,\n",
            "         0.0023, -0.0071,  0.0085,  0.0419,  0.0313, -0.0313,  0.0540, -0.0133],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([128, 64, 1]) Parameter containing:\n",
            "tensor([[[-0.0149],\n",
            "         [ 0.0385],\n",
            "         [ 0.0170],\n",
            "         ...,\n",
            "         [-0.0483],\n",
            "         [-0.0181],\n",
            "         [-0.0377]],\n",
            "\n",
            "        [[ 0.1288],\n",
            "         [-0.0281],\n",
            "         [ 0.0468],\n",
            "         ...,\n",
            "         [-0.0452],\n",
            "         [ 0.0033],\n",
            "         [ 0.0846]],\n",
            "\n",
            "        [[-0.0584],\n",
            "         [ 0.0571],\n",
            "         [-0.0406],\n",
            "         ...,\n",
            "         [-0.1426],\n",
            "         [-0.0720],\n",
            "         [ 0.0526]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0926],\n",
            "         [-0.0259],\n",
            "         [ 0.0021],\n",
            "         ...,\n",
            "         [ 0.0387],\n",
            "         [-0.1162],\n",
            "         [-0.0173]],\n",
            "\n",
            "        [[-0.0451],\n",
            "         [-0.1308],\n",
            "         [ 0.1099],\n",
            "         ...,\n",
            "         [ 0.0230],\n",
            "         [ 0.1470],\n",
            "         [ 0.1431]],\n",
            "\n",
            "        [[-0.0093],\n",
            "         [ 0.1000],\n",
            "         [ 0.1082],\n",
            "         ...,\n",
            "         [-0.0344],\n",
            "         [ 0.1103],\n",
            "         [-0.0211]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([-0.0918,  0.1003, -0.0768, -0.0983, -0.1100,  0.0230,  0.0965,  0.1009,\n",
            "         0.0422,  0.1001, -0.0315, -0.0490, -0.0929,  0.1202,  0.0623,  0.0294,\n",
            "         0.0737,  0.0065, -0.0078, -0.1156, -0.0986, -0.0647,  0.0993,  0.1198,\n",
            "         0.0206,  0.1167, -0.0315,  0.0416, -0.0292, -0.0659,  0.0115,  0.0889,\n",
            "        -0.0586,  0.0026, -0.0614,  0.0959,  0.0423, -0.0184, -0.0892, -0.0712,\n",
            "        -0.0276,  0.0709, -0.0954,  0.0627,  0.0031, -0.0980, -0.1036, -0.0752,\n",
            "        -0.0672,  0.0228,  0.0048, -0.0518,  0.0355,  0.0572, -0.0812,  0.1250,\n",
            "         0.0810,  0.0968,  0.0034, -0.1065,  0.1196,  0.0650,  0.0055, -0.1035,\n",
            "         0.0865,  0.0554,  0.0728, -0.0834, -0.0910,  0.1148,  0.0453, -0.0845,\n",
            "        -0.1118, -0.0747, -0.0649,  0.1054, -0.0227,  0.0407,  0.0475,  0.0633,\n",
            "        -0.0513, -0.1237, -0.0428,  0.0925, -0.0432, -0.0310, -0.0245, -0.0635,\n",
            "        -0.1052, -0.1158, -0.0013,  0.1140,  0.0862,  0.0900, -0.0760,  0.1185,\n",
            "         0.0177,  0.0664,  0.0728,  0.0916, -0.0870,  0.0564,  0.1256, -0.1084,\n",
            "        -0.0682, -0.1256, -0.0344,  0.0176, -0.0685, -0.0522, -0.1072, -0.0428,\n",
            "         0.0233, -0.0412,  0.1166, -0.0541, -0.0571,  0.0854,  0.0317, -0.0195,\n",
            "        -0.0271, -0.0652,  0.1140,  0.0863, -0.0539, -0.0128, -0.0616,  0.0090],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([0.9707, 0.9651, 0.9850, 0.9710, 1.0475, 1.0025, 0.9852, 0.9896, 0.9920,\n",
            "        1.0600, 0.9827, 1.0165, 0.9709, 1.0316, 0.9669, 0.9937, 1.0339, 1.0234,\n",
            "        0.9800, 1.0190, 1.0041, 1.0203, 0.9810, 0.9608, 1.0419, 0.9889, 0.9812,\n",
            "        1.0291, 0.9971, 0.9713, 0.9779, 0.9720, 1.0270, 0.9806, 0.9811, 0.9548,\n",
            "        1.0070, 0.9578, 1.0008, 0.9867, 1.0149, 0.9806, 1.0324, 0.9870, 1.0199,\n",
            "        0.9632, 1.0245, 0.9885, 1.0270, 1.0505, 1.0464, 0.9996, 1.0028, 1.0090,\n",
            "        0.9861, 1.0114, 0.9706, 1.0313, 1.0177, 1.0136, 0.9996, 1.0555, 1.0319,\n",
            "        1.1094, 1.0457, 1.0240, 0.9623, 1.0127, 1.0337, 0.9751, 1.0117, 1.0114,\n",
            "        0.9710, 0.9982, 1.0728, 0.9945, 1.0120, 1.0617, 0.9802, 1.0030, 0.9986,\n",
            "        0.9777, 1.0345, 0.9823, 1.0315, 1.0323, 1.0005, 0.9738, 0.9875, 0.9955,\n",
            "        1.0163, 0.9546, 0.9787, 0.9629, 0.9737, 1.0823, 0.9902, 1.0308, 0.9941,\n",
            "        0.9591, 1.0426, 1.0173, 1.0199, 0.9621, 1.0262, 0.9832, 1.0597, 1.0298,\n",
            "        1.0060, 0.9833, 1.0049, 1.0238, 1.0577, 1.0056, 1.0768, 1.0001, 0.9970,\n",
            "        1.0225, 0.9895, 0.9729, 1.0325, 0.9872, 0.9983, 1.0236, 0.9772, 1.0196,\n",
            "        0.9956, 1.0012], device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([-0.0104,  0.0550,  0.0254, -0.0134, -0.0151, -0.0185, -0.0161,  0.0198,\n",
            "         0.0251,  0.0399,  0.0100, -0.0050, -0.0490, -0.0270, -0.0202, -0.0464,\n",
            "         0.0516,  0.0356,  0.0434,  0.0295,  0.0265,  0.0068, -0.0098, -0.0346,\n",
            "        -0.0480, -0.0096, -0.0103,  0.0392,  0.0143, -0.0287,  0.0142, -0.0010,\n",
            "        -0.0033, -0.0531, -0.0053, -0.0324, -0.0465,  0.0337, -0.0499,  0.0588,\n",
            "         0.0365, -0.0370,  0.0156, -0.0264, -0.0257, -0.0176,  0.0241,  0.0470,\n",
            "         0.0062, -0.0085,  0.0034,  0.0303,  0.0340,  0.0133, -0.0601, -0.0071,\n",
            "        -0.0400,  0.0166, -0.0155,  0.0338, -0.0343, -0.0448,  0.0392,  0.0593,\n",
            "         0.0046,  0.0270, -0.0170,  0.0045,  0.0243, -0.0113, -0.0725,  0.0248,\n",
            "         0.0383, -0.0091,  0.0085, -0.0024, -0.0263, -0.0010, -0.0127, -0.0215,\n",
            "        -0.0155,  0.0029,  0.0351, -0.0309,  0.0245,  0.0132,  0.0069, -0.0071,\n",
            "         0.0368,  0.0494, -0.0091,  0.0086, -0.0177,  0.0165,  0.0095,  0.0198,\n",
            "         0.0171, -0.0065,  0.0206, -0.0324,  0.0235,  0.0262,  0.0229, -0.0268,\n",
            "         0.0022, -0.0205, -0.0050,  0.0520,  0.0136, -0.0195, -0.0504, -0.0122,\n",
            "        -0.0290, -0.0573, -0.0117,  0.0455,  0.0099,  0.0476, -0.0026, -0.0188,\n",
            "         0.0214,  0.0035,  0.0036,  0.0201, -0.0094,  0.0103, -0.0099, -0.0262],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024, 128, 1]) Parameter containing:\n",
            "tensor([[[-0.0079],\n",
            "         [-0.0315],\n",
            "         [-0.0647],\n",
            "         ...,\n",
            "         [ 0.0003],\n",
            "         [-0.0455],\n",
            "         [ 0.0370]],\n",
            "\n",
            "        [[ 0.0037],\n",
            "         [ 0.0341],\n",
            "         [-0.0856],\n",
            "         ...,\n",
            "         [ 0.0598],\n",
            "         [ 0.0923],\n",
            "         [ 0.0378]],\n",
            "\n",
            "        [[ 0.0448],\n",
            "         [-0.0841],\n",
            "         [ 0.0573],\n",
            "         ...,\n",
            "         [-0.0204],\n",
            "         [-0.1056],\n",
            "         [ 0.0464]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0709],\n",
            "         [ 0.0489],\n",
            "         [ 0.0037],\n",
            "         ...,\n",
            "         [-0.0164],\n",
            "         [ 0.0260],\n",
            "         [-0.0381]],\n",
            "\n",
            "        [[ 0.0258],\n",
            "         [-0.0953],\n",
            "         [ 0.0248],\n",
            "         ...,\n",
            "         [-0.1373],\n",
            "         [ 0.0346],\n",
            "         [-0.0449]],\n",
            "\n",
            "        [[ 0.0056],\n",
            "         [-0.0271],\n",
            "         [-0.0690],\n",
            "         ...,\n",
            "         [ 0.1259],\n",
            "         [-0.0789],\n",
            "         [-0.0514]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([ 0.0533, -0.0294,  0.0595,  ..., -0.0757, -0.0672, -0.0407],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([1.0343, 0.9703, 0.9990,  ..., 1.0930, 1.0063, 1.0545], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([ 0.1207,  0.1100,  0.0164,  ...,  0.0004,  0.0005, -0.0269],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512, 1024]) Parameter containing:\n",
            "tensor([[ 0.0125,  0.0021,  0.0117,  ..., -0.0506,  0.0095, -0.0028],\n",
            "        [ 0.0181, -0.0365, -0.0445,  ..., -0.0024,  0.0386, -0.0193],\n",
            "        [-0.0287, -0.0184,  0.0577,  ..., -0.0357,  0.0042, -0.0258],\n",
            "        ...,\n",
            "        [-0.0277, -0.0248, -0.0064,  ..., -0.0399,  0.0025,  0.0171],\n",
            "        [-0.0197,  0.0039, -0.0025,  ..., -0.0100, -0.0574,  0.0550],\n",
            "        [ 0.0305, -0.0206, -0.0328,  ..., -0.0848, -0.0432,  0.0136]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([ 0.0239,  0.0181,  0.0092,  0.0183,  0.0070, -0.0246,  0.0135, -0.0079,\n",
            "         0.0189, -0.0219,  0.0197,  0.0195,  0.0236,  0.0202, -0.0063,  0.0292,\n",
            "         0.0031, -0.0121,  0.0173,  0.0215,  0.0062, -0.0015, -0.0284,  0.0298,\n",
            "        -0.0238, -0.0012,  0.0272,  0.0136, -0.0027, -0.0192, -0.0093,  0.0031,\n",
            "        -0.0147,  0.0168, -0.0327,  0.0095, -0.0253, -0.0127, -0.0192,  0.0178,\n",
            "        -0.0201, -0.0040,  0.0004, -0.0274, -0.0260,  0.0193, -0.0029,  0.0168,\n",
            "         0.0070,  0.0195, -0.0214,  0.0209,  0.0259, -0.0288, -0.0286, -0.0268,\n",
            "        -0.0216, -0.0133,  0.0282,  0.0131,  0.0250,  0.0117,  0.0067, -0.0305,\n",
            "         0.0050, -0.0153,  0.0143, -0.0170,  0.0268, -0.0035,  0.0201,  0.0062,\n",
            "         0.0068, -0.0227, -0.0129, -0.0007,  0.0265,  0.0026,  0.0317,  0.0258,\n",
            "        -0.0202,  0.0079,  0.0146,  0.0293,  0.0324,  0.0254, -0.0273, -0.0044,\n",
            "        -0.0149,  0.0176, -0.0058,  0.0212, -0.0164,  0.0047, -0.0235,  0.0101,\n",
            "        -0.0138, -0.0251,  0.0066, -0.0127, -0.0037,  0.0187, -0.0142, -0.0169,\n",
            "         0.0299,  0.0115, -0.0015, -0.0171,  0.0005, -0.0303,  0.0288, -0.0227,\n",
            "        -0.0022, -0.0101,  0.0017, -0.0141, -0.0235,  0.0222,  0.0238, -0.0145,\n",
            "         0.0203,  0.0234, -0.0276,  0.0192, -0.0122,  0.0198, -0.0252,  0.0113,\n",
            "        -0.0033, -0.0015,  0.0120, -0.0043,  0.0217,  0.0103, -0.0193,  0.0280,\n",
            "         0.0188, -0.0280,  0.0223,  0.0282,  0.0276, -0.0087, -0.0224, -0.0180,\n",
            "         0.0224,  0.0192, -0.0294,  0.0115,  0.0158,  0.0127,  0.0250, -0.0008,\n",
            "        -0.0074, -0.0201,  0.0098, -0.0106, -0.0032, -0.0155,  0.0035,  0.0158,\n",
            "         0.0148,  0.0026,  0.0022, -0.0112,  0.0179,  0.0266, -0.0083, -0.0201,\n",
            "         0.0006,  0.0117, -0.0251,  0.0252, -0.0043, -0.0151, -0.0061, -0.0323,\n",
            "        -0.0032,  0.0296, -0.0164, -0.0076,  0.0057,  0.0093,  0.0310,  0.0023,\n",
            "        -0.0167,  0.0017, -0.0192,  0.0016,  0.0163, -0.0068,  0.0002,  0.0156,\n",
            "         0.0051,  0.0055,  0.0083, -0.0164, -0.0140, -0.0077, -0.0189, -0.0228,\n",
            "         0.0066,  0.0170, -0.0186,  0.0193, -0.0230,  0.0036,  0.0020,  0.0302,\n",
            "        -0.0240,  0.0251,  0.0153, -0.0007,  0.0142, -0.0255, -0.0289,  0.0069,\n",
            "         0.0136, -0.0194,  0.0243,  0.0105, -0.0249,  0.0217, -0.0070,  0.0155,\n",
            "        -0.0072, -0.0024, -0.0095,  0.0001,  0.0178,  0.0221,  0.0187,  0.0010,\n",
            "        -0.0025, -0.0258, -0.0113, -0.0016,  0.0263, -0.0018,  0.0091, -0.0265,\n",
            "         0.0288, -0.0163, -0.0026,  0.0092,  0.0014, -0.0173, -0.0055,  0.0277,\n",
            "         0.0034,  0.0036, -0.0277, -0.0193, -0.0062, -0.0095, -0.0047,  0.0144,\n",
            "         0.0064,  0.0258, -0.0166, -0.0240, -0.0013, -0.0192,  0.0154,  0.0283,\n",
            "         0.0082,  0.0194,  0.0307, -0.0051,  0.0266,  0.0052, -0.0002,  0.0104,\n",
            "         0.0219,  0.0236, -0.0283,  0.0297, -0.0144,  0.0220,  0.0321, -0.0298,\n",
            "        -0.0189,  0.0161, -0.0059, -0.0174, -0.0179,  0.0252,  0.0067, -0.0147,\n",
            "        -0.0093,  0.0227, -0.0186, -0.0323,  0.0289,  0.0289, -0.0201, -0.0308,\n",
            "        -0.0077, -0.0260, -0.0064,  0.0080, -0.0173,  0.0133, -0.0133,  0.0220,\n",
            "         0.0184,  0.0045, -0.0024, -0.0070,  0.0254, -0.0249,  0.0315,  0.0240,\n",
            "         0.0232,  0.0275, -0.0022,  0.0081, -0.0277,  0.0107,  0.0188, -0.0083,\n",
            "         0.0060, -0.0129,  0.0281, -0.0030,  0.0044, -0.0198,  0.0098,  0.0138,\n",
            "        -0.0139, -0.0050,  0.0146,  0.0164,  0.0091, -0.0251,  0.0057,  0.0296,\n",
            "         0.0187,  0.0047, -0.0149,  0.0200, -0.0142, -0.0243, -0.0161, -0.0289,\n",
            "        -0.0196,  0.0116, -0.0081, -0.0167, -0.0215,  0.0135,  0.0323,  0.0184,\n",
            "         0.0005,  0.0210,  0.0217,  0.0158,  0.0107,  0.0258, -0.0249,  0.0152,\n",
            "        -0.0163,  0.0053, -0.0132, -0.0237, -0.0034, -0.0259,  0.0022,  0.0050,\n",
            "        -0.0057, -0.0220,  0.0055,  0.0182,  0.0197, -0.0180, -0.0095,  0.0084,\n",
            "        -0.0161, -0.0125, -0.0091, -0.0091,  0.0255,  0.0106,  0.0334, -0.0196,\n",
            "         0.0241,  0.0154,  0.0263, -0.0328,  0.0142, -0.0236,  0.0104,  0.0077,\n",
            "         0.0104, -0.0181,  0.0048,  0.0155,  0.0297, -0.0082, -0.0158, -0.0170,\n",
            "        -0.0315, -0.0235,  0.0041,  0.0200,  0.0141, -0.0184,  0.0231,  0.0226,\n",
            "         0.0015, -0.0121,  0.0267, -0.0096, -0.0240, -0.0182, -0.0247, -0.0271,\n",
            "        -0.0080, -0.0222, -0.0210,  0.0066, -0.0301, -0.0036,  0.0260, -0.0232,\n",
            "        -0.0299,  0.0274, -0.0079, -0.0181, -0.0025, -0.0084, -0.0034, -0.0078,\n",
            "         0.0022, -0.0240, -0.0081, -0.0112, -0.0138,  0.0148,  0.0250,  0.0001,\n",
            "        -0.0071, -0.0036, -0.0291,  0.0120, -0.0226, -0.0064, -0.0298,  0.0211,\n",
            "        -0.0204,  0.0226,  0.0025,  0.0138, -0.0180,  0.0068, -0.0070,  0.0015,\n",
            "        -0.0243, -0.0059, -0.0112, -0.0177, -0.0177, -0.0201, -0.0148, -0.0263,\n",
            "        -0.0126,  0.0023,  0.0142,  0.0231,  0.0172, -0.0012,  0.0034,  0.0044,\n",
            "         0.0061, -0.0185, -0.0210,  0.0243,  0.0148,  0.0079,  0.0102,  0.0166,\n",
            "         0.0018, -0.0274,  0.0312, -0.0174,  0.0136, -0.0076,  0.0101, -0.0012,\n",
            "        -0.0117,  0.0068,  0.0253,  0.0076, -0.0064,  0.0172,  0.0166, -0.0148,\n",
            "         0.0021, -0.0295, -0.0131,  0.0188,  0.0165,  0.0184, -0.0021, -0.0265,\n",
            "         0.0282, -0.0124, -0.0225, -0.0130,  0.0113,  0.0002,  0.0305,  0.0269],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([0.8923, 1.0140, 0.9687, 0.9435, 0.9608, 0.9465, 0.9489, 0.9544, 0.9717,\n",
            "        1.0196, 1.0172, 0.9436, 0.9779, 0.9512, 1.0377, 1.0026, 0.9657, 0.9187,\n",
            "        1.0295, 0.9099, 0.9355, 0.8926, 1.0234, 1.0074, 0.9753, 0.9753, 1.0296,\n",
            "        0.9869, 1.0720, 1.0552, 0.9490, 0.9286, 0.9975, 0.9490, 0.9582, 0.9408,\n",
            "        1.0119, 0.9587, 0.9700, 0.9279, 0.9791, 0.9969, 0.9972, 0.9621, 0.9172,\n",
            "        1.0244, 0.9698, 0.9782, 1.0256, 0.9122, 0.9801, 0.9997, 0.9883, 1.0198,\n",
            "        0.9310, 0.9821, 0.9507, 0.9888, 0.9503, 0.9545, 1.1307, 0.9417, 0.9750,\n",
            "        0.9236, 1.0395, 0.9258, 0.9380, 0.9388, 1.0717, 1.0045, 1.0058, 0.9894,\n",
            "        0.9427, 0.9434, 1.0616, 0.9964, 1.0653, 0.9993, 0.9495, 0.9902, 0.9231,\n",
            "        0.9942, 1.0716, 1.1027, 1.0290, 1.0010, 0.9458, 1.0087, 1.0196, 0.9989,\n",
            "        0.9868, 0.9772, 1.0041, 0.9579, 1.0068, 1.0496, 1.0089, 0.9564, 0.9399,\n",
            "        0.9798, 0.9972, 1.0515, 0.9389, 0.9530, 0.9825, 1.0224, 1.0993, 1.0301,\n",
            "        0.9573, 1.1221, 0.9540, 0.9451, 0.9597, 1.0252, 1.0510, 0.9979, 0.9528,\n",
            "        0.9831, 0.9754, 1.0819, 1.0335, 0.9197, 1.0106, 1.0132, 1.0084, 0.9804,\n",
            "        0.9186, 1.0438, 1.0874, 1.0633, 1.0478, 1.0038, 0.9300, 0.9204, 0.9929,\n",
            "        0.9415, 0.9393, 0.9862, 1.0519, 1.0165, 0.9378, 0.9528, 0.9715, 0.9857,\n",
            "        0.9882, 0.9772, 0.9591, 1.0063, 1.0469, 1.0509, 0.9042, 0.9486, 0.9657,\n",
            "        0.9854, 0.9168, 0.9793, 1.0345, 1.0405, 1.0767, 1.0334, 0.9298, 0.9826,\n",
            "        0.9441, 0.9647, 1.0222, 0.9470, 1.0240, 1.0382, 0.9541, 0.9610, 0.9406,\n",
            "        0.9806, 0.9846, 1.0079, 1.0775, 1.0131, 1.1708, 0.9482, 1.0555, 0.9956,\n",
            "        0.9240, 1.0023, 0.9617, 1.0879, 0.9432, 0.9679, 1.0667, 1.0037, 1.1110,\n",
            "        0.9293, 0.9997, 1.1137, 0.9370, 0.9745, 1.0096, 1.0333, 0.9685, 0.9743,\n",
            "        0.9771, 0.9465, 0.9177, 1.0560, 0.9613, 1.0165, 1.0075, 1.0454, 1.0062,\n",
            "        0.9695, 0.9668, 0.9530, 1.0016, 1.0018, 1.0506, 1.0927, 0.9986, 0.9625,\n",
            "        0.9658, 1.0224, 1.0975, 0.9219, 0.9865, 0.9479, 0.9795, 0.9307, 1.0065,\n",
            "        1.0136, 0.9568, 1.0012, 0.9886, 1.0023, 1.0927, 0.9754, 1.0030, 0.9291,\n",
            "        1.0046, 0.9817, 1.0443, 1.1191, 1.0095, 0.9731, 1.1133, 1.0033, 0.9834,\n",
            "        0.9408, 1.0992, 0.9175, 1.0363, 0.9541, 1.0530, 0.9973, 1.0027, 1.0013,\n",
            "        1.0216, 1.0257, 0.9419, 0.9537, 0.9241, 1.0118, 1.0283, 0.9376, 1.0226,\n",
            "        1.0277, 0.9892, 0.9296, 0.9712, 1.0969, 1.0171, 1.0232, 0.9251, 0.9186,\n",
            "        0.9945, 1.0554, 1.0327, 1.0129, 1.0455, 1.0184, 0.9694, 0.9344, 1.0203,\n",
            "        1.0077, 0.9961, 1.0239, 1.0452, 1.0482, 0.9700, 0.9425, 0.9474, 0.9823,\n",
            "        0.9465, 0.9666, 1.0374, 1.0096, 0.9766, 0.9818, 1.0084, 0.9982, 0.9458,\n",
            "        1.0136, 0.9756, 0.9793, 1.0513, 0.9860, 1.0053, 1.0143, 0.9456, 0.9450,\n",
            "        0.9320, 0.9198, 0.9730, 1.0059, 0.9899, 0.9995, 0.9710, 1.0453, 1.0232,\n",
            "        0.9798, 0.9613, 0.9771, 1.0498, 1.0353, 1.0465, 0.9723, 1.0361, 0.9204,\n",
            "        1.1050, 1.0112, 1.0439, 0.9638, 0.9574, 1.0070, 1.0125, 1.0128, 0.9665,\n",
            "        1.0146, 1.1088, 0.9915, 0.8956, 1.0023, 0.9594, 1.0287, 1.0437, 1.0120,\n",
            "        0.9654, 0.9883, 0.9732, 0.9193, 1.0089, 0.9710, 0.9370, 0.9930, 0.9578,\n",
            "        0.9611, 0.9546, 0.9404, 1.0065, 0.9840, 1.0177, 0.9635, 1.0210, 1.0423,\n",
            "        1.0267, 1.0596, 0.9922, 1.0016, 0.9913, 1.0694, 0.9427, 1.0782, 1.0309,\n",
            "        0.9356, 0.9215, 1.0433, 1.0137, 0.9771, 1.0456, 0.9866, 0.9956, 1.0116,\n",
            "        1.0222, 0.9424, 0.9360, 1.0003, 1.0157, 1.0505, 0.9525, 0.9341, 1.0340,\n",
            "        1.0452, 1.0026, 0.9284, 1.1945, 1.0052, 1.0505, 0.9860, 1.0597, 0.9450,\n",
            "        0.9803, 0.9591, 1.0178, 1.0083, 0.9152, 1.0469, 0.9555, 0.9641, 0.9313,\n",
            "        1.0089, 0.9669, 0.9775, 0.9407, 0.9531, 0.9875, 0.8983, 0.9825, 1.0100,\n",
            "        1.0830, 0.9675, 0.9493, 0.9996, 1.0253, 0.9655, 1.0036, 0.9697, 0.9588,\n",
            "        0.9715, 0.9902, 0.9809, 1.0229, 1.0361, 0.9663, 0.9791, 0.9878, 0.9454,\n",
            "        1.0608, 1.0298, 1.0091, 1.0017, 0.9287, 0.9996, 0.9759, 1.0365, 0.9726,\n",
            "        0.9738, 0.9299, 1.0466, 0.9149, 0.9226, 0.9513, 1.0093, 1.0378, 0.9589,\n",
            "        0.9591, 0.9775, 0.9412, 1.0362, 1.0476, 0.9895, 0.9421, 0.9966, 1.0091,\n",
            "        0.9970, 1.0245, 1.0054, 0.9855, 0.9520, 0.9645, 0.9974, 0.9609, 1.0139,\n",
            "        0.9773, 0.9975, 0.9421, 1.0296, 1.0418, 0.9651, 0.9343, 0.9814, 1.0806,\n",
            "        1.0498, 1.0157, 1.1086, 0.8539, 1.1241, 1.0626, 0.9959, 0.9696, 0.9733,\n",
            "        0.9839, 0.9767, 0.9922, 0.9341, 0.9810, 0.9349, 0.9873, 0.9595, 0.9531,\n",
            "        0.9259, 1.0253, 0.9486, 1.0175, 0.9163, 0.9644, 1.0241, 1.0924, 0.9653,\n",
            "        0.9906, 0.9518, 1.0558, 1.0106, 0.9746, 0.9661, 1.0092, 1.0220],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([-0.0606,  0.0340,  0.0070, -0.0263,  0.0122,  0.0077, -0.0473,  0.0285,\n",
            "        -0.0007, -0.1063, -0.1353, -0.0819, -0.1262, -0.0444,  0.1113, -0.0633,\n",
            "        -0.0084,  0.0252, -0.0708, -0.0533,  0.0504, -0.0667, -0.1041,  0.0283,\n",
            "         0.0491, -0.0264, -0.0373, -0.1181,  0.1052, -0.0990,  0.0085, -0.0121,\n",
            "         0.0402, -0.1210, -0.0765, -0.0113, -0.0764,  0.0160, -0.0303, -0.0060,\n",
            "         0.0158, -0.1109, -0.0885, -0.0556, -0.0391,  0.0656,  0.0354,  0.1107,\n",
            "        -0.0523, -0.0335, -0.1190,  0.0508,  0.0390, -0.0737, -0.0249,  0.0766,\n",
            "         0.0143,  0.0641, -0.1139,  0.0467, -0.0937, -0.0075,  0.0321,  0.0043,\n",
            "         0.0224, -0.0243, -0.0719, -0.0414, -0.1991, -0.1089, -0.0979, -0.0007,\n",
            "        -0.0139,  0.0328,  0.0148, -0.0789,  0.0728,  0.0079, -0.0242, -0.1594,\n",
            "        -0.0223,  0.0955, -0.0468, -0.1326, -0.1189, -0.0753,  0.0567, -0.1823,\n",
            "         0.0744, -0.1373,  0.0243,  0.0879, -0.0507, -0.0884, -0.0619, -0.1307,\n",
            "        -0.0752, -0.0010,  0.0232,  0.0604, -0.0086,  0.0641, -0.0471, -0.1338,\n",
            "        -0.1018,  0.0375,  0.0291, -0.0520, -0.0294, -0.1194, -0.0163,  0.0043,\n",
            "         0.0553,  0.1047, -0.0220, -0.1531,  0.0030, -0.0023, -0.0236, -0.1023,\n",
            "        -0.1702, -0.0571, -0.1549, -0.0267, -0.0369,  0.1220, -0.0312, -0.0912,\n",
            "        -0.0904,  0.0406, -0.0523, -0.1285, -0.0473, -0.0144,  0.0059,  0.0072,\n",
            "        -0.0407,  0.0399, -0.0722,  0.0038, -0.0214, -0.0467,  0.0070, -0.0676,\n",
            "         0.0429, -0.0202, -0.0270,  0.0800,  0.0135, -0.0813, -0.0198,  0.0087,\n",
            "        -0.1831,  0.0102, -0.0003,  0.0221, -0.0690, -0.1703, -0.0515, -0.0823,\n",
            "        -0.0326, -0.1240,  0.0052,  0.0098, -0.0980,  0.1021,  0.0630,  0.0630,\n",
            "        -0.0830, -0.1309, -0.0144, -0.0097,  0.0540,  0.0092, -0.0358,  0.0522,\n",
            "        -0.0844, -0.0602, -0.1066, -0.0837, -0.0348, -0.0804,  0.0494, -0.0138,\n",
            "        -0.0389,  0.0169,  0.1480, -0.1368, -0.0045,  0.0738, -0.0261, -0.0299,\n",
            "        -0.0702,  0.0171, -0.1294, -0.0031, -0.1044, -0.0711, -0.0890,  0.0178,\n",
            "        -0.0643, -0.1453,  0.0332,  0.0639, -0.1221,  0.1103,  0.0197, -0.1138,\n",
            "         0.0017,  0.0721, -0.1315,  0.0248,  0.0149, -0.0707, -0.0142, -0.0334,\n",
            "        -0.1369, -0.0746, -0.0881, -0.1154,  0.0256,  0.0474, -0.0556,  0.0144,\n",
            "         0.0433, -0.0791,  0.1140,  0.0874, -0.0606, -0.0874, -0.0084, -0.0130,\n",
            "        -0.1215,  0.0133,  0.0532, -0.0158, -0.1366, -0.0185, -0.0231, -0.0250,\n",
            "         0.0242, -0.1468, -0.1071, -0.0415,  0.0292, -0.0852,  0.1282, -0.0306,\n",
            "        -0.1059, -0.0056, -0.1125,  0.0146,  0.0424, -0.0445, -0.0620, -0.1117,\n",
            "        -0.0166, -0.0008, -0.1132, -0.0103,  0.0193, -0.1662, -0.0487, -0.0430,\n",
            "        -0.0949, -0.0710, -0.0276, -0.0738, -0.0072, -0.0171, -0.1107, -0.1367,\n",
            "         0.0773, -0.1528, -0.0140, -0.0559, -0.0355, -0.0095, -0.0786, -0.0774,\n",
            "        -0.1458, -0.0837, -0.1729, -0.0188, -0.1316, -0.0633, -0.0738,  0.0366,\n",
            "        -0.0683, -0.1120, -0.1326,  0.0171,  0.0372, -0.0664,  0.0432, -0.0095,\n",
            "        -0.0404, -0.0291,  0.0692, -0.1634, -0.0791, -0.0448, -0.1759,  0.1480,\n",
            "         0.0285,  0.0019, -0.0404, -0.0383,  0.0246, -0.0816,  0.0530,  0.0110,\n",
            "        -0.1919, -0.0837, -0.0287, -0.0251, -0.1375, -0.0057, -0.0430,  0.0038,\n",
            "        -0.0671, -0.1072, -0.0451,  0.0455, -0.0972, -0.1679,  0.0107,  0.0467,\n",
            "         0.0863, -0.1375, -0.0321,  0.0593, -0.0241, -0.0647, -0.0338, -0.1125,\n",
            "        -0.0218, -0.0966, -0.0274, -0.0957, -0.1549,  0.0405, -0.1422, -0.1112,\n",
            "        -0.1369, -0.1296,  0.0587,  0.0011, -0.1545,  0.0353,  0.0520, -0.0580,\n",
            "        -0.0769, -0.0116,  0.0499, -0.0259, -0.0403,  0.0027, -0.0912, -0.1899,\n",
            "         0.0512, -0.0154, -0.1085, -0.1747, -0.0946, -0.0088,  0.0393, -0.0580,\n",
            "        -0.0329, -0.0358, -0.0290, -0.1362, -0.1449, -0.0235, -0.0404, -0.0336,\n",
            "        -0.0928,  0.0539, -0.0778, -0.0036, -0.0299, -0.0049, -0.1387, -0.0114,\n",
            "        -0.0041, -0.0434,  0.0345, -0.1254,  0.0787, -0.0194, -0.0491, -0.1195,\n",
            "        -0.1151,  0.0360, -0.0599, -0.0314, -0.1563, -0.0244, -0.1413,  0.0028,\n",
            "         0.0253, -0.0187,  0.0291, -0.0406, -0.0621,  0.0171, -0.0130, -0.1204,\n",
            "         0.0487, -0.1336, -0.0569,  0.0084, -0.1714,  0.0697, -0.1448, -0.0358,\n",
            "         0.0082, -0.0613, -0.1298, -0.0946,  0.0963,  0.0345, -0.1155,  0.0320,\n",
            "        -0.0453, -0.0867, -0.1675, -0.0663,  0.0296,  0.0026, -0.1791, -0.0069,\n",
            "        -0.0441, -0.0823, -0.0015, -0.1575, -0.0330, -0.0361,  0.0145,  0.0709,\n",
            "         0.0045, -0.1096, -0.1171, -0.1114, -0.0119, -0.1664, -0.0643, -0.0595,\n",
            "         0.0649, -0.0322, -0.0238,  0.0159, -0.1083, -0.0416, -0.0226,  0.0780,\n",
            "        -0.0332, -0.0585, -0.1567, -0.0353, -0.0389,  0.0012,  0.0408, -0.0137,\n",
            "         0.0185, -0.1555,  0.0069, -0.0851, -0.0172, -0.1017, -0.0315, -0.1407,\n",
            "        -0.0277, -0.0301, -0.0107,  0.0174,  0.0422, -0.0884, -0.0488, -0.0845,\n",
            "        -0.1152, -0.0549,  0.0368, -0.1559, -0.0431, -0.0822, -0.0368,  0.1133,\n",
            "         0.0616,  0.0180,  0.0538, -0.0122,  0.0220,  0.0368, -0.0206, -0.0192,\n",
            "        -0.0827, -0.0032, -0.1230, -0.1150,  0.0301, -0.0815,  0.0229, -0.0597,\n",
            "         0.0539, -0.1293, -0.0104, -0.0237, -0.1192,  0.0229, -0.1436, -0.0849],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([256, 512]) Parameter containing:\n",
            "tensor([[ 0.0362,  0.0552, -0.0188,  ..., -0.0219, -0.0187, -0.0243],\n",
            "        [-0.0089,  0.0097,  0.0220,  ...,  0.0155,  0.0326, -0.0389],\n",
            "        [ 0.0281, -0.0313,  0.0536,  ...,  0.0350, -0.0417,  0.0098],\n",
            "        ...,\n",
            "        [ 0.0609,  0.0174,  0.0385,  ..., -0.0522,  0.0340,  0.0030],\n",
            "        [ 0.0072,  0.0368,  0.0321,  ...,  0.0233, -0.0489, -0.0757],\n",
            "        [ 0.0376,  0.0343, -0.0259,  ...,  0.0463, -0.0218,  0.0194]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([-2.3464e-02,  3.0682e-04, -1.1555e-02, -8.3636e-03, -4.0072e-02,\n",
            "         3.9369e-02, -3.5729e-03, -2.8090e-02, -3.5089e-02,  4.8218e-02,\n",
            "         1.7368e-02,  2.4280e-02,  2.0489e-02,  2.2906e-02,  1.0579e-02,\n",
            "        -1.8694e-04, -1.3520e-02,  3.7588e-02, -7.0483e-03, -4.0764e-02,\n",
            "         8.6598e-03, -7.5300e-03, -4.3191e-03, -3.7255e-02, -2.5334e-02,\n",
            "         2.0493e-02,  1.0459e-02,  1.8418e-02, -5.2431e-03, -2.7245e-03,\n",
            "        -5.9834e-02, -4.2074e-02, -3.5722e-02,  3.3779e-02,  4.0616e-02,\n",
            "         3.0019e-02, -3.4198e-02,  5.9243e-03, -1.5624e-02, -4.4198e-03,\n",
            "        -5.3243e-03,  1.0652e-02, -7.8013e-04, -3.2216e-02,  9.1697e-03,\n",
            "         3.4106e-02, -5.7055e-03, -3.1655e-03, -8.8286e-03, -3.7102e-02,\n",
            "         2.7236e-02,  4.0856e-02, -1.6731e-02,  3.9296e-02, -2.6519e-02,\n",
            "         6.8082e-03,  1.0175e-02, -1.2290e-02, -2.2210e-02,  1.5676e-02,\n",
            "         2.9943e-03,  1.6833e-02, -8.3722e-03,  1.3573e-02, -2.2453e-02,\n",
            "         2.1071e-02, -2.3727e-02, -2.8385e-03,  1.8772e-02, -7.4291e-03,\n",
            "         4.4235e-02, -3.9776e-02,  5.7938e-03, -4.0128e-02,  1.2106e-02,\n",
            "         2.1076e-02, -7.0449e-03,  2.0183e-02, -4.1322e-02,  4.6121e-03,\n",
            "         1.1046e-03, -2.1174e-02,  3.6286e-03,  3.9752e-02, -3.9512e-02,\n",
            "        -1.5884e-02,  2.1503e-02,  2.9551e-03,  3.3364e-02, -3.9265e-02,\n",
            "         4.2644e-02, -2.0205e-02, -1.4129e-02, -2.1449e-02, -2.6056e-02,\n",
            "        -1.7735e-02, -3.1119e-02,  3.4722e-02,  2.4922e-02,  1.5684e-02,\n",
            "         8.5946e-03, -6.9210e-03, -1.5447e-02,  1.4498e-02,  4.2662e-02,\n",
            "         1.9352e-02,  3.9184e-02,  1.0011e-02,  3.7533e-02,  4.0589e-02,\n",
            "        -4.1646e-02,  1.1122e-02,  1.9627e-02,  3.5732e-02, -2.3124e-02,\n",
            "         1.0400e-02, -3.0588e-02,  2.2515e-02, -3.8828e-02, -1.1303e-02,\n",
            "        -4.3462e-02,  2.0114e-02,  3.3188e-03, -1.1990e-02,  3.5084e-02,\n",
            "        -6.4105e-03,  4.0365e-02,  4.3133e-02,  2.6772e-02, -1.5073e-02,\n",
            "        -4.0289e-02, -2.0050e-02,  3.6715e-03,  2.8842e-04,  1.7756e-02,\n",
            "         4.1067e-02,  1.7578e-02,  2.9157e-02,  3.6623e-02, -4.3700e-02,\n",
            "        -7.2152e-03, -2.6624e-02,  2.6605e-02,  3.1173e-03,  7.6241e-03,\n",
            "         2.0046e-02,  1.9397e-02,  2.7918e-02, -2.8936e-02, -1.8197e-02,\n",
            "         3.8781e-02,  2.4401e-02, -2.1437e-02,  2.3162e-02,  1.1307e-02,\n",
            "        -4.4589e-02,  2.6434e-02, -1.9328e-02,  7.2332e-03, -1.8810e-02,\n",
            "         1.7967e-02, -1.5422e-02,  2.1758e-02, -2.3056e-02,  5.9530e-03,\n",
            "         2.8612e-02,  3.5304e-02,  1.9630e-02, -2.1566e-02,  9.6796e-03,\n",
            "         2.3063e-02, -1.1877e-02, -1.7470e-02, -5.2340e-03, -4.4792e-02,\n",
            "         2.0928e-02,  1.8391e-03,  4.0048e-02, -9.5068e-03, -1.9375e-02,\n",
            "         1.4966e-02, -2.1052e-02, -2.1186e-02,  4.0461e-02, -1.3101e-02,\n",
            "        -5.5389e-03,  1.5146e-02,  4.6253e-03, -2.3373e-02,  4.2011e-02,\n",
            "        -2.7761e-02, -3.2141e-02, -3.7885e-02,  1.6613e-02,  2.7463e-02,\n",
            "        -1.7876e-02, -3.5678e-02, -2.3252e-03, -2.3146e-02, -3.3930e-02,\n",
            "         4.0148e-02, -2.4146e-02, -6.9933e-03,  2.2337e-02, -4.1781e-02,\n",
            "         3.9323e-02,  5.8604e-03, -2.6819e-02, -2.5406e-02,  4.7978e-02,\n",
            "        -3.5171e-02,  1.8612e-02, -3.0842e-02,  2.2712e-02, -2.1170e-02,\n",
            "         3.3891e-02,  2.6537e-02, -1.0473e-02,  1.6064e-02, -3.3365e-02,\n",
            "         1.0959e-03, -3.9489e-02, -3.2284e-02,  6.2282e-03, -2.7911e-02,\n",
            "         1.0351e-03,  4.0717e-02, -2.4257e-03,  4.1773e-02,  2.6189e-02,\n",
            "        -3.4818e-03,  1.1558e-02, -3.1671e-02, -1.1136e-03,  4.3585e-02,\n",
            "         1.7943e-02, -9.8543e-04, -8.0853e-03, -1.0748e-02,  2.3351e-02,\n",
            "        -3.6573e-02, -3.1999e-02, -3.9401e-02, -4.2066e-02, -2.1045e-02,\n",
            "         2.7196e-02, -3.2364e-02,  3.4089e-02,  1.6160e-02,  3.0058e-02,\n",
            "         1.6167e-02,  3.0540e-02,  1.7972e-05,  2.4289e-02,  3.3646e-02,\n",
            "         3.6419e-02], device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([0.6706, 0.6741, 0.8070, 0.7729, 0.8076, 0.5937, 0.7395, 0.6304, 0.7474,\n",
            "        0.7948, 0.7995, 0.5757, 0.7781, 0.7685, 0.6850, 0.7514, 0.7493, 0.7414,\n",
            "        0.7306, 0.7513, 0.7538, 0.7475, 0.6605, 0.8394, 0.7044, 0.8448, 0.6536,\n",
            "        0.8172, 0.8378, 0.7314, 0.7968, 0.7259, 0.7231, 0.7528, 0.7856, 0.7816,\n",
            "        0.5176, 0.7739, 0.7747, 0.7517, 0.7047, 0.7214, 0.7497, 0.7617, 0.7270,\n",
            "        0.6838, 0.7179, 0.7067, 0.6473, 0.7738, 0.6670, 0.7426, 0.6196, 0.6524,\n",
            "        0.7837, 0.6130, 0.7112, 0.7123, 0.4843, 0.7504, 0.5323, 0.7720, 0.7312,\n",
            "        0.7539, 0.7488, 0.6879, 0.7856, 0.8605, 0.7046, 0.6850, 0.6668, 0.6227,\n",
            "        0.6198, 0.7637, 0.6950, 0.5976, 0.7208, 0.7423, 0.8113, 0.7905, 0.7574,\n",
            "        0.6805, 0.7291, 0.7716, 0.7374, 0.7455, 0.7343, 0.7868, 0.8501, 0.7196,\n",
            "        0.8046, 0.7697, 0.8389, 0.7454, 0.7364, 0.7580, 0.8238, 0.7572, 0.7766,\n",
            "        0.6935, 0.8231, 0.8141, 0.6102, 0.8339, 0.7703, 0.7215, 0.6814, 0.8048,\n",
            "        0.7627, 0.6796, 0.6241, 0.7669, 0.8185, 0.7441, 0.6088, 0.6966, 0.8187,\n",
            "        0.7235, 0.7151, 0.8151, 0.8268, 0.7181, 0.7037, 0.7769, 0.7245, 0.6697,\n",
            "        0.7712, 0.8393, 0.7251, 0.8850, 0.8013, 0.7003, 0.7989, 0.6847, 0.7024,\n",
            "        0.7421, 0.6834, 0.7777, 0.7731, 0.7572, 0.6981, 0.7338, 0.8074, 0.7210,\n",
            "        0.7060, 0.7211, 0.9135, 0.6712, 0.7348, 0.8182, 0.7364, 0.7946, 0.7491,\n",
            "        0.8127, 0.6957, 0.7261, 0.7963, 0.7559, 0.8454, 0.6472, 0.8562, 0.8017,\n",
            "        0.7822, 0.7132, 0.7186, 0.6994, 0.7358, 0.7309, 0.6845, 0.7851, 0.6968,\n",
            "        0.7448, 0.7734, 0.7624, 0.6542, 0.8242, 0.5757, 0.7612, 0.7366, 0.7884,\n",
            "        0.8043, 0.8852, 0.7111, 0.7578, 0.7338, 0.6144, 0.6782, 0.8043, 0.5216,\n",
            "        0.7038, 0.8542, 0.7338, 0.7800, 0.6543, 0.6533, 0.8226, 0.7507, 0.9218,\n",
            "        0.7105, 0.8056, 0.7020, 0.7748, 0.7661, 0.6859, 0.7224, 0.7688, 0.5821,\n",
            "        0.6956, 0.7008, 0.5352, 0.7825, 0.6542, 0.7774, 0.6866, 0.6303, 0.7622,\n",
            "        0.7860, 0.7710, 0.8708, 0.6547, 0.6908, 0.6657, 0.6978, 0.6256, 0.5797,\n",
            "        0.7187, 0.7681, 0.8171, 0.7881, 0.7763, 0.7939, 0.7029, 0.7870, 0.7572,\n",
            "        0.6934, 0.7065, 0.7447, 0.6426, 0.6513, 0.6998, 0.6870, 0.8321, 0.7484,\n",
            "        0.8065, 0.8027, 0.8185, 0.7032, 0.7730, 0.8014, 0.7300, 0.8902, 0.7751,\n",
            "        0.8557, 0.6961, 0.8429, 0.5252], device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([-0.3891, -0.3656, -0.3057, -0.2592, -0.1830, -0.3571, -0.2817, -0.3783,\n",
            "        -0.3887, -0.2958, -0.2291, -0.3211, -0.2405, -0.3598, -0.3251, -0.3648,\n",
            "        -0.2932, -0.2996, -0.3714, -0.3076, -0.3173, -0.2610, -0.3450, -0.2656,\n",
            "        -0.2822, -0.2061, -0.4101, -0.2446, -0.2277, -0.3059, -0.2435, -0.3411,\n",
            "        -0.3337, -0.3776, -0.2257, -0.2310, -0.4492, -0.2794, -0.2970, -0.3329,\n",
            "        -0.3078, -0.3619, -0.2672, -0.3168, -0.3248, -0.2633, -0.2799, -0.3864,\n",
            "        -0.5432, -0.1959, -0.3747, -0.2710, -0.3291, -0.4044, -0.2576, -0.4304,\n",
            "        -0.3488, -0.3270, -0.4303, -0.3735, -0.4413, -0.2333, -0.3813, -0.3105,\n",
            "        -0.2436, -0.4374, -0.3228, -0.2128, -0.3269, -0.3739, -0.3412, -0.3625,\n",
            "        -0.4541, -0.2712, -0.3822, -0.4149, -0.3153, -0.3480, -0.2641, -0.2411,\n",
            "        -0.3368, -0.3613, -0.2928, -0.2846, -0.3276, -0.3957, -0.3164, -0.2400,\n",
            "        -0.1860, -0.3796, -0.1813, -0.2433, -0.2278, -0.2960, -0.3338, -0.2586,\n",
            "        -0.3307, -0.2700, -0.3502, -0.3528, -0.2452, -0.1644, -0.3571, -0.2013,\n",
            "        -0.3500, -0.3311, -0.3681, -0.2781, -0.2606, -0.3815, -0.3568, -0.3414,\n",
            "        -0.2136, -0.3295, -0.3806, -0.4062, -0.1619, -0.3760, -0.3152, -0.2804,\n",
            "        -0.2074, -0.2850, -0.3486, -0.3180, -0.3584, -0.4026, -0.3377, -0.2196,\n",
            "        -0.3424, -0.1698, -0.2518, -0.3404, -0.2823, -0.3861, -0.3705, -0.3260,\n",
            "        -0.3424, -0.2453, -0.2983, -0.3343, -0.4155, -0.3471, -0.3035, -0.3220,\n",
            "        -0.2845, -0.3853, -0.1245, -0.3294, -0.2676, -0.2251, -0.3652, -0.3223,\n",
            "        -0.3723, -0.2696, -0.3264, -0.2846, -0.3690, -0.3515, -0.2621, -0.4138,\n",
            "        -0.2746, -0.2392, -0.3422, -0.3560, -0.3381, -0.3614, -0.2761, -0.3256,\n",
            "        -0.3763, -0.2927, -0.3455, -0.2930, -0.2865, -0.3221, -0.5167, -0.2060,\n",
            "        -0.2911, -0.3205, -0.2753, -0.2758, -0.3082, -0.1127, -0.4351, -0.3588,\n",
            "        -0.3391, -0.5237, -0.4078, -0.1718, -0.4749, -0.2881, -0.2097, -0.3219,\n",
            "        -0.2415, -0.3839, -0.4048, -0.2432, -0.3908, -0.1094, -0.3809, -0.2566,\n",
            "        -0.3237, -0.3184, -0.2787, -0.3404, -0.2904, -0.3096, -0.4030, -0.2832,\n",
            "        -0.3130, -0.3968, -0.2787, -0.3943, -0.3131, -0.3817, -0.3764, -0.2761,\n",
            "        -0.3061, -0.3180, -0.1639, -0.3820, -0.3794, -0.3522, -0.3591, -0.2774,\n",
            "        -0.3390, -0.3294, -0.3204, -0.2457, -0.2901, -0.2374, -0.2558, -0.3761,\n",
            "        -0.2952, -0.3391, -0.3275, -0.3705, -0.3092, -0.3522, -0.3880, -0.3474,\n",
            "        -0.2975, -0.2154, -0.2727, -0.2249, -0.2273, -0.2101, -0.3511, -0.3542,\n",
            "        -0.3048, -0.3892, -0.1542, -0.2478, -0.2180, -0.3640, -0.2506, -0.4497],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([4096, 256]) Parameter containing:\n",
            "tensor([[ 0.0022,  0.0109, -0.0024,  ..., -0.0093,  0.0049, -0.0029],\n",
            "        [ 0.0206,  0.0236, -0.0008,  ..., -0.0043, -0.0018,  0.0046],\n",
            "        [ 0.0026,  0.0062,  0.0035,  ...,  0.0040,  0.0055,  0.0019],\n",
            "        ...,\n",
            "        [-0.0082, -0.0142, -0.0203,  ...,  0.0043, -0.0094, -0.0069],\n",
            "        [-0.0175, -0.0205, -0.0192,  ...,  0.0106, -0.0087, -0.0142],\n",
            "        [-0.0119, -0.0066, -0.0010,  ..., -0.0056, -0.0167, -0.0173]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([4096]) Parameter containing:\n",
            "tensor([ 9.9659e-01, -1.5313e-02,  5.2970e-03,  ...,  1.0045e-02,\n",
            "         3.6152e-04,  1.0135e+00], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64, 64, 1]) Parameter containing:\n",
            "tensor([[[-0.0273],\n",
            "         [-0.0457],\n",
            "         [ 0.0875],\n",
            "         ...,\n",
            "         [ 0.0694],\n",
            "         [-0.0782],\n",
            "         [-0.0016]],\n",
            "\n",
            "        [[-0.0767],\n",
            "         [-0.0302],\n",
            "         [ 0.1030],\n",
            "         ...,\n",
            "         [-0.0425],\n",
            "         [-0.0286],\n",
            "         [ 0.0454]],\n",
            "\n",
            "        [[-0.0982],\n",
            "         [-0.0403],\n",
            "         [-0.0207],\n",
            "         ...,\n",
            "         [ 0.0375],\n",
            "         [-0.0851],\n",
            "         [-0.0553]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 0.0991],\n",
            "         [ 0.0032],\n",
            "         [ 0.0266],\n",
            "         ...,\n",
            "         [-0.0687],\n",
            "         [ 0.0523],\n",
            "         [ 0.0460]],\n",
            "\n",
            "        [[-0.0208],\n",
            "         [-0.0930],\n",
            "         [ 0.1015],\n",
            "         ...,\n",
            "         [-0.0190],\n",
            "         [ 0.0357],\n",
            "         [ 0.0554]],\n",
            "\n",
            "        [[-0.0464],\n",
            "         [ 0.0436],\n",
            "         [-0.0246],\n",
            "         ...,\n",
            "         [-0.0773],\n",
            "         [ 0.1256],\n",
            "         [-0.0423]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([-0.0727,  0.0342, -0.1127, -0.0729,  0.0759, -0.0909,  0.0628,  0.0782,\n",
            "         0.0244,  0.1051, -0.0320,  0.1080,  0.0762,  0.0386, -0.0424, -0.0236,\n",
            "         0.0466,  0.0476,  0.0780,  0.0261,  0.0491, -0.0659,  0.0709, -0.0575,\n",
            "        -0.0591,  0.0507, -0.0412,  0.1078, -0.1249,  0.0224, -0.0576,  0.1203,\n",
            "        -0.0569, -0.1179, -0.0202,  0.0484,  0.0057, -0.0284, -0.1018,  0.0087,\n",
            "        -0.0621, -0.1103,  0.0323,  0.0870, -0.0342, -0.0410, -0.0004, -0.1055,\n",
            "         0.0753, -0.1208,  0.1184,  0.0517,  0.1258, -0.1137, -0.1105, -0.0108,\n",
            "         0.0854,  0.0232, -0.0254, -0.1189,  0.1175, -0.1160,  0.1110,  0.0541],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([1.0338, 1.0033, 1.0639, 1.0135, 1.0773, 0.9804, 0.9969, 1.0878, 1.0966,\n",
            "        1.0927, 0.9332, 1.0156, 1.0239, 0.9481, 0.9233, 1.0053, 0.9301, 0.9724,\n",
            "        0.9190, 1.0588, 0.9084, 0.9660, 0.9368, 0.9057, 0.8970, 1.0514, 1.0141,\n",
            "        0.9445, 1.0009, 0.8401, 0.9238, 0.9427, 0.9566, 0.9926, 0.8908, 0.9147,\n",
            "        1.0876, 0.9057, 0.9231, 1.0693, 1.0643, 0.9270, 1.0173, 0.9375, 1.0358,\n",
            "        1.0267, 0.9150, 1.0094, 0.8765, 1.0388, 0.9401, 1.0424, 1.0277, 0.8968,\n",
            "        1.0064, 0.9715, 0.9331, 0.9588, 0.9601, 1.0952, 0.9205, 0.9587, 1.0388,\n",
            "        1.0582], device='cuda:0', requires_grad=True)\n",
            "torch.Size([64]) Parameter containing:\n",
            "tensor([-0.0621, -0.1739, -0.0109, -0.0970, -0.1978, -0.1330, -0.1090, -0.1242,\n",
            "         0.0031, -0.0294,  0.1202, -0.0172, -0.0663,  0.0671,  0.0667, -0.1063,\n",
            "        -0.1696, -0.0794,  0.0036,  0.0247,  0.0442,  0.0775,  0.0407,  0.0629,\n",
            "         0.0072, -0.0572, -0.0642,  0.1373, -0.1093, -0.0883, -0.0999, -0.0575,\n",
            "        -0.0123, -0.0734,  0.0789,  0.0100, -0.0544,  0.0042,  0.0357, -0.0705,\n",
            "        -0.0019,  0.0595, -0.0560,  0.0573, -0.0665, -0.1746,  0.0575, -0.0863,\n",
            "        -0.0043,  0.0252,  0.0447,  0.0102,  0.0090, -0.0675, -0.0932,  0.1492,\n",
            "         0.0292,  0.0601, -0.1749,  0.0075,  0.0155,  0.0038, -0.0675,  0.0248],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([128, 64, 1]) Parameter containing:\n",
            "tensor([[[ 2.9665e-02],\n",
            "         [ 1.0450e-01],\n",
            "         [-6.2883e-02],\n",
            "         ...,\n",
            "         [-2.9053e-02],\n",
            "         [ 3.1076e-02],\n",
            "         [ 7.6302e-02]],\n",
            "\n",
            "        [[ 2.3276e-02],\n",
            "         [ 2.2004e-02],\n",
            "         [-1.5641e-01],\n",
            "         ...,\n",
            "         [ 6.9698e-02],\n",
            "         [-1.6412e-01],\n",
            "         [-1.0171e-01]],\n",
            "\n",
            "        [[ 8.9099e-02],\n",
            "         [ 4.3478e-02],\n",
            "         [ 7.0803e-02],\n",
            "         ...,\n",
            "         [-7.8111e-02],\n",
            "         [ 1.0277e-01],\n",
            "         [-1.3790e-01]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[ 1.1486e-01],\n",
            "         [-6.8688e-03],\n",
            "         [ 3.9586e-05],\n",
            "         ...,\n",
            "         [ 3.8912e-02],\n",
            "         [ 3.2973e-02],\n",
            "         [ 1.3275e-01]],\n",
            "\n",
            "        [[-5.9266e-02],\n",
            "         [-1.7916e-02],\n",
            "         [ 7.1135e-02],\n",
            "         ...,\n",
            "         [ 4.7874e-02],\n",
            "         [ 8.6072e-02],\n",
            "         [ 1.0178e-02]],\n",
            "\n",
            "        [[ 1.4481e-01],\n",
            "         [-7.2530e-02],\n",
            "         [ 8.1057e-02],\n",
            "         ...,\n",
            "         [ 8.0870e-02],\n",
            "         [ 7.5238e-03],\n",
            "         [ 5.9719e-02]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([-0.0027,  0.1165,  0.0553, -0.0036,  0.0597,  0.1193, -0.0668,  0.0632,\n",
            "         0.0424, -0.0716,  0.0290,  0.0378,  0.0372,  0.0793, -0.0380, -0.0078,\n",
            "         0.0112, -0.0992, -0.0191,  0.1179,  0.0601,  0.0283, -0.0046,  0.0479,\n",
            "        -0.1186,  0.0392,  0.0852,  0.0767,  0.0515, -0.1178,  0.1087,  0.0579,\n",
            "        -0.1176, -0.0707,  0.0453,  0.0084, -0.0576,  0.0202, -0.0995,  0.0967,\n",
            "        -0.0184,  0.0042, -0.0972, -0.0196,  0.0254,  0.0284,  0.0718,  0.1186,\n",
            "         0.0503, -0.0938,  0.0908, -0.0159, -0.1000,  0.1068, -0.0692, -0.1178,\n",
            "        -0.0648, -0.0170,  0.0327,  0.0281,  0.0004,  0.0285, -0.0992,  0.0216,\n",
            "        -0.0327,  0.0830,  0.0690,  0.1036,  0.0831,  0.0192,  0.0002, -0.1152,\n",
            "        -0.1219, -0.1194,  0.0349, -0.1027, -0.0469, -0.0553, -0.0119, -0.0822,\n",
            "         0.0725, -0.0791,  0.0684,  0.0733, -0.0668,  0.0568, -0.0057, -0.0136,\n",
            "        -0.0012,  0.0176, -0.0522, -0.0053,  0.1047, -0.0011,  0.0053,  0.0151,\n",
            "         0.0170,  0.0142, -0.0755, -0.0685,  0.0712,  0.1171,  0.1099,  0.0653,\n",
            "        -0.1161,  0.0784, -0.0284, -0.0238,  0.0069, -0.0409, -0.1074, -0.0986,\n",
            "         0.1075,  0.0408, -0.0036,  0.0715, -0.0618, -0.0647,  0.0985, -0.0152,\n",
            "        -0.0633,  0.0663, -0.0913,  0.0496,  0.0678,  0.0983,  0.0895,  0.0847],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([0.9781, 0.8939, 0.8213, 0.9185, 1.0703, 1.1054, 0.9880, 1.0320, 0.9841,\n",
            "        1.0609, 1.0907, 0.8303, 0.9537, 0.9494, 1.0085, 0.9333, 0.9272, 0.9320,\n",
            "        0.9983, 1.0693, 1.0574, 0.9297, 0.9112, 0.9585, 0.9850, 1.0463, 1.0309,\n",
            "        0.8883, 0.9524, 1.0891, 1.0993, 0.9593, 0.9355, 0.9080, 0.9734, 0.9350,\n",
            "        1.0776, 1.0478, 0.8681, 0.9258, 1.0376, 1.0550, 0.9431, 0.9271, 1.0268,\n",
            "        1.0607, 1.0000, 1.0557, 0.9665, 1.0571, 1.0402, 1.0300, 0.9137, 1.0469,\n",
            "        0.9125, 1.0586, 0.9804, 0.9484, 0.9256, 0.9783, 0.9726, 1.0609, 0.9991,\n",
            "        0.9429, 0.8940, 0.9969, 0.8708, 1.0754, 1.1158, 1.0433, 0.9129, 0.9671,\n",
            "        1.0101, 0.9311, 1.0146, 1.0341, 0.8808, 0.9094, 0.8301, 1.0466, 1.0464,\n",
            "        0.8907, 0.8830, 1.0190, 0.9531, 1.0337, 0.9311, 1.0501, 1.0316, 1.0406,\n",
            "        0.9916, 1.0674, 0.9348, 0.8447, 0.9768, 0.9725, 1.0294, 0.9221, 0.9434,\n",
            "        1.0299, 1.0699, 0.9714, 0.9841, 0.9559, 0.9443, 1.0128, 1.0317, 0.9186,\n",
            "        0.9364, 1.0941, 0.9706, 0.9090, 1.0283, 0.9851, 0.9301, 0.9893, 0.9673,\n",
            "        1.0038, 1.0314, 0.9810, 0.8627, 1.0288, 0.8853, 1.0908, 0.9044, 1.0325,\n",
            "        1.0295, 1.0390], device='cuda:0', requires_grad=True)\n",
            "torch.Size([128]) Parameter containing:\n",
            "tensor([-5.5734e-03,  2.9652e-02, -1.4127e-01,  2.0392e-02, -4.3515e-02,\n",
            "        -6.2025e-02,  2.4225e-02, -5.5124e-02,  4.2778e-02, -1.9889e-01,\n",
            "        -1.1962e-01, -8.2574e-02,  8.5680e-02,  1.8194e-01,  1.5929e-04,\n",
            "         3.9546e-03, -2.6225e-02, -2.0990e-02,  1.1442e-01, -4.4545e-03,\n",
            "        -3.9425e-02,  1.9163e-02, -2.8955e-02,  1.6684e-01,  2.4105e-02,\n",
            "         6.0442e-03, -7.0770e-02, -7.7842e-02,  8.4673e-02, -5.3544e-02,\n",
            "        -1.0956e-01,  1.3577e-01, -2.3941e-02, -2.3967e-02,  1.6436e-01,\n",
            "         7.9803e-03, -3.7325e-02, -6.8462e-02, -8.2100e-02,  4.8028e-03,\n",
            "        -3.3036e-02, -1.4044e-01,  7.7110e-02,  1.1382e-01, -5.2671e-02,\n",
            "        -5.7564e-02,  4.5501e-02, -1.4379e-01,  1.9501e-02, -5.0005e-02,\n",
            "        -1.4704e-01,  2.5755e-02,  6.1805e-02, -2.0436e-03,  1.2480e-01,\n",
            "        -5.0180e-02, -7.1103e-02, -3.1200e-02,  3.1224e-02,  6.3596e-02,\n",
            "         5.5514e-02, -1.0685e-01, -5.1295e-02,  6.2093e-02,  1.5275e-02,\n",
            "        -9.3040e-02,  5.6142e-02, -1.1946e-01, -1.4975e-01, -2.5651e-02,\n",
            "        -4.3291e-02,  1.3654e-02,  6.8111e-02,  1.7785e-01, -4.9132e-02,\n",
            "        -1.3616e-02, -8.3235e-02,  7.6786e-02, -5.8391e-02, -8.5262e-02,\n",
            "        -9.7297e-03,  3.2541e-02,  4.8176e-03, -6.3623e-02, -2.6576e-02,\n",
            "         2.5306e-02, -7.5495e-03, -3.0829e-02,  1.1039e-02, -8.6664e-02,\n",
            "         2.6082e-02, -1.5537e-01,  6.6376e-02, -1.6296e-01,  2.3426e-01,\n",
            "         1.5162e-01, -4.8093e-02,  4.1752e-02,  4.3361e-02, -1.0709e-02,\n",
            "        -6.4075e-02,  1.6254e-03,  3.6068e-02,  8.3359e-02,  2.6380e-02,\n",
            "         4.3664e-02, -3.9103e-02,  7.6045e-02, -2.9978e-02, -8.5086e-02,\n",
            "         6.7919e-02, -7.3541e-03,  1.2619e-02, -2.4727e-02,  7.5703e-02,\n",
            "        -9.2229e-02,  6.0986e-02, -1.4771e-02,  1.6816e-02,  1.6291e-01,\n",
            "        -6.8868e-02,  4.5159e-02,  6.3458e-02, -9.1852e-02, -5.1478e-02,\n",
            "        -1.6428e-01, -1.0497e-01, -1.3148e-02], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "torch.Size([1024, 128, 1]) Parameter containing:\n",
            "tensor([[[ 0.0327],\n",
            "         [-0.0475],\n",
            "         [ 0.0867],\n",
            "         ...,\n",
            "         [-0.0050],\n",
            "         [ 0.0435],\n",
            "         [ 0.0384]],\n",
            "\n",
            "        [[ 0.0394],\n",
            "         [-0.0953],\n",
            "         [-0.0169],\n",
            "         ...,\n",
            "         [-0.0297],\n",
            "         [-0.0512],\n",
            "         [-0.0581]],\n",
            "\n",
            "        [[-0.1088],\n",
            "         [ 0.0402],\n",
            "         [-0.0562],\n",
            "         ...,\n",
            "         [-0.0103],\n",
            "         [-0.0277],\n",
            "         [ 0.0031]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-0.0492],\n",
            "         [-0.0408],\n",
            "         [-0.0557],\n",
            "         ...,\n",
            "         [-0.1159],\n",
            "         [ 0.0004],\n",
            "         [ 0.0496]],\n",
            "\n",
            "        [[-0.0106],\n",
            "         [-0.0096],\n",
            "         [-0.0677],\n",
            "         ...,\n",
            "         [-0.0052],\n",
            "         [-0.0355],\n",
            "         [-0.0869]],\n",
            "\n",
            "        [[ 0.0084],\n",
            "         [-0.0207],\n",
            "         [ 0.0461],\n",
            "         ...,\n",
            "         [ 0.0411],\n",
            "         [-0.0672],\n",
            "         [ 0.0008]]], device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([ 0.0456,  0.0435, -0.0185,  ...,  0.0112,  0.0348,  0.0551],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([0.8638, 0.4775, 0.8083,  ..., 0.8126, 0.8632, 0.8882], device='cuda:0',\n",
            "       requires_grad=True)\n",
            "torch.Size([1024]) Parameter containing:\n",
            "tensor([-0.2589, -0.4149, -0.2949,  ..., -0.2896, -0.2336, -0.2309],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512, 1024]) Parameter containing:\n",
            "tensor([[ 0.0114, -0.0077, -0.0173,  ...,  0.0076,  0.0321,  0.0232],\n",
            "        [ 0.0033, -0.0173, -0.0358,  ..., -0.0283, -0.0169,  0.0285],\n",
            "        [-0.0134, -0.0045,  0.0214,  ...,  0.0261, -0.0254, -0.0187],\n",
            "        ...,\n",
            "        [-0.0251, -0.0247,  0.0273,  ..., -0.0332,  0.0062, -0.0243],\n",
            "        [ 0.0236, -0.0095,  0.0193,  ...,  0.0159,  0.0286,  0.0075],\n",
            "        [ 0.0175,  0.0009,  0.0224,  ...,  0.0207, -0.0058, -0.0026]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([512]) Parameter containing:\n",
            "tensor([-3.3015e-02, -3.4559e-02, -2.6032e-02, -2.8296e-02, -1.1772e-01,\n",
            "        -4.2701e-02,  3.4695e-02,  7.5892e-02, -1.3644e-01,  3.7885e-02,\n",
            "        -7.7726e-03,  3.6649e-02, -4.7682e-02,  4.2174e-02, -5.6185e-02,\n",
            "        -1.6050e-02, -4.5734e-02,  4.7546e-02,  4.4494e-02,  7.8743e-03,\n",
            "        -2.1767e-01, -2.6468e-02,  3.4332e-02,  3.3639e-02,  1.7983e-01,\n",
            "         6.1861e-02,  5.8127e-03, -5.1991e-02, -1.8019e-03,  2.4805e-02,\n",
            "         1.0502e-01,  5.2882e-02, -3.8910e-02, -3.0201e-02, -4.8525e-02,\n",
            "         1.8610e-02, -5.2307e-02,  2.5482e-02, -3.6708e-03,  3.4957e-03,\n",
            "         1.5096e-03, -9.8403e-02,  1.6494e-02, -2.3393e-02,  4.5986e-02,\n",
            "         5.1770e-03,  3.7619e-03,  5.3946e-02, -5.7316e-02, -1.7386e-02,\n",
            "        -5.1556e-03,  2.6172e-02,  3.5763e-02, -3.3659e-02,  7.9625e-03,\n",
            "         1.0897e-02,  7.0250e-02,  1.3559e-01, -1.9076e-02,  2.4391e-02,\n",
            "         4.9543e-02,  2.3962e-02,  5.5780e-02, -6.5688e-02, -5.8086e-02,\n",
            "        -3.0863e-02, -6.5506e-02,  1.4383e-02, -2.6235e-02,  1.0871e-02,\n",
            "         4.7635e-04, -1.4573e-02,  4.2322e-03,  3.2343e-02,  6.4085e-03,\n",
            "         7.4289e-03, -5.4784e-02,  2.8908e-03,  1.3174e-01, -2.4598e-02,\n",
            "         6.4235e-02, -1.0038e-01,  4.8838e-02,  5.0797e-03, -1.2583e-02,\n",
            "         2.0920e-02, -4.5395e-02,  2.7869e-03,  1.5530e-02, -3.6779e-02,\n",
            "        -7.9192e-02, -5.9843e-02, -5.0997e-02, -6.4405e-02,  2.2278e-01,\n",
            "         6.9592e-03,  1.2264e-02,  1.3455e-02,  3.7305e-02,  2.7679e-02,\n",
            "        -2.0598e-02, -1.3754e-02,  4.1201e-03,  5.5146e-02,  3.5296e-03,\n",
            "         2.5375e-02,  8.2038e-02, -3.3187e-02, -7.1326e-02,  4.9256e-02,\n",
            "         8.4356e-02, -8.2934e-02,  2.2987e-01,  3.8564e-02, -1.8221e-02,\n",
            "        -4.6312e-02, -5.3917e-02, -2.8020e-02, -3.9375e-02, -9.6959e-02,\n",
            "        -3.5739e-02, -3.2500e-02, -7.7635e-02,  1.1025e-02,  6.8920e-03,\n",
            "         6.7658e-02, -7.7743e-03,  3.5227e-02, -4.0079e-02,  3.1409e-02,\n",
            "        -3.1054e-02,  5.9380e-02,  1.2849e-02,  7.7803e-03,  2.7748e-02,\n",
            "        -4.4942e-02, -2.7023e-02,  4.6079e-02,  3.3259e-02, -5.5617e-02,\n",
            "        -1.9646e-02, -1.7245e-02, -8.0466e-03, -4.7311e-02,  1.2995e-02,\n",
            "         1.6185e-02,  1.7413e-02, -1.3490e-02,  1.9514e-02,  8.8398e-02,\n",
            "        -3.7639e-03, -3.0529e-02,  3.1887e-02, -3.7142e-02, -2.5122e-03,\n",
            "         3.9291e-02, -3.9263e-02, -2.0274e-02, -6.1979e-02,  1.5402e-02,\n",
            "        -7.1876e-03,  4.8052e-02, -6.2018e-05,  3.2378e-02,  3.3756e-02,\n",
            "         4.6593e-03,  7.1666e-02, -2.9823e-02,  7.5051e-02, -4.7214e-02,\n",
            "        -1.6174e-01, -3.4717e-02,  2.3555e-02,  1.8397e-01,  2.1735e-03,\n",
            "         6.5464e-02,  9.5267e-02,  1.3026e-01,  3.9624e-02,  3.4774e-02,\n",
            "        -5.9611e-02,  1.3566e-02, -1.0886e-01, -1.5220e-02, -4.4125e-02,\n",
            "         3.1604e-02, -5.0249e-03, -2.4080e-02,  5.6549e-02,  3.7559e-02,\n",
            "        -3.7479e-03, -4.5552e-02,  5.4649e-02, -7.5676e-02,  2.7599e-02,\n",
            "         1.9433e-02,  1.6036e-02,  4.8570e-03,  2.9763e-02, -6.6185e-02,\n",
            "        -5.6544e-03, -4.1637e-02, -9.7764e-02, -5.8992e-03, -1.5188e-02,\n",
            "         6.3846e-02, -9.6329e-03,  2.5103e-02, -2.0457e-02,  1.6812e-01,\n",
            "        -4.6596e-02, -8.9201e-03,  3.7375e-02, -6.3232e-02,  8.3513e-02,\n",
            "        -1.5916e-02,  8.0003e-03, -2.8528e-02, -4.0372e-03, -8.5953e-03,\n",
            "        -5.8444e-02,  3.5237e-02,  5.4822e-02, -5.4016e-02,  2.9060e-02,\n",
            "         1.7838e-02, -7.1784e-02,  6.3190e-03, -3.3833e-02,  4.2177e-02,\n",
            "        -9.8938e-03, -3.0159e-02,  4.4840e-02, -6.5981e-03,  5.1752e-02,\n",
            "         1.5373e-02, -3.3865e-02, -5.5561e-02,  8.1120e-03,  2.5512e-03,\n",
            "        -1.9254e-02, -2.2526e-02,  1.1546e-02,  2.4009e-02,  4.8348e-02,\n",
            "         3.0167e-02,  9.1269e-02,  4.0500e-02,  1.1579e-02, -1.9146e-02,\n",
            "         1.7541e-02,  4.5447e-02,  1.4446e-01, -3.4448e-02,  3.1190e-02,\n",
            "        -5.2032e-02,  1.0166e-02, -5.6496e-02,  1.7425e-02,  4.2288e-02,\n",
            "        -1.1341e-02,  3.8357e-02, -1.0784e-02, -7.0620e-04, -1.6829e-01,\n",
            "        -6.1435e-02,  9.8193e-03,  6.7184e-02,  1.2651e-02,  8.4971e-03,\n",
            "        -1.2699e-01,  3.3522e-02,  7.3768e-03,  2.3488e-02,  4.0349e-02,\n",
            "        -1.1091e-03,  1.0683e-01,  2.8418e-02,  8.4145e-02,  4.0025e-02,\n",
            "        -1.6297e-02, -2.6105e-02, -2.7333e-02,  6.8832e-03, -4.7400e-02,\n",
            "        -1.7007e-01, -5.6318e-02,  2.0029e-02,  4.5800e-02, -2.3279e-02,\n",
            "         3.3410e-02, -3.6467e-03, -1.7909e-02, -1.9558e-01, -4.0579e-02,\n",
            "        -8.2167e-02, -6.3801e-02, -3.5906e-02, -4.0796e-03, -1.1517e-02,\n",
            "         1.2138e-01,  1.7091e-02,  4.6320e-02, -2.1200e-02,  5.7884e-02,\n",
            "         7.1312e-04,  3.9371e-02, -2.9520e-02, -2.8451e-02,  7.8078e-03,\n",
            "         1.6441e-01,  5.2116e-02,  9.5109e-02,  1.8408e-02,  7.4535e-02,\n",
            "         5.8135e-02,  1.0628e-01,  4.8298e-02,  1.8367e-02,  3.5326e-02,\n",
            "        -6.0477e-03,  1.7510e-02, -3.9596e-02, -8.6662e-02,  4.0958e-02,\n",
            "         2.6792e-02,  6.8889e-02, -6.3260e-02, -4.4969e-02,  1.2088e-02,\n",
            "        -2.4715e-03,  1.4144e-02, -4.1824e-02,  2.4259e-01, -4.0151e-02,\n",
            "        -1.1730e-01, -2.1861e-02,  7.9799e-03,  4.2674e-02, -3.0112e-02,\n",
            "        -7.6444e-03, -7.2171e-02, -9.9194e-03,  3.0907e-02,  7.1167e-03,\n",
            "         6.8827e-03, -4.0177e-02, -2.1054e-02,  5.6401e-02, -2.7578e-02,\n",
            "        -1.7744e-02,  9.6954e-02, -7.0923e-02,  2.2752e-02, -2.7851e-02,\n",
            "         6.3298e-02,  5.7706e-02,  1.7131e-02, -5.4746e-02, -1.7636e-03,\n",
            "         4.2844e-02, -6.0444e-02,  9.1954e-02,  1.5977e-03,  1.6031e-02,\n",
            "         6.3223e-03, -8.3557e-02, -4.4408e-02,  1.9469e-02,  2.0401e-02,\n",
            "         3.2990e-02,  3.8596e-03,  2.4947e-02, -4.5652e-02, -7.2621e-02,\n",
            "        -1.7209e-02, -1.5695e-02,  9.9258e-03,  1.5551e-02, -7.2479e-02,\n",
            "        -2.0007e-02,  1.8275e-01, -5.7544e-02,  6.4039e-02, -1.4820e-03,\n",
            "         1.7026e-02,  3.0116e-02,  6.7936e-02,  7.1374e-02, -1.0826e-02,\n",
            "        -9.1577e-04, -6.5003e-02, -1.2083e-02, -1.8223e-01, -2.2584e-02,\n",
            "         9.9392e-03,  2.5444e-03, -1.4340e-02, -1.9697e-02,  4.4716e-02,\n",
            "         3.2624e-02, -9.3089e-03,  1.0795e-02,  5.4433e-02, -6.7448e-02,\n",
            "         1.8403e-02,  2.8574e-02,  1.8180e-02, -5.6832e-02, -8.5685e-02,\n",
            "        -5.8160e-02, -3.8040e-02, -2.9663e-02, -2.6358e-03,  2.6201e-02,\n",
            "        -3.5362e-02, -4.2028e-03, -3.4651e-02, -2.5179e-03,  4.0855e-02,\n",
            "         4.4284e-02, -2.6097e-02,  5.9968e-02,  7.3902e-04, -1.7782e-01,\n",
            "         2.1518e-02,  2.2930e-02, -9.1647e-02, -2.0702e-02, -6.3286e-04,\n",
            "        -6.6860e-03,  4.0487e-03, -2.8902e-02,  5.7806e-02, -8.7313e-02,\n",
            "         3.1387e-02,  2.8683e-02,  6.9478e-02,  6.2586e-02,  1.3134e-03,\n",
            "         1.9231e-01,  2.7069e-02, -3.7175e-02,  3.4203e-02, -2.5571e-02,\n",
            "         3.6931e-02,  8.6565e-03,  3.9925e-02, -1.9120e-02,  4.2087e-02,\n",
            "         1.7444e-02, -7.0203e-02,  1.5715e-02, -4.1871e-02,  2.8075e-02,\n",
            "         1.4826e-02, -4.2877e-02,  5.0934e-02, -2.5278e-02, -7.3454e-02,\n",
            "         1.0415e-01, -7.4399e-03, -3.3091e-03,  1.9615e-02,  3.3983e-02,\n",
            "        -3.9284e-02, -3.4295e-02,  6.5958e-02, -6.3995e-02,  5.1958e-02,\n",
            "         1.2819e-02,  3.8094e-02,  7.9500e-02,  4.3911e-03, -1.1731e-01,\n",
            "         1.3045e-02, -2.6283e-02,  3.9439e-02,  4.0780e-02, -6.0276e-02,\n",
            "        -3.4325e-03,  1.8406e-03,  6.3931e-02,  1.8271e-02,  3.5494e-02,\n",
            "        -6.2285e-02,  2.5261e-03, -3.2637e-02, -2.5305e-02,  2.3108e-02,\n",
            "         4.4942e-02,  5.9167e-02, -3.8228e-02,  1.6943e-02,  2.9430e-02,\n",
            "         5.5257e-02, -1.1605e-02, -5.2953e-02, -1.8552e-02,  3.3122e-02,\n",
            "        -4.5702e-02,  3.4497e-02,  4.7684e-02,  3.7496e-02, -1.6362e-01,\n",
            "         5.2959e-02,  4.1843e-02,  2.0637e-02, -1.4229e-02,  8.3231e-02,\n",
            "        -2.2107e-02,  4.4229e-02], device='cuda:0', requires_grad=True)\n",
            "torch.Size([256, 512]) Parameter containing:\n",
            "tensor([[-0.0091,  0.0472,  0.0024,  ..., -0.0310, -0.0070,  0.0188],\n",
            "        [-0.0211,  0.0358,  0.0028,  ...,  0.0063,  0.0254,  0.0048],\n",
            "        [-0.0382, -0.0031, -0.0200,  ...,  0.0247, -0.0121,  0.0126],\n",
            "        ...,\n",
            "        [-0.0375,  0.0109, -0.0195,  ..., -0.0072, -0.0203, -0.0311],\n",
            "        [-0.0054, -0.0317,  0.0251,  ..., -0.0078,  0.0409,  0.0182],\n",
            "        [ 0.0009, -0.0281,  0.0202,  ...,  0.0030, -0.0102, -0.0216]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([256]) Parameter containing:\n",
            "tensor([ 0.1775, -0.0868,  0.1223,  0.1734,  0.0573,  0.1429,  0.0805, -0.1281,\n",
            "        -0.2148, -0.1685, -0.0609,  0.0736, -0.1537,  0.2049, -0.0739, -0.0723,\n",
            "        -0.0244,  0.2131, -0.1789,  0.0529, -0.3272,  0.0492, -0.0232, -0.0705,\n",
            "         0.0710,  0.1012, -0.1198, -0.2592,  0.1338, -0.2107,  0.1126,  0.0433,\n",
            "        -0.1260,  0.1550,  0.0340,  0.1611, -0.1016, -0.0030,  0.0634,  0.0538,\n",
            "         0.1531, -0.1411, -0.0775, -0.0958,  0.1598,  0.0237, -0.0792,  0.0475,\n",
            "         0.1721,  0.0012, -0.0036, -0.0031, -0.1972,  0.0818, -0.0586, -0.1650,\n",
            "        -0.0281, -0.2054, -0.1014, -0.1036, -0.0007, -0.1779, -0.0029, -0.0518,\n",
            "         0.0317, -0.1431,  0.1697, -0.2186, -0.0707,  0.0842, -0.1224,  0.0046,\n",
            "        -0.0715,  0.0571, -0.0851,  0.0369, -0.1476,  0.0917,  0.0211,  0.0286,\n",
            "        -0.0454,  0.1670, -0.0132,  0.1170,  0.0197, -0.0098, -0.0527, -0.0144,\n",
            "         0.0645, -0.0842, -0.0114,  0.1750, -0.0160, -0.1478,  0.1588,  0.1772,\n",
            "         0.1441,  0.0385, -0.0398,  0.1283,  0.1694, -0.2648, -0.0146, -0.0617,\n",
            "         0.1393,  0.0578, -0.0669, -0.0435, -0.1604, -0.1209,  0.1649, -0.0120,\n",
            "         0.0787, -0.1913, -0.0276, -0.0738, -0.1960,  0.0887,  0.0821,  0.0580,\n",
            "         0.1923, -0.2355, -0.1081, -0.1030, -0.0805, -0.0464, -0.0726, -0.0664,\n",
            "         0.2194,  0.0291, -0.1719,  0.0351,  0.0126,  0.0788,  0.1308,  0.2011,\n",
            "        -0.2587,  0.2217, -0.2017,  0.1748,  0.1672,  0.2394,  0.1235,  0.0319,\n",
            "         0.0798, -0.1136, -0.1680,  0.2404, -0.2039,  0.1550, -0.0438,  0.1205,\n",
            "        -0.0871, -0.0823,  0.0552, -0.0783,  0.0563,  0.1401, -0.2848,  0.0694,\n",
            "        -0.0604, -0.0211, -0.0260, -0.0776,  0.1637, -0.1727, -0.1002, -0.0495,\n",
            "        -0.1372,  0.2683,  0.1195,  0.2963, -0.2629, -0.1227, -0.1317, -0.2688,\n",
            "         0.0313,  0.2604, -0.1055, -0.0502, -0.1098,  0.0450, -0.0449, -0.2226,\n",
            "        -0.0349, -0.1567,  0.0576,  0.0628, -0.2077,  0.0548, -0.0882,  0.1611,\n",
            "         0.2239,  0.2016,  0.0653, -0.0194,  0.1619, -0.0357, -0.0763,  0.1284,\n",
            "         0.0556, -0.0256,  0.0854, -0.0581,  0.0521, -0.1489,  0.2823,  0.0004,\n",
            "        -0.2009,  0.2283,  0.1539, -0.1460,  0.0047, -0.0196,  0.0627,  0.1775,\n",
            "         0.0703,  0.1497,  0.0565,  0.2301,  0.0024, -0.0692, -0.1317, -0.0096,\n",
            "         0.0486,  0.0663,  0.0646, -0.0919, -0.0037,  0.0299, -0.0321,  0.0690,\n",
            "         0.0108, -0.0140,  0.0373,  0.0395, -0.0656,  0.0019, -0.0337,  0.0387,\n",
            "        -0.0641, -0.1685, -0.0371, -0.1510, -0.1482,  0.0403,  0.0898, -0.0933,\n",
            "         0.2498,  0.2051,  0.1221, -0.0295, -0.0353, -0.0114,  0.1600,  0.1637],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([40, 256]) Parameter containing:\n",
            "tensor([[ 0.0427, -0.0241,  0.0245,  ..., -0.0231,  0.0381,  0.0236],\n",
            "        [-0.1155,  0.0961, -0.0442,  ...,  0.0620, -0.0975, -0.0592],\n",
            "        [ 0.0151, -0.0451, -0.0200,  ..., -0.0215,  0.0017,  0.0216],\n",
            "        ...,\n",
            "        [ 0.0211, -0.0025,  0.0415,  ..., -0.0018,  0.0398,  0.0017],\n",
            "        [-0.0170,  0.0357, -0.0535,  ..., -0.0191, -0.0340,  0.0001],\n",
            "        [-0.0106,  0.0150,  0.0088,  ...,  0.0527,  0.0208,  0.0022]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "torch.Size([40]) Parameter containing:\n",
            "tensor([ 0.2052, -0.3622,  0.1656, -0.0755,  0.1473,  0.0705, -0.1052,  0.0022,\n",
            "         0.2837, -0.0336, -0.0956, -0.0615, -0.4248, -0.0214, -0.4776, -0.1122,\n",
            "        -0.0988, -0.0531, -0.0103, -0.1223, -0.1161,  0.0217, -0.4443, -0.4997,\n",
            "        -0.1517,  0.0316,  0.0356, -0.0846, -0.1364, -0.1331, -0.3883, -0.0590,\n",
            "        -0.1286, -0.5070, -0.0734,  0.0420,  0.0513,  0.1721, -0.1525, -0.0666],\n",
            "       device='cuda:0', requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7zCdhIO5mTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}